diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index f5908cb7..d328dfa3 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -14,6 +14,11 @@ from stable_baselines3 import PPO
 # gym environment
 import gym
 
+# video logging
+from matplotlib import animation
+import matplotlib.pyplot as plt
+import numpy as np
+
 # logging python
 import logging
 import sys
@@ -123,7 +128,7 @@ class PPO_PolicyGradient_V1:
         in_dim, 
         out_dim,
         total_training_steps,
-        batch_size,
+        max_trajectory_size,
         n_rollout_steps,
         n_optepochs=5,
         lr_p=1e-3,
@@ -146,7 +151,7 @@ class PPO_PolicyGradient_V1:
         self.in_dim = in_dim
         self.out_dim = out_dim
         self.total_training_steps = total_training_steps
-        self.batch_size = batch_size
+        self.max_trajectory_size = max_trajectory_size
         self.n_rollout_steps = n_rollout_steps
         self.n_optepochs = n_optepochs
         self.lr_p = lr_p
@@ -169,7 +174,7 @@ class PPO_PolicyGradient_V1:
         self.log_video = log_video
 
         # keep track of rewards per episode
-        self.ep_returns = deque(maxlen=batch_size)
+        self.ep_returns = deque(maxlen=max_trajectory_size)
         self.csv_writer = csv_writer
         self.stats_plotter = stats_plotter
         self.stats_data = {
@@ -212,7 +217,7 @@ class PPO_PolicyGradient_V2:
         :param env: The environment to learn from (if registered in Gym)
         :param learning_rate: The learning rate, it can be a function
         of the current progress remaining (from 1 to 0)
-        :param batch_size: Minibatch size of collected experiences
+        :param max_trajectory_size: Minibatch size of collected experiences
         :param n_optepochs: Number of epoch when optimizing the surrogate loss
         :param gamma: Discount factor
         :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
@@ -233,7 +238,7 @@ class PPO_PolicyGradient_V2:
         in_dim, 
         out_dim,
         total_training_steps,
-        batch_size,
+        max_trajectory_size,
         n_rollout_steps,
         n_optepochs=5,
         lr_p=1e-3,
@@ -250,6 +255,7 @@ class PPO_PolicyGradient_V2:
         csv_writer=None,
         stats_plotter=None,
         log_video=False,
+        video_log_steps=100,
         device='cpu',
         exp_path='./log/',
         exp_name='PPO_V2_experiment',
@@ -260,7 +266,7 @@ class PPO_PolicyGradient_V2:
         self.in_dim = in_dim
         self.out_dim = out_dim
         self.total_training_steps = total_training_steps
-        self.batch_size = batch_size
+        self.max_trajectory_size = max_trajectory_size
         self.n_rollout_steps = n_rollout_steps
         self.n_optepochs = n_optepochs
         self.lr_p = lr_p
@@ -286,9 +292,10 @@ class PPO_PolicyGradient_V2:
         self.exp_name = exp_name
         # track video of gym
         self.log_video = log_video
+        self.video_log_steps = video_log_steps
 
         # keep track of rewards per episode
-        self.ep_returns = deque(maxlen=batch_size)
+        self.ep_returns = deque(maxlen=max_trajectory_size)
         self.csv_writer = csv_writer
         self.stats_plotter = stats_plotter
         self.stats_data = {
@@ -563,9 +570,11 @@ class PPO_PolicyGradient_V2:
         pass 
 
     def collect_rollout(self, n_steps=1):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)
+           # TODO: rollout length - 2048 t0 4096
+        """
         
-        t_step, rewards = 0, []
+        t_step, rewards, frames = 0, [], deque(maxlen=24) # 4 fps - 6 sec
 
         # log time
         episode_time = []
@@ -584,7 +593,7 @@ class PPO_PolicyGradient_V2:
         while t_step < n_steps:
             
             # rewards collected
-            rewards, done = [], False 
+            rewards, done, frames = [], False, []
             obs = self.env.reset()
 
             # measure time elapsed for one episode
@@ -593,10 +602,10 @@ class PPO_PolicyGradient_V2:
 
             # Run episode for a fixed amount of timesteps
             # to keep rollout size fixed and episodes independent
-            for t_batch in range(0, self.batch_size):
+            for t_batch in range(0, self.max_trajectory_size):
                 # render gym envs
                 if self.render_video and t_batch % self.render_steps == 0:
-                    self.env.render()
+                    frames.append(self.env.render(mode="rgb_array"))
                 
                 t_step += 1 
 
@@ -642,7 +651,7 @@ class PPO_PolicyGradient_V2:
         action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
         dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time)
+        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time), frames
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -672,7 +681,7 @@ class PPO_PolicyGradient_V2:
             # Collect data over one episode
             # Episode = recording of actions and states that an agent performed from a start state to an end state
             # STEP 3: simulate and collect trajectories --> the following values are all per batch over one episode
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.n_rollout_steps)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time, frames = self.collect_rollout(n_steps=self.n_rollout_steps)
 
             # experiences simulated so far
             training_steps += np.sum(ep_lens)
@@ -681,9 +690,9 @@ class PPO_PolicyGradient_V2:
             values, _ , _ = self.get_values(obs, actions)
             # Calculate advantage function
             # advantages, cum_returns = self.advantage_reinforce(rewards, normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            # advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             # advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            # advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             
             # update network params 
             for _ in range(self.n_optepochs):
@@ -696,7 +705,7 @@ class PPO_PolicyGradient_V2:
 
             # log all statistical values to CSV
             self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time, done_so_far, exp_name=self.exp_name)
-            
+
             # increment for each iteration
             done_so_far += 1
 
@@ -729,6 +738,16 @@ class PPO_PolicyGradient_V2:
                     for value in self.stats_data.values():
                         del value[:]
 
+                # Log to video
+                if self.render_video and done_so_far % self.video_log_steps == 0:
+                    filename='pendulum_v1.gif'
+                    self.save_frames_as_gif(frames, self.exp_path, filename)
+                    wandb.log({
+                        "train/video": wandb.Video(os.path.join(self.exp_path, filename), 
+                        caption='episode: '+str(done_so_far), 
+                        fps=4, format="gif"), "step": done_so_far
+                        })
+
         # Finalize and plot stats
         if self.stats_plotter:
             df = self.stats_plotter.read_csv() # read all files in folder
@@ -745,6 +764,20 @@ class PPO_PolicyGradient_V2:
             # Save any files starting with "ppo"
             wandb.save(os.path.join(wandb.run.dir, "ppo*"))
 
+    def save_frames_as_gif(self, frames, path='./', filename='pendulum_v1.gif'):
+
+        #Mess with this to change frame size
+        plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)
+
+        patch = plt.imshow(frames[0])
+        plt.axis('off')
+
+        def animate(i):
+            patch.set_data(frames[i])
+
+        anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)
+        video_path = os.path.join(path, filename)
+        anim.save(video_path, writer='imagemagick', fps=60)
 
     def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, done_so_far, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
@@ -796,9 +829,11 @@ class PPO_PolicyGradient_V2:
         logging.info('\n')
         logging.info(f'------------ Episode: {training_steps} --------------')
         logging.info(f"Mean return:          {mean_ep_ret}")
+        logging.info(f"Min return:           {min_ep_ret}")
+        logging.info(f"Max return:           {max_ep_ret}")
         logging.info(f"Mean policy loss:     {mean_p_loss}")
         logging.info(f"Mean value loss:      {mean_v_loss}")
-        logging.info('--------------------------------------------')
+        logging.info('-----------------------------------------------------')
         logging.info('\n')
 
 ####################
@@ -914,10 +949,10 @@ def _log_summary(ep_len, ep_ret, ep_num):
         logging.info(f"--------------------------------------------")
         logging.info('\n')
 
-def train(env, in_dim, out_dim, total_training_steps, batch_size=512, n_rollout_steps=2048,
+def train(env, in_dim, out_dim, total_training_steps, max_trajectory_size=512, n_rollout_steps=2048,
           n_optepochs=32, learning_rate_p=1e-4, learning_rate_v=1e-3, gae_lambda=0.95, gamma=0.99, epsilon=0.2,
           adam_epsilon=1e-8, render_steps=10, render_video=False, save_steps=10, csv_writer=None, stats_plotter=None,
-          normalize_adv=False, normalize_ret=False, log_video=False, ppo_version='v2', 
+          normalize_adv=False, normalize_ret=False, log_video=False, video_log_steps=1000, ppo_version='v2', 
           device='cpu', exp_path='./log/', exp_name='PPO-experiment'):
     """Train the policy network (actor) and the value network (critic) with PPO (clip version)"""
     agent = None
@@ -927,7 +962,7 @@ def train(env, in_dim, out_dim, total_training_steps, batch_size=512, n_rollout_
                     in_dim=in_dim, 
                     out_dim=out_dim,
                     total_training_steps=total_training_steps,
-                    batch_size=batch_size,
+                    max_trajectory_size=max_trajectory_size,
                     n_rollout_steps=n_rollout_steps,
                     n_optepochs=n_optepochs,
                     lr_p=learning_rate_p,
@@ -944,31 +979,10 @@ def train(env, in_dim, out_dim, total_training_steps, batch_size=512, n_rollout_
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
                     log_video=log_video,
+                    video_log_steps=video_log_steps,
                     device=device,
                     exp_path=exp_path,
                     exp_name=exp_name)
-    else:
-        agent = PPO_PolicyGradient_V1(
-                    env, 
-                    in_dim=in_dim, 
-                    out_dim=out_dim,
-                    total_training_steps=total_training_steps,
-                    batch_size=batch_size,
-                    n_rollout_steps=n_rollout_steps,
-                    n_optepochs=n_optepochs,
-                    lr_p=learning_rate_p,
-                    lr_v=learning_rate_v,
-                    gae_lambda = gae_lambda,
-                    gamma=gamma,
-                    epsilon=epsilon,
-                    adam_eps=adam_epsilon,
-                    render_steps=render_steps,
-                    render_video=render_video,
-                    save_model=save_steps,
-                    csv_writer=csv_writer,
-                    stats_plotter=stats_plotter,
-                    log_video=log_video,
-                    device=device)
     # run training for a total amount of steps
     agent.learn()
 
@@ -992,7 +1006,7 @@ def hyperparam_tuning(config=None):
                 in_dim=config.obs_dim, 
                 out_dim=config.act_dim,
                 total_training_steps=config.total_training_steps,
-                batch_size=config.batch_size,
+                max_trajectory_size=config.max_trajectory_size,
                 n_rollout_steps=config.n_rollout_steps,
                 n_optepochs=config.n_optepochs,
                 gae_lambda = config.gae_lambda,
@@ -1021,25 +1035,26 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 1_500_000     # time steps regarding batches collected and train agent
-    batch_size = 1024                    # max number of episode samples to be sampled per time step. 
+    total_training_steps = 1_200_000     # time steps regarding batches collected and train agent
+    max_trajectory_size = 1024                    # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
-    n_optepochs = 32                     # Number of epochs per time step to optimize the neural networks
+    n_optepochs = 64                     # Number of epochs per time step to optimize the neural networks
     learning_rate_p = 1e-4               # learning rate for policy network
     learning_rate_v = 1e-3               # learning rate for value network
-    gae_lambda = 0.9                     # factor for trade-off of bias vs variance for GAE
-    gamma = 0.96                         # discount factor
+    gae_lambda = 0.92                    # factor for trade-off of bias vs variance for GAE
+    gamma = 0.95                         # discount factor
     adam_epsilon = 1e-7                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
     epsilon = 0.2                        # clipping factor
     clip_range_vf = 0.2                  # clipping factor for the value loss function. Depends on reward scaling.
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0', 'takeoff-aviary-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_adv = True                 # wether to normalize the advantage estimate
-    normalize_ret = True                # wether to normalize the return function
+    normalize_adv = False                # wether to normalize the advantage estimate
+    normalize_ret = True                 # wether to normalize the return function
     
     # setup for torch save models and rendering
     render_video = False
+    video_log_steps = 1000
     render_steps = 10
     save_steps = 100
 
@@ -1102,7 +1117,7 @@ if __name__ == '__main__':
                     'env name': env_name,
                     'env number': env_number,
                     'total_training_steps': total_training_steps,
-                    'max sampled trajectories': batch_size,
+                    'max sampled trajectories': max_trajectory_size,
                     'batches per episode': n_rollout_steps,
                     'number of epochs for update': n_optepochs,
                     'input layer size': obs_dim,
@@ -1135,7 +1150,7 @@ if __name__ == '__main__':
             in_dim=obs_dim, 
             out_dim=act_dim,
             total_training_steps=total_training_steps,
-            batch_size=batch_size,
+            max_trajectory_size=max_trajectory_size,
             n_rollout_steps=n_rollout_steps,
             n_optepochs=n_optepochs,
             learning_rate_p=learning_rate_p, 
@@ -1152,6 +1167,7 @@ if __name__ == '__main__':
             csv_writer=csv_writer,
             stats_plotter=stats_plotter,
             log_video=args.video,
+            video_log_steps=video_log_steps,
             device=device,
             exp_path=exp_folder_name,
             exp_name=exp_name)
@@ -1179,6 +1195,7 @@ if __name__ == '__main__':
         for key, val in param_dict.items():
             logging.info(f'Hyperparam Tuning \n----- Key: {key} -----\n')
             logging.info(f'Hyperparam Tuning \n----- Values: {val} -----\n')
+            
             match key:
                     case 'learning rate (policy net)':
                         for i in range(0, len(val)):
@@ -1204,7 +1221,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': val[i],'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': adam_epsilon,
@@ -1256,7 +1273,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': val[i],'epsilon (adam optimizer)': adam_epsilon,
@@ -1308,7 +1325,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': adam_epsilon,
@@ -1360,7 +1377,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': adam_epsilon,
@@ -1412,7 +1429,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': adam_epsilon,
@@ -1465,7 +1482,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': n_optepochs,'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': val[i],
@@ -1518,7 +1535,7 @@ if __name__ == '__main__':
                                         project=args.project_name,
                                         entity='drone-mechanics',
                                         sync_tensorboard=True,
-                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': batch_size,
+                                        config={ 'env name': env_name, 'env number': env_number, 'total_training_steps': total_training_steps, 'max sampled trajectories': max_trajectory_size,
                                             'batches per episode': n_rollout_steps,'number of epochs for update': val[i],'input layer size': obs_dim,'output layer size': act_dim,
                                             'observation space': obs_shape,'action space': act_shape,'action space upper bound': upper_bound,'action space lower bound': lower_bound,
                                             'learning rate (policy net)': learning_rate_p,'learning rate (value net)': learning_rate_v,'epsilon (adam optimizer)': adam_epsilon,
@@ -1589,7 +1606,7 @@ if __name__ == '__main__':
                                 in_dim=obs_dim, 
                                 out_dim=act_dim,
                                 total_training_steps=total_training_steps,
-                                batch_size=val[i],
+                                max_trajectory_size=val[i],
                                 exp_path=exp_folder_name,
                                 exp_name=exp_name,
                                 normalize_adv=normalize_adv,
diff --git a/gym_pybullet_drones/gym_pybullet_drones/examples/learn.py b/gym_pybullet_drones/gym_pybullet_drones/examples/learn.py
index 331fa54b..d947dc20 100644
--- a/gym_pybullet_drones/gym_pybullet_drones/examples/learn.py
+++ b/gym_pybullet_drones/gym_pybullet_drones/examples/learn.py
@@ -20,12 +20,13 @@ import time
 import argparse
 import gym
 import numpy as np
+import torch
 from stable_baselines3 import A2C
 from stable_baselines3.a2c import MlpPolicy
 from stable_baselines3.common.env_checker import check_env
 import ray
 from ray.tune import register_env
-#from ray.rllib.agents import ppo
+# from ray.rllib.agents import ppo
 
 from gym_pybullet_drones.utils.Logger import Logger
 from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary
@@ -39,10 +40,18 @@ DEFAULT_RECORD_VIDEO = False
 DEFAULT_OUTPUT_FOLDER = 'results'
 DEFAULT_COLAB = False
 
-def run(rllib=DEFAULT_RLLIB,output_folder=DEFAULT_OUTPUT_FOLDER, gui=DEFAULT_GUI, plot=True, colab=DEFAULT_COLAB, record_video=DEFAULT_RECORD_VIDEO):
+def make_env(env_id, seed=42):
+    env = gym.make(env_id)
+    env.seed(seed)
+    env.action_space.seed(seed)
+    env.observation_space.seed(seed)
+    return env
+
+def run(rllib=DEFAULT_RLLIB,output_folder=DEFAULT_OUTPUT_FOLDER, gui=DEFAULT_GUI, plot=True, colab=DEFAULT_COLAB, record_video=DEFAULT_RECORD_VIDEO, seed=42):
 
     #### Check the environment's spaces ########################
-    env = gym.make("takeoff-aviary-v0")
+    env = make_env("takeoff-aviary-v0", seed=seed)
+
     print("[INFO] Action space:", env.action_space)
     print("[INFO] Observation space:", env.observation_space)
     #print(env.action_space.sample())
@@ -57,11 +66,11 @@ def run(rllib=DEFAULT_RLLIB,output_folder=DEFAULT_OUTPUT_FOLDER, gui=DEFAULT_GUI
                     env,
                     verbose=1
                     )
-        model.learn(total_timesteps=10000) # Typically not enough
+        model.learn(total_timesteps=10_000) # Typically not enough
     else:
-        ray.shutdown()
-        ray.init(ignore_reinit_error=True)
-        register_env("takeoff-aviary-v0", lambda _: TakeoffAviary())
+        # ray.shutdown()
+        # ray.init(ignore_reinit_error=True)
+        # register_env("takeoff-aviary-v0", lambda _: TakeoffAviary())
         #config = ppo.DEFAULT_CONFIG.copy()
         #config["num_workers"] = 2
         #config["framework"] = "torch"
@@ -76,9 +85,17 @@ def run(rllib=DEFAULT_RLLIB,output_folder=DEFAULT_OUTPUT_FOLDER, gui=DEFAULT_GUI
                                                                                    )
                   )
         policy = agent.get_policy()'''
-        ppo.env_name = "takeoff-aviary-v0"
-        ppo.train()
-        ray.shutdown()
+
+        trainer = ppo.PPO_Trainer(env)
+        # set everything up
+        trainer.setup_env()
+        trainer.setup_logging()
+        trainer.setup_wb()
+        # train PPO
+        agent = trainer.create_ppo()
+        agent.learn()
+
+        trainer.shutdown()
 
     #### Show (and record a video of) the model's performance ##
     env = TakeoffAviary(gui=gui,
@@ -118,10 +135,10 @@ def run(rllib=DEFAULT_RLLIB,output_folder=DEFAULT_OUTPUT_FOLDER, gui=DEFAULT_GUI
 if __name__ == "__main__":
     #### Define and parse (optional) arguments for the script ##
     parser = argparse.ArgumentParser(description='Single agent reinforcement learning example script using TakeoffAviary')
-    parser.add_argument('--rllib',      default=DEFAULT_RLLIB,        type=str2bool,       help='Whether to use RLlib PPO in place of stable-baselines A2C (default: False)', metavar='')
+    parser.add_argument('--rllib',              default=DEFAULT_RLLIB,        type=str2bool,       help='Whether to use RLlib PPO in place of stable-baselines A2C (default: False)', metavar='')
     parser.add_argument('--gui',                default=DEFAULT_GUI,       type=str2bool,      help='Whether to use PyBullet GUI (default: True)', metavar='')
     parser.add_argument('--record_video',       default=DEFAULT_RECORD_VIDEO,      type=str2bool,      help='Whether to record a video (default: False)', metavar='')
-    parser.add_argument('--output_folder',     default=DEFAULT_OUTPUT_FOLDER, type=str,           help='Folder where to save logs (default: "results")', metavar='')
+    parser.add_argument('--output_folder',      default=DEFAULT_OUTPUT_FOLDER, type=str,           help='Folder where to save logs (default: "results")', metavar='')
     parser.add_argument('--colab',              default=DEFAULT_COLAB, type=bool,           help='Whether example is being run by a notebook (default: "False")', metavar='')
     ARGS = parser.parse_args()
 
diff --git a/gym_pybullet_drones/gym_pybullet_drones/examples/ppo.py b/gym_pybullet_drones/gym_pybullet_drones/examples/ppo.py
index 3a7744a1..fa1978a5 100644
--- a/gym_pybullet_drones/gym_pybullet_drones/examples/ppo.py
+++ b/gym_pybullet_drones/gym_pybullet_drones/examples/ppo.py
@@ -1,6 +1,5 @@
 from collections import deque
 import time
-from gym_pybullet_drones.envs.single_agent_rl.TakeoffAviary import TakeoffAviary
 import torch
 from torch import nn
 from torch.optim import Adam, SGD
@@ -11,9 +10,14 @@ from datetime import datetime
 import os
 import argparse
 
+from stable_baselines3 import PPO
 # gym environment
 import gym
-from ray.tune import register_env
+
+# video logging
+from matplotlib import animation
+import matplotlib.pyplot as plt
+import numpy as np
 
 # logging python
 import logging
@@ -21,33 +25,26 @@ import sys
 
 # monitoring/logging ML
 import wandb
-from stats_logger import StatsPlotter, CSVWriter
 
 # hyperparameter tuning
 import optuna
 from optuna.integration.wandb import WeightsAndBiasesCallback
 
+# logging
+from stats_logger import CSVWriter, StatsPlotter
+
 # Paths and other constants
 MODEL_PATH = './models/'
 LOG_PATH = './log/'
 VIDEO_PATH = './video/'
 RESULTS_PATH = './results/'
 
-DEFAULT_GUI = True
-DEFAULT_RECORD_VIDEO = False
-
 # get current date and time
 CURR_DATE = datetime.today().strftime('%Y-%m-%d')
 CURR_TIME = datetime.now().strftime("%Y%m%d-%H%M%S")
 
-
-####################
-####### TODO #######
-####################
-
-# Hint: Please if working on it mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 3) Check calculation of advantage and GAE
+# config logging
+logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
 
 ####################
 ####################
@@ -57,23 +54,25 @@ class Net(nn.Module):
     def __init__(self) -> None:
         super(Net, self).__init__()
 
+
 class ValueNet(Net):
     """Setup Value Network (Critic) optimizer"""
+
     def __init__(self, in_dim, out_dim) -> None:
         super(ValueNet, self).__init__()
         self.layer1 = layer_init(nn.Linear(in_dim, 64))
         self.layer2 = layer_init(nn.Linear(64, 64))
         self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
         self.relu = nn.ReLU()
-    
+
     def forward(self, obs):
         if isinstance(obs, np.ndarray):
             obs = torch.tensor(obs, dtype=torch.float)
         x = self.relu(self.layer1(obs))
         x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
+        out = self.layer3(x)  # head has linear activation
         return out
-    
+
     def loss(self, values, returns):
         """ Objective function defined by mean-squared error.
             ValueNet is approximated via regression.
@@ -82,8 +81,10 @@ class ValueNet(Net):
         # return 0.5 * ((returns - values)**2).mean() # MSE loss
         return nn.MSELoss()(values, returns)
 
+
 class PolicyNet(Net):
     """Setup Policy Network (Actor)"""
+
     def __init__(self, in_dim, out_dim) -> None:
         super(PolicyNet, self).__init__()
         self.layer1 = layer_init(nn.Linear(in_dim, 64))
@@ -96,9 +97,9 @@ class PolicyNet(Net):
             obs = torch.tensor(obs, dtype=torch.float)
         x = self.relu(self.layer1(obs))
         x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
+        out = self.layer3(x)  # head has linear activation (continuous space)
         return out
-    
+
     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
         """ Make the clipped surrogate objective function to compute policy loss.
                 - The ratio is clipped to be close to 1. 
@@ -109,155 +110,101 @@ class PolicyNet(Net):
         # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
         ratio = torch.exp(curr_log_probs - batch_log_probs)
         clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)) # negative as Adam mins loss, but we want to max it
+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps,
+                             1.0 + clip_eps) * advantages
+        # negative as Adam mins loss, but we want to max it
+        policy_loss = (-torch.min(clip_1, clip_2))
         # calc clip frac
-        self.clip_fraction = (abs((ratio - 1.0)) > clip_eps).to(torch.float).mean()
-        return policy_loss.mean() # return mean
+        self.clip_fraction = (abs((ratio - 1.0)) >
+                              clip_eps).to(torch.float).mean()
+        return policy_loss.mean()  # return mean
 
 
 ####################
 ####################
 
-class PPO_PolicyGradient_V1:
-
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_training_steps,
-        max_batch_size,
-        n_rollout_steps,
-        noptepochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gae_lambda=0.95,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5,
-        momentum=0.9,
-        adam=True,
-        render=10,
-        save_model=10,
-        csv_writer=None,
-        stats_plotter=None,
-        log_video=False,
-        device='cpu') -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_training_steps = total_training_steps
-        self.max_batch_size = max_batch_size
-        self.n_rollout_steps = n_rollout_steps
-        self.noptepochs = noptepochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-        self.gae_lambda = gae_lambda
-        self.momentum = momentum
-        self.adam = adam
-
-        # environment
-        self.env = env
-        self.render_steps = render
-        self.save_model = save_model
-        self.device = device
-
-        # track video of gym
-        self.log_video = log_video
-
-        # keep track of rewards per episode
-        self.ep_returns = deque(maxlen=max_batch_size)
-        self.csv_writer = csv_writer
-        self.stats_plotter = stats_plotter
-        self.stats_data = {
-            'experiment': [], 
-            'timestep': [],
-            'mean episodic runtime': [],
-            'mean episodic length': [],
-            'eval episodes': [],
-            'mean episodic returns': [],
-            'min episodic returns': [],
-            'max episodic returns': [],
-            'std episodic returns': [],
-            }
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic) -  (value-based method) "How good the action taken is."
+class PPO_PolicyGradient:
+    """ Proximal Policy Optimization algorithm (PPO) (clip version)
 
-        # add optimizer for actor and critic
-        if self.adam:
-            self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-            self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer  
-        else:
-            self.policy_net_optim = SGD(self.policy_net.parameters(), lr=self.lr_p, momentum=self.momentum)
-            self.value_net_optim = SGD(self.value_net.parameters(), lr=self.lr_v, momentum=self.momentum)
+        Paper: https://arxiv.org/abs/1707.06347
+        Stable Baseline: https://github.com/hill-a/stable-baselines
 
-class PPO_PolicyGradient_V2:
-    """ Proximal Policy Optimization (PPO) is an online policy gradient method.
+        Proximal Policy Optimization (PPO) is an online policy gradient method.
         As an online policy method it updates the policy and then discards the experience (no replay buffer).
         Thus the agent does well in environments with dense reward signals.
         The clipped objective function in PPO allows to keep the policy close to the policy 
-        that was used to sample the data resulting in a more stable training. 
+        that was used to sample the data resulting in a more stable training.
+
+        :param env: The environment to learn from (if registered in Gym)
+        :param learning_rate: The learning rate, it can be a function
+        of the current progress remaining (from 1 to 0)
+        :param max_trajectory_size: Minibatch size of collected experiences
+        :param n_optepochs: Number of epoch when optimizing the surrogate loss
+        :param gamma: Discount factor
+        :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator
+        :param epsilon: Clipping parameter, it can be a function of the current progress
+        remaining (from 1 to 0).
+        :param normalize_advantage: Whether to normalize or not the advantage
+        :param normalize_returns: Whether to normalize or not the return
+        :param device: Device (cpu, cuda, ...) on which the code should be run.
+        Setting it to auto, the code will be run on the GPU if possible.
     """
     # Further reading
     # PPO experiments: https://nn.labml.ai/rl/ppo/experiment.html
     #                  https://nn.labml.ai/rl/ppo/index.html
     # PPO explained:   https://huggingface.co/blog/deep-rl-ppo
 
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_training_steps,
-        max_batch_size,
-        n_rollout_steps,
-        noptepochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gae_lambda=0.95,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5,
-        momentum=0.9,
-        adam=True,
-        render_steps=10,
-        render=False,
-        save_model=10,
-        csv_writer=None,
-        stats_plotter=None,
-        log_video=False,
-        device='cpu',
-        exp_path='./log/',
-        exp_name='PPO_V2_experiment',
-        normalize_adv=False,
-        normalize_ret=False,
-        wandb=None) -> None:
-        
+    def __init__(self,
+                 env,
+                 in_dim,
+                 out_dim,
+                 total_training_steps=2_000_000,
+                 max_trajectory_size=1024,
+                 n_rollout_steps=2048,
+                 n_optepochs=32,
+                 learning_rate_p=1e-4,
+                 learning_rate_v=1e-3,
+                 gae_lambda=0.95,
+                 gamma=0.99,
+                 epsilon=0.22,
+                 adam_eps=1e-5,
+                 momentum=0.9,
+                 adam=True,
+                 save_model=10,
+                 csv_writer=None,
+                 stats_plotter=None,
+                 log_video=False,
+                 log_video_steps=100,
+                 render_steps=10,
+                 render_video=False,
+                 device='cpu',
+                 exp_path='./log/',
+                 exp_name='PPO_V2_experiment',
+                 advantage_type='gae',
+                 normalize_adv=False,
+                 normalize_ret=False) -> None:
+
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
         self.total_training_steps = total_training_steps
-        self.max_batch_size = max_batch_size
+        self.max_trajectory_size = max_trajectory_size
         self.n_rollout_steps = n_rollout_steps
-        self.noptepochs = noptepochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
+        self.n_optepochs = n_optepochs
+        self.learning_rate_p = learning_rate_p
+        self.learning_rate_v = learning_rate_v
         self.gamma = gamma
         self.epsilon = epsilon
         self.adam_eps = adam_eps
         self.gae_lambda = gae_lambda
         self.momentum = momentum
         self.adam = adam
+        self.advantage_type = advantage_type
 
         # environment
         self.env = env
         self.render_steps = render_steps
-        self.render = render
+        self.render_video = render_video
         self.save_model = save_model
         self.device = device
         self.normalize_advantage = normalize_adv
@@ -266,16 +213,17 @@ class PPO_PolicyGradient_V2:
         # keep track of information
         self.exp_path = exp_path
         self.exp_name = exp_name
+
         # track video of gym
         self.log_video = log_video
-        self.wandb = wandb
+        self.log_video_steps = log_video_steps
 
         # keep track of rewards per episode
-        self.ep_returns = deque(maxlen=max_batch_size)
+        self.ep_returns = deque(maxlen=max_trajectory_size)
         self.csv_writer = csv_writer
         self.stats_plotter = stats_plotter
         self.stats_data = {
-            'experiment': [], 
+            'experiment': [],
             'timestep': [],
             'mean episodic runtime': [],
             'mean episodic length': [],
@@ -284,19 +232,26 @@ class PPO_PolicyGradient_V2:
             'min episodic returns': [],
             'max episodic returns': [],
             'std episodic returns': [],
-            }
+            'episodes': [],
+        }
 
         # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic) -  (value-based method) "How good the action taken is."
+        # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
+        self.policy_net = PolicyNet(self.in_dim, self.out_dim)
+        # Setup Value Network (Critic) -  (value-based method) "How good the action taken is."
+        self.value_net = ValueNet(self.in_dim, 1)
 
         # add optimizer for actor and critic
         if self.adam:
-            self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-            self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer  
+            self.policy_net_optim = Adam(self.policy_net.parameters(
+            ), lr=self.learning_rate_p, eps=self.adam_eps)  # Setup Policy Network (Actor) optimizer
+            self.value_net_optim = Adam(self.value_net.parameters(
+            ), lr=self.learning_rate_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
         else:
-            self.policy_net_optim = SGD(self.policy_net.parameters(), lr=self.lr_p, momentum=self.momentum)
-            self.value_net_optim = SGD(self.value_net.parameters(), lr=self.lr_v, momentum=self.momentum)
+            self.policy_net_optim = SGD(
+                self.policy_net.parameters(), lr=self.learning_rate_p, momentum=self.momentum)
+            self.value_net_optim = SGD(
+                self.value_net.parameters(), lr=self.learning_rate_v, momentum=self.momentum)
 
     def get_continuous_policy(self, obs):
         """Make function to compute action distribution in continuous action space."""
@@ -304,8 +259,10 @@ class PPO_PolicyGradient_V2:
         # fixes the detection of outliers, allows to capture correlation between features
         # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
         # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
+        # query Policy Network (Actor) for mean action
+        action_prob = self.policy_net(obs)
+        cov_matrix = torch.diag(torch.full(
+            size=(self.out_dim,), fill_value=0.5))
         return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
 
     def get_action(self, dist):
@@ -314,7 +271,16 @@ class PPO_PolicyGradient_V2:
         log_prob = dist.log_prob(action)
         entropy = dist.entropy()
         return action, log_prob, entropy
-    
+
+    def get_random_action(self, dist):
+        """Make random action selection."""
+        action = self.env.action_space.sample()
+        if isinstance(action, np.ndarray):
+            action = torch.tensor(action, dtype=torch.float)
+        log_prob = dist.log_prob(action)
+        entropy = dist.entropy()
+        return action, log_prob, entropy
+
     def get_values(self, obs, actions):
         """Make value selection function (outputs values for obs in a batch)."""
         values = self.value_net(obs).squeeze()
@@ -328,10 +294,15 @@ class PPO_PolicyGradient_V2:
 
     def step(self, obs):
         """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
+        action_dist = self.get_continuous_policy(obs)
         action, log_prob, entropy = self.get_action(action_dist)
         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
 
+    def random_step(self, obs):
+        action_dist = self.get_continuous_policy(obs)
+        action, log_prob, entropy = self.get_random_action(action_dist)
+        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
+
     def advantage_estimate(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
@@ -343,8 +314,9 @@ class PPO_PolicyGradient_V2:
         cum_returns = []
         for rewards in reversed(episode_rewards):  # reversed order
             for reward in reversed(rewards):
-                cum_returns.insert(0, reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+                cum_returns.insert(0, reward)  # reverse it again
+        cum_returns = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         # Step 5: Calculate advantage
@@ -354,72 +326,82 @@ class PPO_PolicyGradient_V2:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
-    def advantage_reinforce(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
-        """ Advantage Reinforce A(s_t, a_t) = G(t)
+    def advantage_reinforce(self, episode_rewards, normalized_adv=False, normalized_ret=False):
+        """ Advantage Reinforce 
+            A(s_t, a_t) = G(t)
+            - G(t) = total disounted reward
+            - Discounted return: G(t) = R(t) + gamma * R(t-1)
         """
         # Returns: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
         # Example Reinforce: https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py
-        
+
         # Step 4: Calculate returns
         # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
-        for rewards in reversed(episode_rewards): # reversed order
+        for rewards in reversed(episode_rewards):  # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
-                cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+                cum_returns.insert(0, discounted_reward)  # reverse it again
+        cum_returns = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
+        # normalize for more stability
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         # Step 5: Calculate advantage
         # A(s,a) = G(t)
-        advantages = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
         # normalize for more stability
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
     def advantage_actor_critic(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
-        """Advantage Actor-Critic by calculating delta = G(t) - V(s_t)
+        """ Advantage Actor-Critic
+            Discounted return: G(t) = R(t) + gamma * R(t-1)
+            Advantage: delta = G(t) - V(s_t)
         """
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
         # Step 4: Calculate returns
         # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
-        for rewards in reversed(episode_rewards): # reversed order
+        for rewards in reversed(episode_rewards):  # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
-                cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+                cum_returns.insert(0, discounted_reward)  # reverse it again
+        cum_returns = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
         # normalize returns
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         # Step 5: Calculate advantage
         # delta = G(t) - V(s_t)
-        advantages = cum_returns - values 
+        advantages = cum_returns - values
         # normalize advantage for more stability
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
     def advantage_TD_actor_critic(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
-        """ Advantage TD Actor-Critic A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
-            TD Error can be used as an estimator for Advantage function
+        """ Advantage TD Actor-Critic 
+            TD Error = _t = r_t +  * V(s_t+1)  V(s_t)
+            TD Error is used as an estimator for the advantage function
+            A(s,a) = r_t + (gamma * V(s_t+1)) - V(s_t)
         """
         # Step 4: Calculate returns
-        # G(t) is the total disounted reward
-        # return value: G(t) = R(t) + gamma * R(t-1)
         advantages = []
         cum_returns = []
         for rewards in reversed(episode_rewards):  # reversed order
             for reward in reversed(rewards):
-                cum_returns.insert(0, reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+                cum_returns.insert(0, reward)  # reverse it again
+        cum_returns = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         # Step 5: Calculate advantage
@@ -429,9 +411,10 @@ class PPO_PolicyGradient_V2:
             # TD residual of V with discount gamma
             # _t = r_t +  * V(s_t+1)  V(s_t)
             delta = cum_returns[i] + (self.gamma * last_values) - values[i]
-            advantages.insert(0, delta) # reverse it again
+            advantages.insert(0, delta)  # reverse it again
             last_values = values[i]
-        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(
+            np.array(advantages), device=self.device, dtype=torch.float)
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
@@ -444,15 +427,16 @@ class PPO_PolicyGradient_V2:
                 - gamma (dicount factor): allows reduce variance by downweighting rewards that correspond to delayed effects
         """
         advantages = []
-        cum_returns = []#
+        cum_returns = []
         # Step 4: Calculate returns
         for rewards in reversed(episode_rewards):  # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
-                cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+                cum_returns.insert(0, discounted_reward)  # reverse it again
+        cum_returns = torch.tensor(
+            np.array(cum_returns), device=self.device, dtype=torch.float)
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         # Step 5: Calculate advantage
@@ -466,15 +450,16 @@ class PPO_PolicyGradient_V2:
             delta = cum_returns[i] + (self.gamma * last_values) - values[i]
             # discounted sum of Bellman residual term
             # A_t = _t +  *  * A(t+1)
-            prev_advantage = delta + (self.gamma * self.gae_lambda * prev_advantage)
-            advantages.insert(0, prev_advantage) # reverse it again
+            prev_advantage = delta + \
+                (self.gamma * self.gae_lambda * prev_advantage)
+            advantages.insert(0, prev_advantage)  # reverse it again
             last_values = values[i]
-        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(
+            np.array(advantages), device=self.device, dtype=torch.float)
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
-
     def generalized_advantage_estimate_2(self, obs, next_obs, episode_rewards, dones, normalized_adv=False, normalized_ret=False):
         """ Generalized Advantage Estimate calculation
             - GAE defines advantage as a weighted average of A_t
@@ -510,24 +495,28 @@ class PPO_PolicyGradient_V2:
             advantages = self.normalize_adv(advantages)
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
-        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
-        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(
+            np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(
+            np.array(returns), device=self.device, dtype=torch.float)
         return advantages, returns
 
     def normalize_adv(self, advantages):
         return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-    
+
     def normalize_ret(self, returns):
         eps = np.finfo(np.float32).eps.item()
         return (returns - returns.mean()) / (returns.std() + eps)
 
     def finish_episode(self):
-        pass 
+        pass
 
-    def collect_rollout(self, n_steps=1, render=False):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-        
-        t_step, rewards = 0, []
+    def collect_rollout(self, n_steps=1):
+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)
+           # TODO: rollout length - 2048 t0 4096
+        """
+
+        t_step, rewards, frames = 0, [], deque(maxlen=24)  # 4 fps - 6 sec
 
         # log time
         episode_time = []
@@ -542,11 +531,11 @@ class PPO_PolicyGradient_V2:
         episode_lens = []
 
         # Run Monte Carlo simulation for n timesteps per batch
-        logging.info("Collecting batch trajectories...")
+        logging.info(f"Collecting trajectories in batch...")
         while t_step < n_steps:
-            
+
             # rewards collected
-            rewards, done = [], False 
+            rewards, done, frames = [], False, []
             obs = self.env.reset()
 
             # measure time elapsed for one episode
@@ -555,18 +544,21 @@ class PPO_PolicyGradient_V2:
 
             # Run episode for a fixed amount of timesteps
             # to keep rollout size fixed and episodes independent
-            for t_episode in range(0, self.max_batch_size):
+            for t_batch in range(0, self.max_trajectory_size):
                 # render gym envs
-                if render and t_episode % self.render_steps == 0:
-                    self.env.render()
-                
-                t_step += 1 
+                if self.render_video and t_batch % self.render_steps == 0:
+                    frames.append(self.env.render(mode="rgb_array"))
+
+                t_step += 1
 
-                # action logic 
+                # action logic
                 # sampled via policy which defines behavioral strategy of an agent
                 action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
+
+                # Perform action logic at random
+                # action, log_probability, _ = self.random_step(obs)
+
+                # STEP 3: collecting set of trajectories D_k by running action
                 # that was sampled from policy in environment
                 __obs, reward, done, truncated = self.env.step(action)
 
@@ -577,13 +569,13 @@ class PPO_PolicyGradient_V2:
                 episode_action_probs.append(log_probability)
                 rewards.append(reward)
                 episode_dones.append(done)
-                    
+
                 obs = __obs
 
                 # break out of loop if episode is terminated
                 if done or truncated:
                     break
-            
+
             # stop time per episode
             # Waits for everything to finish running
             # torch.cuda.synchronize()
@@ -591,30 +583,35 @@ class PPO_PolicyGradient_V2:
             time_elapsed = end_epoch - start_epoch
             episode_time.append(time_elapsed)
 
-            episode_lens.append(t_episode + 1) # as we started at 0
+            episode_lens.append(t_batch + 1)  # as we started at 0
             episode_rewards.append(rewards)
 
         # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(episode_obs), device=self.device, dtype=torch.float)
-        next_obs = torch.tensor(np.array(episode_nextobs), device=self.device, dtype=torch.float)
-        actions = torch.tensor(np.array(episode_actions), device=self.device, dtype=torch.float)
-        action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
-        dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
-
-        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time)
-                
+        obs = torch.tensor(np.array(episode_obs),
+                           device=self.device, dtype=torch.float)
+        next_obs = torch.tensor(np.array(episode_nextobs),
+                                device=self.device, dtype=torch.float)
+        actions = torch.tensor(np.array(episode_actions),
+                               device=self.device, dtype=torch.float)
+        action_log_probs = torch.tensor(
+            np.array(episode_action_probs), device=self.device, dtype=torch.float)
+        dones = torch.tensor(np.array(episode_dones),
+                             device=self.device, dtype=torch.float)
+
+        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time), frames
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
         """Calculate loss and update weights of both networks."""
         logging.info("Updating network parameter...")
         # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
+        self.policy_net_optim.zero_grad()  # reset optimizer
+        policy_loss = self.policy_net.loss(
+            advantages, batch_log_probs, curr_log_probs, epsilon)
+        policy_loss.backward()  # backpropagation
+        self.policy_net_optim.step()  # single optimization step (updates parameter)
 
         # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
+        self.value_net_optim.zero_grad()  # reset optimizer
         value_loss = self.value_net.loss(values, returns)
         value_loss.backward()
         self.value_net_optim.step()
@@ -624,59 +621,80 @@ class PPO_PolicyGradient_V2:
     def learn(self):
         """"""
         training_steps = 0
-        
+        done_so_far = 0
+        adv_func = self.advantage_type
         while training_steps < self.total_training_steps:
             policy_losses, value_losses = [], []
 
             # Collect data over one episode
+            # Episode = recording of actions and states that an agent performed from a start state to an end state
             # STEP 3: simulate and collect trajectories --> the following values are all per batch over one episode
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.n_rollout_steps, render=self.render)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time, frames = self.collect_rollout(
+                n_steps=self.n_rollout_steps)
 
-            # timesteps simulated so far for batch collection
+            # experiences simulated so far
             training_steps += np.sum(ep_lens)
 
             # STEP 4-5: Calculate cummulated reward and advantage at timestep t_step
-            values, _ , _ = self.get_values(obs, actions)
-            # advantages, cum_returns = self.advantage_reinforce(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            # advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            # advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            
+            values, _, _ = self.get_values(obs, actions)
             advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             
-            # update network params 
-            for _ in range(self.noptepochs):
+            # Calculate advantage function
+            # match adv_func:
+            #         case 'gae':
+            #             advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(
+            #             ), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            #         case 'ac':
+            #             advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(
+            #             ), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            #         case 'td-ac':
+            #             advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(
+            #             ), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            #         case 'reinforce':
+            #             advantages, cum_returns = self.advantage_reinforce(
+            #                 rewards, normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+
+            # update network params
+            for _ in range(self.n_optepochs):
                 # STEP 6-7: calculate loss and update weights
                 values, curr_log_probs, _ = self.get_values(obs, actions)
-                policy_loss, value_loss = self.train(values, cum_returns, advantages, batch_log_probs, curr_log_probs, self.epsilon)
-                
+                policy_loss, value_loss = self.train(
+                    values, cum_returns, advantages, batch_log_probs, curr_log_probs, self.epsilon)
+
                 policy_losses.append(policy_loss.detach().numpy())
                 value_losses.append(value_loss.detach().numpy())
 
             # log all statistical values to CSV
-            self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time, exp_name=self.exp_name, wandb=self.wandb)
+            self.log_stats(policy_losses, value_losses, rewards, ep_lens,
+                           training_steps, ep_time, done_so_far, exp_name=self.exp_name)
+
+            # increment for each iteration
+            done_so_far += 1
 
             # store model with checkpoints
             if training_steps % self.save_model == 0:
                 env_name = self.env.unwrapped.spec.id
                 env_model_path = os.path.join(self.exp_path, 'models')
-                policy_net_name = os.path.join(env_model_path, f'{env_name}_policyNet.pth')
-                value_net_name = os.path.join(env_model_path, f'{env_name}_valueNet.pth')
+                policy_net_name = os.path.join(
+                    env_model_path, f'{env_name}_policyNet.pth')
+                value_net_name = os.path.join(
+                    env_model_path, f'{env_name}_valueNet.pth')
                 torch.save({
                     'epoch': training_steps,
                     'model_state_dict': self.policy_net.state_dict(),
                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
                     'loss': policy_loss,
-                    }, policy_net_name)
+                }, policy_net_name)
                 torch.save({
                     'epoch': training_steps,
                     'model_state_dict': self.value_net.state_dict(),
                     'optimizer_state_dict': self.value_net_optim.state_dict(),
                     'loss': value_loss,
-                    }, value_net_name)
+                }, value_net_name)
 
-                if self.wandb:
-                    self.wandb.save(policy_net_name)
-                    self.wandb.save(value_net_name)
+                if wandb:
+                    wandb.save(policy_net_name)
+                    wandb.save(value_net_name)
 
                 # Log to CSV
                 if self.csv_writer:
@@ -684,24 +702,48 @@ class PPO_PolicyGradient_V2:
                     for value in self.stats_data.values():
                         del value[:]
 
+                # Log to video
+                if self.render_video and done_so_far % self.log_video_steps == 0:
+                    filename = 'pendulum_v1.gif'
+                    self.save_frames_as_gif(frames, self.exp_path, filename)
+                    wandb.log({
+                        "train/video": wandb.Video(os.path.join(self.exp_path, filename),
+                                                   caption='episode: ' +
+                                                   str(done_so_far),
+                                                   fps=4, format="gif"), "step": done_so_far
+                    })
+
         # Finalize and plot stats
         if self.stats_plotter:
-            df = self.stats_plotter.read_csv() # read all files in folder
-            self.stats_plotter.plot_seaborn_fill(df, x='timestep', y='mean episodic returns', 
-                                                y_min='min episodic returns', y_max='max episodic returns',  
-                                                title=f'{env_name}', x_label='Timestep', y_label='Mean Episodic Return', 
-                                                color='blue', smoothing=6, wandb=wandb, xlim_up=self.total_training_steps)
-
-            # self.stats_plotter.plot_box(df, x='timestep', y='mean episodic runtime', 
-            #                             title='title', x_label='Timestep', y_label='Mean Episodic Time', wandb=wandb)
-        if self.wandb:
+            df = self.stats_plotter.read_csv()  # read all files in folder
+            self.stats_plotter.plot_seaborn_fill(df, x='timestep', y='mean episodic returns',
+                                                 y_min='min episodic returns', y_max='max episodic returns',
+                                                 title=f'{env_name}', x_label='Timestep', y_label='Mean Episodic Return',
+                                                 color='blue', smoothing=6, wandb=wandb, xlim_up=self.total_training_steps)
+        if wandb:
             # save files in path
             wandb.save(os.path.join(self.exp_path, "*csv"))
             # Save any files starting with "ppo"
             wandb.save(os.path.join(wandb.run.dir, "ppo*"))
 
+    def save_frames_as_gif(self, frames, path='./', filename='pendulum_v1.gif'):
 
-    def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, exp_name='experiment', wandb=None):
+        # Mess with this to change frame size
+        plt.figure(figsize=(frames[0].shape[1] / 72.0,
+                   frames[0].shape[0] / 72.0), dpi=72)
+
+        patch = plt.imshow(frames[0])
+        plt.axis('off')
+
+        def animate(i):
+            patch.set_data(frames[i])
+
+        anim = animation.FuncAnimation(
+            plt.gcf(), animate, frames=len(frames), interval=50)
+        video_path = os.path.join(path, filename)
+        anim.save(video_path, writer='imagemagick', fps=60)
+
+    def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, done_so_far, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
         if torch.is_tensor(batch_return):
             batch_return = batch_return.detach().numpy()
@@ -711,7 +753,7 @@ class PPO_PolicyGradient_V2:
 
         # Calculate the stats of an episode
         cum_ret = [np.sum(ep_rews) for ep_rews in batch_return]
-        mean_ep_time = round(np.mean(time), 6) 
+        mean_ep_time = round(np.mean(time), 6)
         mean_ep_len = round(np.mean(episode_lens), 6)
 
         # statistical values for return
@@ -723,6 +765,7 @@ class PPO_PolicyGradient_V2:
         std_ep_rew = round(np.std(cum_ret), 6)
 
         # Log stats to CSV file
+        self.stats_data['episodes'].append(done_so_far)
         self.stats_data['experiment'].append(exp_name)
         self.stats_data['mean episodic length'].append(mean_ep_len)
         self.stats_data['mean episodic returns'].append(mean_ep_ret)
@@ -734,16 +777,18 @@ class PPO_PolicyGradient_V2:
         self.stats_data['timestep'].append(training_steps)
 
         # Monitoring via W&B
-        if wandb:
-            wandb.log({
-                'train/timesteps': training_steps,
-                'train/mean policy loss': mean_p_loss,
-                'train/mean value loss': mean_v_loss,
-                'train/mean episode returns': mean_ep_ret,
-                'train/std episode returns': std_ep_rew,
-                'train/mean episode runtime': mean_ep_time,
-                'train/mean episode length': mean_ep_len
-            })
+        wandb.log({
+            'train/timesteps': training_steps,
+            'train/mean policy loss': mean_p_loss,
+            'train/mean value loss': mean_v_loss,
+            'train/mean episode returns': mean_ep_ret,
+            'train/min episode returns': min_ep_ret,
+            'train/max episode returns': max_ep_ret,
+            'train/std episode returns': std_ep_rew,
+            'train/mean episode runtime': mean_ep_time,
+            'train/mean episode length': mean_ep_len,
+            'train/episodes': done_so_far,
+        })
 
         logging.info('\n')
         logging.info(f'------------ Episode: {training_steps} --------------')
@@ -753,6 +798,7 @@ class PPO_PolicyGradient_V2:
         logging.info('--------------------------------------------')
         logging.info('\n')
 
+
 ####################
 ####################
 
@@ -772,14 +818,15 @@ def arg_parser():
     parser.add_argument("--total-timesteps", type=int, default=2000000, help="total timesteps of the experiments")
     parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, `torch.backends.cudnn.deterministic=False`")
     parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, cuda will be enabled by default")
-    
+
     # Parse arguments if they are given
     args = parser.parse_args()
     return args
 
-def make_env(env_id='Pendulum-v1', gym_wrappers=False, gui=DEFAULT_GUI, record_video=DEFAULT_RECORD_VIDEO, seed=42):
+def make_env(env_id='Pendulum-v1', gym_wrappers=False, seed=42):
     # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)                
+    env = gym.make(env_id)
+
     # gym wrapper
     if gym_wrappers:
         env = gym.wrappers.ClipAction(env)
@@ -787,17 +834,13 @@ def make_env(env_id='Pendulum-v1', gym_wrappers=False, gui=DEFAULT_GUI, record_v
         env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
         env = gym.wrappers.NormalizeReward(env)
         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    
+
     # seed env for reproducability
     env.seed(seed)
     env.action_space.seed(seed)
     env.observation_space.seed(seed)
     return env
 
-def make_vec_env(num_env=1):
-    """ Create a vectorized environment for parallelized training."""
-    pass
-
 def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
     """ Initialize the hidden layers with orthogonal initialization
         Engstrom, Ilyas, et al., (2020)
@@ -815,350 +858,186 @@ def load_model(path, model, device='cpu'):
     model.load_state_dict(checkpoint['model_state_dict'], map_location=device)
     return model
 
-def simulate_rollout(policy_net, env, render=True):
-    # Rollout until user kills process
-	while True:
-		obs = env.reset()
-		done = False
-
-		# number of timesteps so far
-		t = 0
-
-		# Logging data
-		ep_len = 0            # episodic length
-		ep_ret = 0            # episodic return
-
-		while not done:
-			t += 1
-
-			# Render environment if specified, off by default
-			if render:
-				env.render()
-
-			# Query deterministic action from policy and run it
-			action = policy_net(obs).detach().numpy()
-			obs, rew, done, _ = env.step(action)
-
-			# Sum all episodic rewards as we go along
-			ep_ret += rew
-			
-		# Track episodic length
-		ep_len = t
 
-		# returns episodic length and return in this iteration
-		yield ep_len, ep_ret
+####################
+####################
 
-def _log_summary(ep_len, ep_ret, ep_num):
+class PPO_Trainer:
 
-        # Monitoring via W&B
-        wandb.log({
-            'test/timesteps': ep_num,
-            'test/episode length': ep_len,
-            'test/episode returns': ep_ret
-        })
+    def __init__(self, 
+                env,
+                total_training_steps=10_000,
+                max_trajectory_size=1024,
+                n_rollout_steps=2048,
+                n_optepochs=32,
+                learning_rate_p=1e-4,
+                learning_rate_v=1e-3,
+                gae_lambda=0.95,
+                gamma=0.99,
+                epsilon=0.22,
+                adam_eps=1e-5,
+                momentum=0.9,
+                adam=True,
+                save_model=10,
+                log_video=False,
+                log_video_steps=100,
+                render_steps=10,
+                render_video=False,
+                device='cpu',
+                advantage_type='gae',
+                normalize_adv=False,
+                normalize_ret=True,
+                deterministic=True, 
+                seed=42, 
+                project_name='PyBulletGym-Drone') -> None:
 
-		# Print logging statements
-        logging.info('\n')
-        logging.info(f'------------ Episode: {ep_num} --------------')
-        logging.info(f"Episodic Length: {ep_len}")
-        logging.info(f"Episodic Return: {ep_ret}")
-        logging.info(f"--------------------------------------------")
-        logging.info('\n')
+        self.env = env
+        self.env_name = self.env.unwrapped.spec.id
 
-def train(env, in_dim, out_dim, total_training_steps, max_batch_size, n_rollout_steps,
-          noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
-          adam_epsilon, render_steps, render, save_steps, csv_writer, stats_plotter,
-          normalize_adv=False, normalize_ret=False, log_video=False, ppo_version='v2', 
-          device='cpu', exp_path='./log/', exp_name='PPO-experiment', wandb=None):
-    """Train the policy network (actor) and the value network (critic) with PPO"""
-    agent = None
-    if ppo_version == 'v2':
-        agent = PPO_PolicyGradient_V2(
-                    env, 
-                    in_dim=in_dim, 
-                    out_dim=out_dim,
-                    total_training_steps=total_training_steps,
-                    max_batch_size=max_batch_size,
-                    n_rollout_steps=n_rollout_steps,
-                    noptepochs=noptepochs,
-                    lr_p=learning_rate_p,
-                    lr_v=learning_rate_v,
-                    gae_lambda = gae_lambda,
-                    gamma=gamma,
-                    epsilon=epsilon,
-                    adam_eps=adam_epsilon,
-                    normalize_adv=normalize_adv,
-                    normalize_ret=normalize_ret,
-                    render_steps=render_steps,
-                    render=render,
-                    save_model=save_steps,
-                    csv_writer=csv_writer,
-                    stats_plotter=stats_plotter,
-                    log_video=log_video,
-                    device=device,
-                    exp_path=exp_path,
-                    exp_name=exp_name,
-                    wandb=wandb)
-    else:
-        agent = PPO_PolicyGradient_V1(
-                    env, 
-                    in_dim=in_dim, 
-                    out_dim=out_dim,
-                    total_training_steps=total_training_steps,
-                    max_batch_size=max_batch_size,
-                    n_rollout_steps=n_rollout_steps,
-                    noptepochs=noptepochs,
-                    lr_p=learning_rate_p,
-                    lr_v=learning_rate_v,
-                    gae_lambda = gae_lambda,
-                    gamma=gamma,
-                    epsilon=epsilon,
-                    adam_eps=adam_epsilon,
-                    render=render_steps,
-                    save_model=save_steps,
-                    csv_writer=csv_writer,
-                    stats_plotter=stats_plotter,
-                    log_video=log_video,
-                    device=device)
-    # run training for a total amount of steps
-    agent.learn()
-
-def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False, device='cpu'):
-    """Test the policy network (actor)"""
-    # load model and test it
-    policy_net = PolicyNet(in_dim, out_dim)
-    policy_net = load_model(path, policy_net, device)
-    
-    for ep_num, (ep_len, ep_ret) in enumerate(simulate_rollout(policy_net, env, render)):
-        _log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)
+        # hyperparam
+        self.total_training_steps = total_training_steps
+        self.max_trajectory_size = max_trajectory_size
+        self.n_rollout_steps = n_rollout_steps
+        self.n_optepochs = n_optepochs
+        self.learning_rate_p = learning_rate_p
+        self.learning_rate_v =learning_rate_v
+        self.gae_lambda = gae_lambda
+        self.gamma = gamma
+        self.epsilon = epsilon
+        self.adam_eps = adam_eps
+        self.momentum = momentum
+        self.adam = adam
+        self.advantage_type = advantage_type
+        self.normalize_advantage = normalize_adv
+        self.normalize_return = normalize_ret
+        
+        # experiment
+        self.exp_name = f"exp_name: {self.env_name}_{CURR_DATE}"
 
-        if log_video:
-            wandb.log({"test/video": wandb.Video(VIDEO_PATH, caption='episode: '+str(ep_num), fps=4, format="gif"), "step": ep_num})
+        # logging
+        self.project_name = project_name
+        self.save_model = save_model
+        self.render_video = render_video
+        self.render_steps = render_steps
+        self.log_video = log_video
+        self.log_video_steps = log_video_steps
+        
+        # other
+        self.deterministic = deterministic,
+        self.seed = seed
+        self.device = device
 
-def hyperparam_tuning(config=None):
-    # set config
-    with wandb.init(config=config):
-        agent = PPO_PolicyGradient_V2(
-                env,
-                in_dim=config.obs_dim, 
-                out_dim=config.act_dim,
-                total_training_steps=config.total_training_steps,
-                max_batch_size=config.max_batch_size,
-                n_rollout_steps=config.n_rollout_steps,
-                noptepochs=config.noptepochs,
-                gae_lambda = config.gae_lambda,
-                gamma=config.gamma,
-                epsilon=config.epsilon,
-                adam_eps=config.adam_epsilon,
-                lr_p=config.learning_rate_p,
-                lr_v=config.learning_rate_v)
-    
-        # run training for a total amount of steps
-        agent.learn()
+    def setup_logging(self):
+        self.exp_dir = f'{LOG_PATH}exp_{self.env_name}_{CURR_TIME}'
+        self.model_dir = os.path.join(self.exp_dir, 'models')
 
+        if not os.path.exists(self.exp_dir):
+            os.makedirs(self.exp_dir)
+        if not os.path.exists(self.model_dir):
+            os.makedirs(self.model_dir)
 
-if __name__ == '__main__':
-    
-    """ Classic control gym environments 
-        Find docu: https://www.gymlibrary.dev/environments/classic_control/
-    """
-    # parse arguments
-    args = arg_parser()
-
-    # check if path exists otherwise create
-    if args.video:
-        create_path(VIDEO_PATH)
-    create_path(LOG_PATH)
-    create_path(RESULTS_PATH)
-    
-    # Hyperparameter
-    total_training_steps = 3_000_000     # time steps regarding batches collected and train agent
-    max_batch_size = 512                 # max number of episode samples to be sampled per time step. 
-    n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
-    noptepochs = 32                      # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4               # learning rate for policy network
-    learning_rate_v = 1e-3               # learning rate for value network
-    gae_lambda = 0.95                    # factor for trade-off of bias vs variance for GAE
-    gamma = 0.99                         # discount factor
-    adam_epsilon = 1e-8                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
-    epsilon = 0.2                        # clipping factor
-    clip_range_vf = 0.2                  # clipping factor for the value loss function. Depends on reward scaling.
-    env_name = 'takeoff-aviary-v0'       # name of OpenAI gym environment other: 'takeoff-aviary-v0'
-    env_number = 1                       # number of actors
-    seed = 42                            # seed gym, env, torch, numpy 
-    normalize_adv = False                # wether to normalize the advantage estimate
-    normalize_ret = True                 # wether to normalize the return function
-    
-    # setup for torch save models and rendering
-    render = False
-    render_steps = 10
-    save_steps = 100
+        csv_file = os.path.join(self.exp_dir, f'{self.env_name}_{CURR_TIME}.csv')
+        png_file = os.path.join(self.exp_dir, f'{self.env_name}_{CURR_TIME}.png')
 
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-    
-    upper_bound = env.action_space.high[0]
-    lower_bound = env.action_space.low[0]
+        self.csv_writer = CSVWriter(csv_file)
+        self.stats_plotter = StatsPlotter(self.exp_dir, file_name_and_path=png_file)
 
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    logging.info(f'env action upper bound: {upper_bound}')
-    logging.info(f'env action lower bound: {lower_bound}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
+    def setup_wb(self):
 
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-
-    # create folder for project
-    exp_name = f"exp_name: {env_name}_{CURR_DATE} {args.exp_name}"
-    exp_folder_name = f'{LOG_PATH}exp_{env_name}_{CURR_TIME}'
-    create_path(exp_folder_name)
-    # create model folder
-    model_path = os.path.join(exp_folder_name, 'models')
-    create_path(model_path)
-
-    # create CSV writer
-    csv_file = os.path.join(exp_folder_name, f'{env_name}_{CURR_TIME}.csv')
-    png_file = os.path.join(exp_folder_name, f'{env_name}_{CURR_TIME}.png')
-    csv_writer = CSVWriter(csv_file)
-    stats_plotter = StatsPlotter(csv_folder_path=exp_folder_name, file_name_and_path=png_file)
-
-    # Monitoring with W&B
-    wandb.init(
-            project=args.project_name,
+        wandb.init(
+            project=self.project_name,
             entity='drone-mechanics',
             sync_tensorboard=True,
             config={ # stores hyperparams in job
-                'env name': env_name,
-                'env number': env_number,
-                'total number of steps': total_training_steps,
-                'max sampled trajectories': max_batch_size,
-                'batches per episode': n_rollout_steps,
-                'number of epochs for update': noptepochs,
-                'input layer size': obs_dim,
-                'output layer size': act_dim,
-                'observation space': obs_shape,
-                'action space': act_shape,
-                'action space upper bound': upper_bound,
-                'action space lower bound': lower_bound,
-                'learning rate (policy net)': learning_rate_p,
-                'learning rate (value net)': learning_rate_v,
-                'epsilon (adam optimizer)': adam_epsilon,
-                'gamma (discount)': gamma,
-                'epsilon (clipping)': epsilon,
-                'gae lambda (GAE)': gae_lambda,
-                'normalize advantage': normalize_adv,
-                'normalize return': normalize_ret,
-                'seed': seed,
-                'experiment path': exp_folder_name,
-                'experiment name': args.exp_name
-            },
-            dir=os.getcwd(),
-            name=exp_name, # needs flag --exp-name
-            monitor_gym=True,
-            save_code=True
-        )
-
-    if args.train:
-        logging.info('Training model...')
-        train(env,
-            in_dim=obs_dim, 
-            out_dim=act_dim,
-            total_training_steps=total_training_steps,
-            max_batch_size=max_batch_size,
-            n_rollout_steps=n_rollout_steps,
-            noptepochs=noptepochs,
-            learning_rate_p=learning_rate_p, 
-            learning_rate_v=learning_rate_v,
-            gae_lambda = gae_lambda,
-            gamma=gamma,
-            epsilon=epsilon,
-            adam_epsilon=adam_epsilon,
-            normalize_adv=normalize_adv,
-            normalize_ret=normalize_ret,
-            render_steps=render_steps,
-            render=render,
-            save_steps=save_steps,
-            csv_writer=csv_writer,
-            stats_plotter=stats_plotter,
-            log_video=args.video,
-            device=device,
-            exp_path=exp_folder_name,
-            exp_name=exp_name,
-            wandb=wandb)
-    
-    elif args.test:
-        logging.info('Evaluation model...')
-        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth' # TODO: define path
-        test(PATH, env, in_dim=obs_dim, out_dim=act_dim, device=device)
+                    'env name': self.env_name,
+                    'env number': 1, # only single env
+                    'total_training_steps': self.total_training_steps,
+                    'max sampled trajectories': self.max_trajectory_size,
+                    'batches per episode': self.n_rollout_steps,
+                    'number of epochs for update': self.n_optepochs,
+                    'input layer size': self.obs_dim,
+                    'output layer size': self.act_dim,
+                    'observation space': self.obs_shape,
+                    'action space': self.act_shape,
+                    'action space upper bound': self.upper_bound,
+                    'action space lower bound': self.lower_bound,
+                    'learning rate (policy net)': self.learning_rate_p,
+                    'learning rate (value net)': self.learning_rate_v,
+                    'epsilon (adam optimizer)': self.adam_eps,
+                    'gamma (discount)': self.gamma,
+                    'epsilon (clip_range)': self.epsilon,
+                    'gae lambda (GAE)': self.gae_lambda,
+                    'normalize advantage': self.normalize_advantage,
+                    'normalize return': self.normalize_return,
+                    'seed': self.seed,
+                    'experiment path': self.exp_dir,
+                    'experiment name': self.exp_name
+                },
+                dir=os.getcwd(),
+                name=self.exp_name,
+                monitor_gym=True,
+                save_code=True
+            )
+
+    def setup_env(self):
+        # seeding
+        torch.manual_seed(self.seed)
+        np.random.seed(self.seed)
+        torch.backends.cudnn.deterministic = True # self.deterministic
+
+        # get dimensions of obs (what goes in?)
+        # and actions (what goes out?)
+        self.obs_shape = self.env.observation_space.shape
+        self.act_shape = self.env.action_space.shape
+
+        self.upper_bound = self.env.action_space.high[0]
+        self.lower_bound = self.env.action_space.low[0]
+
+        logging.info(f'env observation space: {self.obs_shape}')
+        logging.info(f'env action space: {self.act_shape}')
+        logging.info(f'env action upper bound: {self.upper_bound}')
+        logging.info(f'env action lower bound: {self.lower_bound}')
+
+        self.obs_dim = self.obs_shape[0] 
+        self.act_dim = self.act_shape[0]
+
+        logging.info(f'env observation dim: {self.obs_dim}')
+        logging.info(f'env action dim: {self.act_dim}')
+
+    def create_ppo(self):
+        agent = PPO_PolicyGradient(self.env,
+                self.obs_dim,
+                self.act_dim,
+                total_training_steps=self.total_training_steps,
+                max_trajectory_size=self.max_trajectory_size,
+                n_rollout_steps=self.n_rollout_steps,
+                n_optepochs=self.n_optepochs,
+                learning_rate_p=self.learning_rate_p,
+                learning_rate_v=self.learning_rate_v,
+                gae_lambda=self.gae_lambda,
+                gamma=self.gamma,
+                epsilon=self.epsilon,
+                adam_eps=self.adam_eps,
+                momentum=self.momentum,
+                adam=self.adam,
+                save_model=self.save_model,
+                csv_writer=self.csv_writer,
+                stats_plotter=self.stats_plotter,
+                log_video=self.log_video,
+                log_video_steps=self.log_video_steps,
+                render_steps=self.render_steps,
+                render_video=self.render_video,
+                device=self.device,
+                exp_path=self.exp_dir,
+                exp_name=self.exp_name,
+                advantage_type=self.advantage_type,
+                normalize_adv=self.normalize_advantage,
+                normalize_ret=self.normalize_return)
+        return agent
     
-    elif args.hyperparam:
-        # hyperparameter tuning with sweeps
-
-        logging.info('Hyperparameter tuning...')
-        # sweep config
-        sweep_config = {
-            'method': 'bayes'
-            }
-        metric = {
-            'name': 'mean_ep_rews',
-            'goal': 'maximize'   
-            }
-        parameters_dict = {
-            'learning_rate_p': {
-                'values': [1e-5, 1e-4, 1e-3, 1e-2]
-            },
-            'learning_rate_v': {
-                'values': [1e-5, 1e-4, 1e-3, 1e-2]
-            },
-            'gamma': {
-                'values': [0.95, 0.96, 0.97, 0.98, 0.99]
-            },
-            'adam_epsilon': {
-                'values': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
-            },
-            'epsilon': {
-                'values': [0.1, 0.2, 0.25, 0.3, 0.4]
-            },
-            }
-
-        # set defined parameters
-        sweep_config['parameters'] = parameters_dict
-        sweep_config['metric'] = metric
-
-        # run sweep with sweep controller
-        sweep_id = wandb.sweep(sweep_config, project="ppo-OpenAIGym-hyperparam-tuning")
-        wandb.agent(sweep_id, hyperparam_tuning, count=5)
-
-    else:
-        assert("Needs training (--train), testing (--test) or hyperparameter tuning (--hyperparam) flag set!")
-
-    #################
-    #### Cleanup ####
-    #################
-
-    logging.info('### Done ###')
-
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
+    def shutdown(self):
+        # cleanup 
+        self.env.close()
+        wandb.run.finish() if wandb and wandb.run else None
+
+        
