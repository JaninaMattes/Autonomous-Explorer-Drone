wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.13.6
    code_path: code/PPO/ppo_torch/ppo_discrete.py
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.10.8
    start_time: 1671547518.687156
    t:
      1:
      - 1
      - 55
      2:
      - 1
      - 55
      3:
      - 13
      - 16
      - 23
      - 35
      4: 3.10.8
      5: 0.13.6
      8:
      - 4
      - 5
batches per episode:
  desc: null
  value: 4600
epsilon (adam optimizer):
  desc: null
  value: 1.0e-08
epsilon (clipping):
  desc: null
  value: 0.2
gamma (discount):
  desc: null
  value: 0.99
input layer size:
  desc: null
  value: 8
learning rate (policy net):
  desc: null
  value: 0.0001
learning rate (value net):
  desc: null
  value: 0.001
max sampled trajectories:
  desc: null
  value: 1000
number of epochs for update:
  desc: null
  value: 5
output layer size:
  desc: null
  value: 4
seerd:
  desc: null
  value: 42
total number of steps:
  desc: null
  value: 30000000
