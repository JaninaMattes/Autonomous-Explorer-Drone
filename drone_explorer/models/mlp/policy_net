# models/mlp/policy_net.py

import torch
from torch import nn

import numpy as np

from drone_explorer.models.base.network import Net
from drone_explorer.utils.utility import layer_init


class PolicyNet(Net):
    """Setup Policy Network (Actor)"""

    def __init__(self, in_dim, out_dim) -> None:
        super(PolicyNet, self).__init__()
        self.layer1 = layer_init(nn.Linear(in_dim, 64))
        self.layer2 = layer_init(nn.Linear(64, 64))
        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
        self.relu = nn.ReLU()


    def forward(self, obs):
        if isinstance(obs, np.ndarray):
            obs = torch.tensor(obs, dtype=torch.float)
        x = self.relu(self.layer1(obs))
        x = self.relu(self.layer2(x))
        out = self.layer3(x)                                    # head has linear activation (continuous space)
        return out


    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
        """ Make the clipped surrogate objective function to compute policy loss.
                - The ratio is clipped to be close to 1. 
                - The clipping ensures that the update will not be too large so that training is more stable.
                - The minimum is taken, so that the gradient will pull π_new towards π_OLD 
                  if the ratio is not between 1-ϵ and 1+ϵ.
        """
        ratio = torch.exp(curr_log_probs - batch_log_probs)     # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
        clip_1 = ratio * advantages
        clip_2 = torch.clamp(ratio, 1.0 - clip_eps,
                             1.0 + clip_eps) * advantages
        policy_loss = (-torch.min(clip_1, clip_2))              # negative as Adam mins loss, but we want to max it
        self.clip_fraction = (abs((ratio - 1.0)) >              # calc clip frac
                              clip_eps).to(torch.float).mean()
        return policy_loss.mean()                               # return mean
