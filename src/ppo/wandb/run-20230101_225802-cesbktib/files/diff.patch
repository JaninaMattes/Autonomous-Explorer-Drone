diff --git a/.gitignore b/.gitignore
index 2d5f4324..b9ee61da 100644
--- a/.gitignore
+++ b/.gitignore
@@ -37,7 +37,7 @@ sysinfo.txt
 
 # Monitoring
 */wandb/*
-ppo_impl/ppo_torch/wandb/*
+PPO/ppo_torch/wandb/
 
 # Checkpoints
 # IPython notebook checkpoints
diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index 617da7b1..8a7de900 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -224,6 +224,7 @@ class PPO_PolicyGradient:
 
         advantages = []
         returns = []
+        values = values.detach().numpy()
         for rewards in reversed(batch_rewards): # reversed order
             prev_advantage = 0
             discounted_reward = 0
@@ -248,10 +249,12 @@ class PPO_PolicyGradient:
                 advantages.insert(0, prev_advantage) # reverse it again
                 # store current value as V(s_t+1)
                 last_value = values[i]
-        advantages = np.array(advantages)
+        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        returns = torch.tensor(np.array(returns), dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
+            returns = self.normalize_ret(returns)
+        return advantages, returns
 
 
     def advantage_estimate(self, next_obs, obs, batch_rewards, dones, normalized=True):
@@ -322,7 +325,10 @@ class PPO_PolicyGradient:
 
     def normalize_adv(self, advantages):
         return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        
+    
+    def normalize_ret(self, returns):
+        return (returns - returns.mean()) / returns.std()
+
     def finish_episode(self):
         pass 
 
@@ -425,9 +431,9 @@ class PPO_PolicyGradient:
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, curr_log_probs , _ = self.get_values(obs, actions)
-            # cum_returns = self.cummulative_return(batch_rewards)
+            cum_returns = self.cummulative_return(batch_rewards)
             # advantages = self.advantage_estimate(cum_returns, values.detach())
-            advantages, cum_returns = self.generalized_advantage_estimate_3(batch_rewards, values, dones)
+            advantages, _ = self.generalized_advantage_estimate_3(batch_rewards, values, dones)
             #advantages = self.generalized_advantage_estimate_1(cum_returns, values)
 
             # update network params 
