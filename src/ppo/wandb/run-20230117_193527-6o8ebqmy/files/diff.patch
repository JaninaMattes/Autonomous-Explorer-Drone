diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index f5908cb7..ee420855 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -14,6 +14,11 @@ from stable_baselines3 import PPO
 # gym environment
 import gym
 
+# video logging
+from matplotlib import animation
+import matplotlib.pyplot as plt
+import numpy as np
+
 # logging python
 import logging
 import sys
@@ -565,7 +570,7 @@ class PPO_PolicyGradient_V2:
     def collect_rollout(self, n_steps=1):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        t_step, rewards = 0, []
+        t_step, rewards, frames = 0, [], deque(maxlen=24) # 4 fps - 6 sec
 
         # log time
         episode_time = []
@@ -584,7 +589,7 @@ class PPO_PolicyGradient_V2:
         while t_step < n_steps:
             
             # rewards collected
-            rewards, done = [], False 
+            rewards, done, frames = [], False, []
             obs = self.env.reset()
 
             # measure time elapsed for one episode
@@ -596,7 +601,7 @@ class PPO_PolicyGradient_V2:
             for t_batch in range(0, self.batch_size):
                 # render gym envs
                 if self.render_video and t_batch % self.render_steps == 0:
-                    self.env.render()
+                    frames.append(self.env.render(mode="rgb_array"))
                 
                 t_step += 1 
 
@@ -642,7 +647,7 @@ class PPO_PolicyGradient_V2:
         action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
         dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time)
+        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time), frames
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -672,7 +677,7 @@ class PPO_PolicyGradient_V2:
             # Collect data over one episode
             # Episode = recording of actions and states that an agent performed from a start state to an end state
             # STEP 3: simulate and collect trajectories --> the following values are all per batch over one episode
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.n_rollout_steps)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time, frames = self.collect_rollout(n_steps=self.n_rollout_steps)
 
             # experiences simulated so far
             training_steps += np.sum(ep_lens)
@@ -696,7 +701,7 @@ class PPO_PolicyGradient_V2:
 
             # log all statistical values to CSV
             self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time, done_so_far, exp_name=self.exp_name)
-            
+
             # increment for each iteration
             done_so_far += 1
 
@@ -729,6 +734,15 @@ class PPO_PolicyGradient_V2:
                     for value in self.stats_data.values():
                         del value[:]
 
+                # Log to video
+                if self.render_video:
+                    filename='pendulum_v1.gif'
+                    self.save_frames_as_gif(frames, self.exp_path, filename)
+                    wandb.log({
+                        "train/video": wandb.Video(os.path.join(self.exp_path, filename), 
+                        caption='episode: '+str(done_so_far), 
+                        fps=4, format="gif"), "step": done_so_far})
+
         # Finalize and plot stats
         if self.stats_plotter:
             df = self.stats_plotter.read_csv() # read all files in folder
@@ -745,6 +759,20 @@ class PPO_PolicyGradient_V2:
             # Save any files starting with "ppo"
             wandb.save(os.path.join(wandb.run.dir, "ppo*"))
 
+    def save_frames_as_gif(self, frames, path='./', filename='pendulum_v1.gif'):
+
+        #Mess with this to change frame size
+        plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)
+
+        patch = plt.imshow(frames[0])
+        plt.axis('off')
+
+        def animate(i):
+            patch.set_data(frames[i])
+
+        anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)
+        video_path = os.path.join(path, filename)
+        anim.save(video_path, writer='imagemagick', fps=60)
 
     def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, done_so_far, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
@@ -1021,7 +1049,7 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 1_500_000     # time steps regarding batches collected and train agent
+    total_training_steps = 1_200_000     # time steps regarding batches collected and train agent
     batch_size = 1024                    # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
     n_optepochs = 32                     # Number of epochs per time step to optimize the neural networks
@@ -1036,10 +1064,10 @@ if __name__ == '__main__':
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
     normalize_adv = True                 # wether to normalize the advantage estimate
-    normalize_ret = True                # wether to normalize the return function
+    normalize_ret = False                # wether to normalize the return function
     
     # setup for torch save models and rendering
-    render_video = False
+    render_video = True
     render_steps = 10
     save_steps = 100
 
