diff --git a/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv b/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
index 1f53afe7..121ddc6d 100644
--- a/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
+++ b/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
@@ -401,3 +401,56 @@
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",880000,0.035997,200.0,11,-833.371375,-1006.385232,-748.792749,84.414518
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",882200,0.035903,200.0,11,-858.969178,-992.490244,-737.422276,88.048019
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",884400,0.035936,200.0,11,-833.454353,-1076.09056,-630.415911,117.493608
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",886600,0.038754,200.0,11,-857.078007,-980.811077,-752.516913,71.570946
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",888800,0.038257,200.0,11,-876.095189,-1105.845301,-746.1719,122.322252
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",891000,0.03673,200.0,11,-839.162794,-969.127477,-738.130164,72.744673
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",893200,0.036408,200.0,11,-842.19847,-1061.209549,-624.200784,119.360579
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",895400,0.036677,200.0,11,-917.76437,-1022.485926,-752.777606,91.843686
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",897600,0.036618,200.0,11,-870.279145,-1077.013351,-663.909212,135.130288
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",899800,0.039166,200.0,11,-818.156357,-1083.651878,-627.972492,132.034869
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",902000,0.037677,200.0,11,-780.094618,-967.940425,-629.884505,110.910147
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",904200,0.036356,200.0,11,-753.906084,-971.151297,-517.400992,118.363642
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",906400,0.036633,200.0,11,-845.319073,-1042.529031,-627.870358,124.470976
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",908600,0.03635,200.0,11,-809.9515,-1007.868424,-652.024548,103.093718
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",910800,0.036115,200.0,11,-870.078368,-1158.810354,-748.115539,136.552901
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",913000,0.039106,200.0,11,-893.785059,-1034.406016,-665.398243,96.955644
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",915200,0.037282,200.0,11,-894.055817,-1092.717992,-750.782596,124.781175
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",917400,0.036134,200.0,11,-895.442175,-1006.91564,-768.010515,77.671126
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",919600,0.036313,200.0,11,-861.203935,-977.75027,-720.897296,77.066715
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",921800,0.036191,200.0,11,-859.540887,-1037.434027,-638.688797,104.619191
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",924000,0.036389,200.0,11,-810.79916,-1010.469437,-632.240027,137.22701
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",926200,0.039543,200.0,11,-854.306071,-1230.008874,-632.043718,148.938971
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",928400,0.037522,200.0,11,-865.472109,-1136.622667,-645.062059,132.914213
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",930600,0.036315,200.0,11,-844.78535,-1141.099626,-636.891747,142.123732
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",932800,0.037085,200.0,11,-795.260196,-890.409398,-508.622309,110.27884
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",935000,0.036921,200.0,11,-786.2414,-946.581973,-657.388571,73.849423
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",937200,0.036463,200.0,11,-854.245854,-1106.55928,-694.647837,124.879004
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",939400,0.039078,200.0,11,-856.084112,-1016.192372,-498.359577,132.074899
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",941600,0.037322,200.0,11,-805.215081,-977.443023,-634.704329,97.374283
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",943800,0.037056,200.0,11,-810.465872,-964.729632,-624.223022,109.68309
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",946000,0.037802,200.0,11,-906.576345,-1103.605694,-753.411969,108.749588
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",948200,0.038704,200.0,11,-806.098964,-955.124984,-660.799117,82.460255
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",950400,0.038542,200.0,11,-798.920901,-966.351483,-657.195904,85.468548
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",952600,0.039449,200.0,11,-821.929234,-897.724971,-739.409329,68.437043
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",954800,0.037567,200.0,11,-866.603211,-1057.149498,-744.717274,96.52451
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",957000,0.038671,200.0,11,-855.263938,-1033.412353,-629.336216,151.418982
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",959200,0.03735,200.0,11,-784.942257,-1047.490388,-624.223858,125.519983
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",961400,0.038646,200.0,11,-794.168942,-1022.534528,-622.399333,109.357904
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",963600,0.038163,200.0,11,-873.476111,-1065.73287,-751.61934,108.826687
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",965800,0.040247,200.0,11,-842.248058,-1128.518475,-632.253506,133.30544
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",968000,0.04183,200.0,11,-875.157421,-1133.79506,-686.700712,142.863003
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",970200,0.036948,200.0,11,-795.079274,-965.433539,-631.628935,85.458706
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",972400,0.038347,200.0,11,-850.973606,-1150.810043,-630.95158,133.111151
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",974600,0.037154,200.0,11,-922.554718,-1175.163883,-749.419444,109.723578
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",976800,0.038381,200.0,11,-816.554044,-945.983093,-630.646995,87.212889
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",979000,0.038924,200.0,11,-827.563709,-1010.481885,-669.048055,99.560132
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",981200,0.038884,200.0,11,-808.271064,-978.510334,-626.974863,106.880283
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",983400,0.036206,200.0,11,-854.834684,-1042.752105,-514.831189,151.580617
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",985600,0.037356,200.0,11,-845.963295,-881.016269,-738.150227,40.801569
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",987800,0.036428,200.0,11,-823.854304,-1040.046524,-621.108176,112.620593
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",990000,0.038333,200.0,11,-823.652742,-975.010229,-637.735261,95.782509
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",992200,0.039628,200.0,11,-812.199598,-959.24321,-636.647322,95.722072
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",994400,0.03642,200.0,11,-841.928176,-1105.321961,-703.533603,134.140078
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",996600,0.036456,200.0,11,-770.435797,-936.660575,-635.10654,70.62929
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",998800,0.037045,200.0,11,-779.643216,-1041.303149,-633.472395,117.670294
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",1001000,0.037158,200.0,11,-803.910809,-956.557849,-641.002719,83.079681
diff --git a/PPO/ppo_torch/network.py b/PPO/ppo_torch/network.py
index 66d716e1..240760a6 100644
--- a/PPO/ppo_torch/network.py
+++ b/PPO/ppo_torch/network.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 from torch.optim import Adam
 from torch.distributions import MultivariateNormal
+from torch import functional as F
 import numpy as np
 
 
@@ -10,6 +11,25 @@ class Net(nn.Module):
     def __init__(self) -> None:
         super(Net, self).__init__()
 
+class CNN(nn.Module):
+    def __init__(self):
+        super(Net, self).__init__()
+        self.conv1 = nn.Conv2d(3, 6, 5)
+        self.pool = nn.MaxPool2d(2, 2)
+        self.conv2 = nn.Conv2d(6, 16, 5)
+        self.fc1 = nn.Linear(16 * 5 * 5, 120)
+        self.fc2 = nn.Linear(120, 84)
+        self.fc3 = nn.Linear(84, 10)
+
+    def forward(self, x):
+        x = self.pool(F.relu(self.conv1(x)))
+        x = self.pool(F.relu(self.conv2(x)))
+        x = x.view(-1, 16 * 5 * 5)
+        x = F.relu(self.fc1(x))
+        x = F.relu(self.fc2(x))
+        x = self.fc3(x)
+        return x
+
 class ValueNet(Net):
     """Setup Value Network (Critic) optimizer"""
     def __init__(self, in_dim, out_dim) -> None:
diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index fba62e12..4959607d 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -220,8 +220,8 @@ class PPO_PolicyGradient_V2:
         device='cpu',
         exp_path='./log/',
         exp_name='PPO_V2_experiment',
-        normalize_advantage=False,
-        normalize_returns=False) -> None:
+        normalize_adv=False,
+        normalize_ret=False) -> None:
         
         # hyperparams
         self.in_dim = in_dim
@@ -242,8 +242,8 @@ class PPO_PolicyGradient_V2:
         self.render_steps = render
         self.save_model = save_model
         self.device = device
-        self.normalize_advantage = normalize_advantage
-        self.normalize_returns = normalize_returns
+        self.normalize_advantage = normalize_adv
+        self.normalize_return = normalize_ret
 
         # keep track of information
         self.exp_path = exp_path
@@ -309,16 +309,16 @@ class PPO_PolicyGradient_V2:
         action, log_prob, entropy = self.get_action(action_dist)
         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
 
-    def cummulative_return(self, batch_rewards):
+    def cummulative_return(self, episode_rewards):
         cum_returns = []
-        for rewards in reversed(batch_rewards): # reversed order
+        for rewards in reversed(episode_rewards): # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
         return torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
 
-    def advantage_estimate(self, batch_rewards, values, normalize_advantage=False, normalized_return=False):
+    def advantage_estimate(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
             - bias-variance: TD has low variance, but IS biased
@@ -326,32 +326,34 @@ class PPO_PolicyGradient_V2:
         """
         advantages = []
         cum_returns = []
-        for rewards in reversed(batch_rewards):  # reversed order
+        for rewards in reversed(episode_rewards):  # reversed order
             for reward in reversed(rewards):
                 cum_returns.insert(0, reward) # reverse it again
         # TD error: A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
         cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
         advantages = cum_returns - values
-        if normalize_advantage:
+        if normalized_adv:
             advantages = self.normalize_adv(advantages)
-        if normalized_return:
+        if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
         return advantages, cum_returns
 
-    def advantage_reinforce(self, batch_rewards, values, normalized_adv=False, normalized_ret=False):
+    def advantage_reinforce(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
         """ Advantage Reinforce A(s_t, a_t) = G(t)
         """
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # Returns: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # Example Reinforce: https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py
         # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
-        for rewards in reversed(batch_rewards): # reversed order
+        for rewards in reversed(episode_rewards): # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
         advantages = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
         # normalize for more stability
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
@@ -359,7 +361,7 @@ class PPO_PolicyGradient_V2:
             cum_returns = self.normalize_ret(cum_returns)
         return advantages, cum_returns
 
-    def advantage_actor_critic(self, batch_rewards, values, normalized_adv=False, normalized_ret=False):
+    def advantage_actor_critic(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
         """ Advantage Actor-Critic A(s_t, a_t) = G(t) - V(s_t)
             Calculate delta, which is defined by delta = r - v 
         """
@@ -367,31 +369,58 @@ class PPO_PolicyGradient_V2:
         # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
-        for rewards in reversed(batch_rewards): # reversed order
+        for rewards in reversed(episode_rewards): # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
                 # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
         cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        # normalize returns
+        if normalized_ret:
+            cum_returns = self.normalize_ret(cum_returns)
         # delta = G(t) - V(s_t)
         advantages = cum_returns - values 
-        # normalize for more stability
+        # normalize advantage for more stability
         if normalized_adv:
             advantages = self.normalize_adv(advantages)
+        return advantages, cum_returns
+
+    def advantage_TD_actor_critic(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
+        """ Advantage TD Actor-Critic A(s_t, a_t) = G(t) - V(s_t)
+            Calculate delta, which is defined by delta = r - v 
+        """
+        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # G(t) is the total disounted reward
+        # return value: G(t) = R(t) + gamma * R(t-1)
+        cum_returns = []
+        advantages = []
+        episode_rewards = torch.tensor(np.array(episode_rewards), device=self.device, dtype=torch.float)
+        cum_returns = torch.flatten(episode_rewards)
+        # normalize returns
         if normalized_ret:
             cum_returns = self.normalize_ret(cum_returns)
+        last_value = values[1]
+        for i in reversed(range(len(cum_returns))):
+            # δ_t = r_t + γ * V(s_t+1) − V(s_t)
+            advantage = cum_returns[i] + (self.gamma * values[i]) - last_value
+            advantages.insert(0, advantage)
+            last_value = values[i]
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        # normalize advantage for more stability
+        if normalized_adv:
+            advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
-    def generalized_advantage_estimate_1(self, batch_rewards, values, normalized_adv=False, normalized_ret=False):
+    def generalized_advantage_estimate_1(self, episode_rewards, values, normalized_adv=False, normalized_ret=False):
         """ Generalized Advantage Estimate calculation
             - GAE defines advantage as a weighted average of A_t
             - advantage measures if an action is better or worse than the policy's default behavior
             - want to find the maximum Advantage representing the benefit of choosing a specific action
         """
         # check if tensor and convert to numpy
-        if torch.is_tensor(batch_rewards):
-            batch_rewards = batch_rewards.detach().numpy()
+        if torch.is_tensor(episode_rewards):
+            episode_rewards = episode_rewards.detach().numpy()
         if torch.is_tensor(values):
             values = values.detach().numpy()
 
@@ -399,7 +428,7 @@ class PPO_PolicyGradient_V2:
         # STEP 5: compute advantage estimates δ_t = − V(s_t) + r_t
         cum_returns = []
         advantages = []
-        for rewards in reversed(batch_rewards): # reversed order
+        for rewards in reversed(episode_rewards): # reversed order
             discounted_reward = 0
             for i in reversed(range(len(rewards))):
                 discounted_reward = rewards[i] + (self.gamma * discounted_reward)
@@ -419,7 +448,7 @@ class PPO_PolicyGradient_V2:
         return advantages, cum_returns
 
 
-    def generalized_advantage_estimate_2(self, obs, next_obs, batch_rewards, dones, normalized_adv=False, normalized_ret=False):
+    def generalized_advantage_estimate_2(self, obs, next_obs, episode_rewards, dones, normalized_adv=False, normalized_ret=False):
         """ Generalized Advantage Estimate calculation
             - GAE defines advantage as a weighted average of A_t
             - advantage measures if an action is better or worse than the policy's default behavior
@@ -434,7 +463,7 @@ class PPO_PolicyGradient_V2:
         returns = []
 
         # STEP 4: Calculate cummulated reward
-        for rewards in reversed(batch_rewards):
+        for rewards in reversed(episode_rewards):
             prev_advantage = 0
             returns_current = ns_values[-1]  # V(s_t+1)
             for i in reversed(range(len(rewards))):
@@ -459,7 +488,7 @@ class PPO_PolicyGradient_V2:
         return advantages, returns
 
 
-    def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized_adv=False, normalized_ret=False):
+    def generalized_advantage_estimate_3(self, episode_rewards, values, dones, normalized_adv=False, normalized_ret=False):
         """ Calculate advantage as a weighted average of A_t
                 - advantage measures if an action is better or worse than the policy's default behavior
                 - GAE allows to balance bias and variance through a weighted average of A_t
@@ -472,8 +501,8 @@ class PPO_PolicyGradient_V2:
 
         advantages = []
         returns = []
-        values = values.detach().numpy()
-        for rewards in reversed(batch_rewards): # reversed order
+
+        for rewards in reversed(episode_rewards): # reversed order
             prev_advantage = 0
             discounted_reward = 0
             last_value = values[-1] # V(s_t+1)
@@ -511,7 +540,8 @@ class PPO_PolicyGradient_V2:
         return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
     
     def normalize_ret(self, returns):
-        return (returns - returns.mean()) / returns.std()
+        eps = np.finfo(np.float32).eps.item()
+        return (returns - returns.mean()) / (returns.std() + eps)
 
     def finish_episode(self):
         pass 
@@ -519,7 +549,7 @@ class PPO_PolicyGradient_V2:
     def collect_rollout(self, n_steps=1, render=False):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        t_step, episode_rewards = 0, []
+        t_step, rewards = 0, []
 
         # log time
         episode_time = []
@@ -530,15 +560,15 @@ class PPO_PolicyGradient_V2:
         episode_actions = []
         episode_action_probs = []
         episode_dones = []
-        batch_rewards = []
-        batch_lens = []
+        episode_rewards = []
+        episode_lens = []
 
         # Run Monte Carlo simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
         while t_step < n_steps:
             
-            # rewards collected per episode
-            episode_rewards, done = [], False 
+            # rewards collected
+            rewards, done = [], False 
             obs = self.env.reset()
 
             # measure time elapsed for one episode
@@ -567,7 +597,7 @@ class PPO_PolicyGradient_V2:
                 episode_nextobs.append(__obs)
                 episode_actions.append(action)
                 episode_action_probs.append(log_probability)
-                episode_rewards.append(reward)
+                rewards.append(reward)
                 episode_dones.append(done)
                     
                 obs = __obs
@@ -583,8 +613,8 @@ class PPO_PolicyGradient_V2:
             time_elapsed = end_epoch - start_epoch
             episode_time.append(time_elapsed)
 
-            batch_lens.append(t_episode + 1) # as we started at 0
-            batch_rewards.append(episode_rewards)
+            episode_lens.append(t_episode + 1) # as we started at 0
+            episode_rewards.append(rewards)
 
         # convert trajectories to torch tensors
         obs = torch.tensor(np.array(episode_obs), device=self.device, dtype=torch.float)
@@ -593,7 +623,7 @@ class PPO_PolicyGradient_V2:
         action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
         dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens, np.array(episode_time)
+        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time)
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -616,7 +646,7 @@ class PPO_PolicyGradient_V2:
     def learn(self):
         """"""
         training_steps = 0
-
+        
         while training_steps < self.total_training_steps:
             policy_losses, value_losses = [], []
             # Collect episode
@@ -628,8 +658,10 @@ class PPO_PolicyGradient_V2:
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_returns)
-
+            #advantages, cum_returns = self.advantage_reinforce(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            #advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            
             # update network params 
             for _ in range(self.noptepochs):
                 # STEP 6-7: calculate loss and update weights
@@ -646,7 +678,6 @@ class PPO_PolicyGradient_V2:
             if training_steps % self.save_model == 0:
                 env_name = self.env.unwrapped.spec.id
                 env_model_path = os.path.join(self.exp_path, 'models')
-                print(env_model_path)
                 policy_net_name = os.path.join(env_model_path, f'{env_name}_policyNet.pth')
                 value_net_name = os.path.join(env_model_path, f'{env_name}_valueNet.pth')
                 torch.save({
@@ -675,13 +706,21 @@ class PPO_PolicyGradient_V2:
         # Finalize and plot stats
         if self.stats_plotter:
             df = self.stats_plotter.read_csv() # read all files in folder
-            self.stats_plotter.plot_seaborn_fill(df, x='timestep', y='mean episodic returns', y_min='min episodic returns', y_max='max episodic returns',  title=f'{env_name}', x_label='Timestep', y_label='Mean Episodic Return', color='blue', smoothing=6)
-            # self.stats_plotter.plot_box(df, x='timestep', y='mean episodic runtime', title='title', x_label='Timestep', y_label='Mean Episodic Time')
+            self.stats_plotter.plot_seaborn_fill(df, x='timestep', y='mean episodic returns', 
+                                                y_min='min episodic returns', y_max='max episodic returns',  
+                                                title=f'{env_name}', x_label='Timestep', y_label='Mean Episodic Return', 
+                                                color='blue', smoothing=6, wandb=wandb, xlim_up=self.total_training_steps)
+
+            # self.stats_plotter.plot_box(df, x='timestep', y='mean episodic runtime', 
+            #                             title='title', x_label='Timestep', y_label='Mean Episodic Time', wandb=wandb)
 
         # save files in path
-        wandb.save(self.exp_path)
+        wandb.save(os.path.join(self.exp_path, "*csv"))
+        # Save any files starting with "ppo"
+        wandb.save(os.path.join(wandb.run.dir, "ppo*"))
+
 
-    def log_stats(self, p_losses, v_losses, batch_return, batch_lens, training_steps, time, exp_name='experiment'):
+    def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
         if torch.is_tensor(batch_return):
             batch_return = batch_return.detach().numpy()
@@ -692,7 +731,7 @@ class PPO_PolicyGradient_V2:
         # Calculate the stats of an episode
         cum_ret = [np.sum(ep_rews) for ep_rews in batch_return]
         mean_ep_time = round(np.mean(time), 6) 
-        mean_ep_len = round(np.mean(batch_lens), 6)
+        mean_ep_len = round(np.mean(episode_lens), 6)
 
         # statistical values for return
         mean_ep_ret = round(np.mean(cum_ret), 6)
@@ -739,28 +778,18 @@ def arg_parser():
     parser = argparse.ArgumentParser()
     # fmt: off
     parser = argparse.ArgumentParser()
-    parser.add_argument("--video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False,
-        help="if toggled, capture video of run")
-    parser.add_argument("--train", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
-        help="if toggled, run model in training mode")
-    parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False,
-        help="if toggled, run model in testing mode")
-    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
-        help="if toggled, log hyperparameters")
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False, help="if toggled, capture video of run")
+    parser.add_argument("--train", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True, help="if toggled, run model in training mode")
+    parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False, help="if toggled, run model in testing mode")
+    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True, help="if toggled, log hyperparameters")
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"), help="the name of this experiment")
+    parser.add_argument("--project-name", type=str, default='OpenAIGym-PPO', help="the name of this project") 
+    parser.add_argument("--gym-id", type=str, default="Pendulum-v1", help="the id of the gym environment")
+    parser.add_argument("--learning-rate", type=float, default=3e-4, help="the learning rate of the optimizer")
+    parser.add_argument("--seed", type=int, default=1, help="seed of the experiment")
+    parser.add_argument("--total-timesteps", type=int, default=2000000, help="total timesteps of the experiments")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, cuda will be enabled by default")
     
     # Parse arguments if they are given
     args = parser.parse_args()
@@ -858,7 +887,7 @@ def _log_summary(ep_len, ep_ret, ep_num):
 def train(env, in_dim, out_dim, total_training_steps, max_batch_size, n_rollout_steps,
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
           adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter,
-          normalize_advantage=False, normalize_returns=False,
+          normalize_adv=False, normalize_ret=False,
           log_video=False, ppo_version='v2', device='cpu', exp_path='./log/', exp_name='PPO-experiment'):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = None
@@ -877,8 +906,8 @@ def train(env, in_dim, out_dim, total_training_steps, max_batch_size, n_rollout_
                     gamma=gamma,
                     epsilon=epsilon,
                     adam_eps=adam_epsilon,
-                    normalize_advantage=normalize_advantage,
-                    normalize_returns=normalize_returns,
+                    normalize_adv=normalize_adv,
+                    normalize_ret=normalize_ret,
                     render=render_steps,
                     save_model=save_steps,
                     csv_writer=csv_writer,
@@ -960,7 +989,7 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 1_000_000     # time steps regarding batches collected and train agent
+    total_training_steps = 3_000_000     # time steps regarding batches collected and train agent
     max_batch_size = 512                 # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
     noptepochs = 12                      # Number of epochs per time step to optimize the neural networks
@@ -974,8 +1003,8 @@ if __name__ == '__main__':
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_advantage = True           # wether to normalize the advantage estimate
-    normalize_returns = False            # wether to normalize the return function
+    normalize_adv = False                # wether to normalize the advantage estimate
+    normalize_ret = True                 # wether to normalize the return function
     # setup for torch save models and rendering
     render = True
     render_steps = 10
@@ -1032,7 +1061,7 @@ if __name__ == '__main__':
 
     # Monitoring with W&B
     wandb.init(
-            project=f'drone-mechanics-ppo-OpenAIGym',
+            project=args.project_name,
             entity='drone-mechanics',
             sync_tensorboard=True,
             config={ # stores hyperparams in job
@@ -1054,12 +1083,13 @@ if __name__ == '__main__':
                 'gamma (discount)': gamma,
                 'epsilon (clipping)': epsilon,
                 'gae lambda (GAE)': gae_lambda,
-                'normalize advantage': normalize_advantage,
-                'normalize return': normalize_returns,
+                'normalize advantage': normalize_adv,
+                'normalize return': normalize_ret,
                 'seed': seed,
                 'experiment path': exp_folder_name,
                 'experiment name': args.exp_name
             },
+            dir=os.getcwd(),
             name=exp_name, # needs flag --exp-name
             monitor_gym=True,
             save_code=True
@@ -1080,8 +1110,8 @@ if __name__ == '__main__':
             gamma=gamma,
             epsilon=epsilon,
             adam_epsilon=adam_epsilon,
-            normalize_advantage=normalize_advantage,
-            normalize_returns=normalize_returns,
+            normalize_adv=normalize_adv,
+            normalize_ret=normalize_ret,
             render_steps=render_steps,
             save_steps=save_steps,
             csv_writer=csv_writer,
diff --git a/PPO/ppo_torch/wrapper/stats_logger.py b/PPO/ppo_torch/wrapper/stats_logger.py
index e332ee60..25b4471f 100644
--- a/PPO/ppo_torch/wrapper/stats_logger.py
+++ b/PPO/ppo_torch/wrapper/stats_logger.py
@@ -9,6 +9,8 @@ import os
 # switch style
 sns.set_theme()
 sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
+diverging_colors = sns.color_palette("RdBu", 10)
+
 # setup plotting 
 plt.rcParams["figure.figsize"] = [12.50, 9.50]
 plt.rcParams["figure.autolayout"] = True
@@ -54,7 +56,8 @@ class StatsPlotter:
         df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
         return df
 
-    def plot_box(self, dataframe, x, y, title='title', x_label='Timestep', y_label='Mean Episodic Time'):        
+    def plot_box(self, dataframe, x, y, title='title', x_label='Timestep', y_label='Mean Episodic Time', wandb=None):
+        """Create a box plot for time needed to converge per experiment."""     
         # get exp names
         df_exp = dataframe['experiment'].values.astype('str')
         df_exp_unique = list(dict.fromkeys(df_exp))
@@ -67,35 +70,42 @@ class StatsPlotter:
         ax.figure.savefig(self.file_name_and_path)
         plt.show()
 
-    def plot_seaborn_fill(self, dataframe, x, y, y_min, y_max, title='title', x_label='Timestep', y_label='Mean Episodic Return', upper_bound=0, lower_bound=-1800, ylim_low=-2000, ylim_up=200, color='blue', smoothing=2):
+        if wandb:
+            wandb.log({'Mean Episodic Time': plt})
+
+    def plot_seaborn_fill(self, dataframe, x, y, y_min, y_max, title='title', x_label='Timestep', y_label='Mean Episodic Return', upper_bound=0, lower_bound=-1800, xlim_up=3_000_000, ylim_low=-2000, ylim_up=200, color='blue', smoothing=2, wandb=None):
         # get values from df
         # add smoothing
         df_min = gaussian_filter1d(dataframe[y_min].to_numpy(), sigma=smoothing)
         df_max = gaussian_filter1d(dataframe[y_max].to_numpy(), sigma=smoothing)
-        df_x = dataframe[x].to_numpy()
+        df_x = dataframe[x].to_numpy(dtype=int)
         df_y = gaussian_filter1d(dataframe[y].to_numpy(), sigma=smoothing)
         # get exp names
         df_exp = dataframe['experiment'].values.astype('str')
         df_exp_unique = list(dict.fromkeys(df_exp))
 
         # draw mean line
-        ax = sns.lineplot(x=df_x, y=df_y)
+        ax = sns.lineplot(x=df_x, y=df_y, lw=2)
         # fill std
         ax.fill_between(x=df_x, y1=df_min, y2=df_max, color=sns.xkcd_rgb[color], alpha=0.2)
         # draw upper and lower bounds
         ax.axhline(lower_bound, linewidth=1, color='red', label='lower bound')
-        ax.axhline(upper_bound, linewidth=1, color='red', label='upper bound')
-        ax.set(title=title, xlabel=x_label, ylabel=y_label, ylim=(ylim_low, ylim_up))
-
+        ax.axhline(upper_bound, linewidth=1, color='red', label='upper bound')        
+        # change x-y axis scale
+        ax.set(title=title, xlabel=x_label, ylabel=y_label, xlim=(0, xlim_up), ylim=(ylim_low, ylim_up))
         # set legend
         plt.legend(labels=df_exp_unique, loc='upper right')
 
         # plot the file to given destination
         ax.figure.savefig(self.file_name_and_path)
         plt.show()
+        
+        if wandb:
+            images = wandb.Image(plt)
+            wandb.log({'Mean Episodic Return': images})
 
 
-    def plot_seaborn(self, dataframe, x, y, hue='hue', title='title', x_label='Timestep', y_label='Mean Episodic Return', upper_bound=0, lower_bound=-1800):
+    def plot_seaborn(self, dataframe, x, y, hue='hue', title='title', x_label='Timestep', y_label='Mean Episodic Return', upper_bound=0, lower_bound=-1800, wandb=None):
         """ Create a lineplot with seaborn.
             Doc: https://seaborn.pydata.org/tutorial/introduction
         """
@@ -111,13 +121,24 @@ class StatsPlotter:
 
         # plot the file to given destination
         g.figure.savefig(self.file_name_and_path)
+        plt.show()
+
+        if wandb:
+            wandb.log({'Mean Episodic Return': plt})
     
-    def plot_matplot(self, x_values, y_values, y_lower, y_upper):
+    def plot_matplot(self, x_values, y_values, y_lower, y_upper, wandb=None):
         """ Create a matplot lineplot with filling between ."""
         plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) # standard deviation
         plt.plot(x_values, y_values) # plotted mean 
         plt.show()
 
+        if wandb:
+            wandb.log({'Mean Episodic Return': plt})
+    
+    def get_random_col(self):
+        colour = (np.random.random(), np.random.random(), np.random.random())
+        return colour
+
 class CSVWriter:
     """Log the network outputs via pandas to a CSV file.
     """
