diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index c22fb37e..a1a7e5b1 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -1,4 +1,5 @@
 from collections import deque
+import time
 import torch
 from torch import nn
 from torch.optim import Adam
@@ -114,9 +115,9 @@ class PPO_PolicyGradient_V1:
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -128,14 +129,15 @@ class PPO_PolicyGradient_V1:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -148,6 +150,7 @@ class PPO_PolicyGradient_V1:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
         # track video of gym
         self.log_video = log_video
@@ -183,9 +186,9 @@ class PPO_PolicyGradient_V2:
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -197,14 +200,15 @@ class PPO_PolicyGradient_V2:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -217,6 +221,7 @@ class PPO_PolicyGradient_V2:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
         # track video of gym
         self.log_video = log_video
@@ -228,10 +233,12 @@ class PPO_PolicyGradient_V2:
         self.stats_data = {'experiment': [], 
                            'timestep': [], 
                            'mean episodic length': [], 
-                           'mean episodic returns': [], 
-                           'info/mean episodic returns': [], 
-                           'info/mean episodic length': [], 
-                           'info/mean episodic time': []
+                           'mean episodic returns': [],
+                           'mean episodic runtime': [],
+                           'std episodic returns': [], 
+                        #    'info/mean episodic returns': [], 
+                        #    'info/mean episodic length': [], 
+                        #    'info/mean episodic time': []
                         }
 
         # add net for actor and critic
@@ -283,47 +290,43 @@ class PPO_PolicyGradient_V2:
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        return torch.tensor(np.array(cum_returns), dtype=torch.float)
+        return torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
 
     def advantage_estimate(self, batch_rewards, values, normalized=True):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
+            - bias-variance: TD has low variance, but IS biased
             - dones: only get reward at end of episode, not disounted next state value
         """
-        # check if tensor and convert to numpy
-        if torch.is_tensor(batch_rewards):
-            batch_rewards = batch_rewards.detach().numpy()
-        if torch.is_tensor(values):
-            values = values.detach().numpy()
-
         advantages = []
-        last_value = values[-1]
+        cum_returns = []
         for rewards in reversed(batch_rewards):  # reversed order
-            for i in reversed(range(len(rewards))):
-                # TD error: A(s,a) = r + gamma * V(s_t+1) - V(s_t)
-                temp_pred = gamma * last_value - values[i]
-                advantage = rewards[i] + temp_pred
-                advantages.insert(0, advantage)
-                last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+            for reward in reversed(rewards):
+                cum_returns.insert(0, reward) # reverse it again
+        # TD error: A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = cum_returns - values
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return advantages
+        return advantages, cum_returns
 
     def generalized_advantage_estimate_0(self, batch_rewards, values, normalized=True):
         """ Generalized Advantage Estimate calculation
             Calculate delta, which is defined by delta = r - v 
         """
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
         for rewards in reversed(batch_rewards): # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
+                # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = cum_returns - values # delta = r - v
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        # delta = G(t) - V(s_t)
+        advantages = cum_returns - values 
         # normalize for more stability
         if normalized:
             advantages = self.normalize_adv(advantages)
@@ -356,8 +359,8 @@ class PPO_PolicyGradient_V2:
                 cum_returns.insert(0, discounted_reward) # reverse it again
 
         # convert numpy to torch tensor
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
@@ -385,7 +388,8 @@ class PPO_PolicyGradient_V2:
                 # STEP 5: compute advantage estimates A_t at step t
                 mask = (1.0 - dones[i])
                 gamma = self.gamma * mask
-                td_error = rewards[i] + gamma * ns_values[i] - s_values[i]
+                td_target = rewards[i] + (gamma * ns_values[i])
+                td_error = td_target - s_values[i]
                 # A_t = δ_t + γ * λ * A(t+1)
                 prev_advantage = td_error + gamma * self.gae_lambda * prev_advantage
                 returns_current = rewards[i] + gamma * returns_current
@@ -395,7 +399,9 @@ class PPO_PolicyGradient_V2:
         advantages = np.array(advantages)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
+        return advantages, returns
 
 
     def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
@@ -436,8 +442,9 @@ class PPO_PolicyGradient_V2:
                 advantages.insert(0, prev_advantage) # reverse it again
                 # store current value as V(s_t+1)
                 last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
-        returns = torch.tensor(np.array(returns), dtype=torch.float)
+
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, returns
@@ -452,36 +459,43 @@ class PPO_PolicyGradient_V2:
     def finish_episode(self):
         pass 
 
-    def collect_rollout(self, n_step=1, render=False):
+    def collect_rollout(self, n_steps=1, render=False):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        step, trajectory_rewards = 0, []
+        t_step, episode_rewards = 0, []
+
+        # log time
+        episode_time = []
 
         # collect trajectories
-        trajectory_obs = []
-        trajectory_nextobs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_dones = []
+        episode_obs = []
+        episode_nextobs = []
+        episode_actions = []
+        episode_action_probs = []
+        episode_dones = []
         batch_rewards = []
         batch_lens = []
 
         # Run Monte Carlo simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
-        while step < n_step:
+        while t_step < n_steps:
             
             # rewards collected per episode
-            trajectory_rewards, done = [], False 
+            episode_rewards, done = [], False 
             obs = self.env.reset()
 
+            # measure time elapsed for one episode
+            # torch.cuda.synchronize()
+            start_epoch = time.time()
+
             # Run episode for a fixed amount of timesteps
             # to keep rollout size fixed and episodes independent
-            for ep_t in range(0, self.max_batch_size):
+            for t_episode in range(0, self.max_batch_size):
                 # render gym envs
-                if render and ep_t % self.render_steps == 0:
+                if render and t_episode % self.render_steps == 0:
                     self.env.render()
                 
-                step += 1 
+                t_step += 1 
 
                 # action logic 
                 # sampled via policy which defines behavioral strategy of an agent
@@ -489,46 +503,40 @@ class PPO_PolicyGradient_V2:
                         
                 # STEP 3: collecting set of trajectories D_k by running action 
                 # that was sampled from policy in environment
-                __obs, reward, done, info = self.env.step(action)
+                __obs, reward, done, truncated = self.env.step(action)
 
                 # collection of trajectories in batches
-                trajectory_obs.append(obs)
-                trajectory_nextobs.append(__obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_dones.append(done)
+                episode_obs.append(obs)
+                episode_nextobs.append(__obs)
+                episode_actions.append(action)
+                episode_action_probs.append(log_probability)
+                episode_rewards.append(reward)
+                episode_dones.append(done)
                     
                 obs = __obs
 
                 # break out of loop if episode is terminated
-                if done:
-                    info_episode = info[0]['episode']
-                    info_episode_r = info_episode['r']
-                    info_episode_l = info_episode['l']
-                    info_episode_t = info_episode['t']
-                    self.stats_data['info/mean episodic returns'].append(info_episode_r)
-                    self.stats_data['info/mean episodic length'].append(info_episode_l)
-                    self.stats_data['info/mean episodic time'].append(info_episode_t)
-
-                    wandb.log({
-                        'info/mean episodic returns': info_episode_r,
-                        'info/mean episodic length': info_episode_l,
-                        'info/mean episodic time': info_episode_t
-                    })
+                if done or truncated:
                     break
+            
+            # stop time per episode
+            # Waits for everything to finish running
+            # torch.cuda.synchronize()
+            end_epoch = time.time()
+            time_elapsed = end_epoch - start_epoch
+            episode_time.append(time_elapsed)
 
-            batch_lens.append(ep_t + 1) # as we started at 0
-            batch_rewards.append(trajectory_rewards)
+            batch_lens.append(t_episode + 1) # as we started at 0
+            batch_rewards.append(episode_rewards)
 
         # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        next_obs = torch.tensor(np.array(trajectory_nextobs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        action_log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        dones = torch.tensor(np.array(trajectory_dones), dtype=torch.float)
+        obs = torch.tensor(np.array(episode_obs), device=self.device, dtype=torch.float)
+        next_obs = torch.tensor(np.array(episode_nextobs), device=self.device, dtype=torch.float)
+        actions = torch.tensor(np.array(episode_actions), device=self.device, dtype=torch.float)
+        action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
+        dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens
+        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens, np.array(episode_time)
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -550,23 +558,23 @@ class PPO_PolicyGradient_V2:
 
     def learn(self):
         """"""
-        steps = 0
+        training_steps = 0
 
-        while steps < self.total_timesteps:
+        while training_steps < self.total_training_steps:
             policy_losses, value_losses = [], []
-            # Collect trajectory
+            # Collect episode
             # STEP 3: simulate and collect trajectories --> the following values are all per batch
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens = self.collect_rollout(n_step=self.trajectory_iterations)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.episode_iterations)
 
             # timesteps simulated so far for batch collection
-            steps += np.sum(ep_lens)
+            training_steps += np.sum(ep_lens)
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            cum_returns = self.cummulative_return(rewards)
+            #cum_returns = self.cummulative_return(rewards)
             # advantages = self.advantage_estimate_(cum_returns, values.detach())
-            #advantages, cum_returns = self.generalized_advantage_estimate_0(rewards, values.detach())
-            advantages = self.advantage_estimate(rewards, values)
+            advantages, cum_returns = self.generalized_advantage_estimate_0(rewards, values.detach())
+            #advantages, cum_returns = self.advantage_estimate(rewards, values)
             # update network params 
             for _ in range(self.noptepochs):
                 # STEP 6-7: calculate loss and update weights
@@ -577,21 +585,21 @@ class PPO_PolicyGradient_V2:
                 value_losses.append(value_loss.detach().numpy())
 
             # log all statistical values to CSV
-            self.log_stats(policy_losses, value_losses, rewards, ep_lens, steps)
+            self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time)
 
             # store model in checkpoints
-            if steps % self.save_model == 0:
+            if training_steps % self.save_model == 0:
                 env_name = self.env.unwrapped.spec.id
                 policy_net_name = f'{MODEL_PATH}{env_name}_{CURR_DATE}_policyNet.pth'
                 value_net_name = f'{MODEL_PATH}{env_name}_{CURR_DATE}_valueNet.pth'
                 torch.save({
-                    'epoch': steps,
+                    'epoch': training_steps,
                     'model_state_dict': self.policy_net.state_dict(),
                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
                     'loss': policy_loss,
                     }, policy_net_name)
                 torch.save({
-                    'epoch': steps,
+                    'epoch': training_steps,
                     'model_state_dict': self.value_net.state_dict(),
                     'optimizer_state_dict': self.value_net_optim.state_dict(),
                     'loss': value_loss,
@@ -610,9 +618,9 @@ class PPO_PolicyGradient_V2:
         # Finalize and plot stats
         if self.stats_plotter:
             df = self.stats_plotter.read_csv(CURR_DATE) # just select files from current date
-            self.stats_plotter.plot(df, kind='line', hue='std episodic returns', x='timestep', y='mean episodic returns', title=env_name)
+            self.stats_plotter.plot(df, x='timestep', y='mean episodic returns', title=env_name)
 
-    def log_stats(self, p_losses, v_losses, batch_return, batch_lens, steps, exp_name='experiment'):
+    def log_stats(self, p_losses, v_losses, batch_return, batch_lens, steps, time, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
         if torch.is_tensor(batch_return):
             batch_return = batch_return.detach().numpy()
@@ -622,6 +630,7 @@ class PPO_PolicyGradient_V2:
 
         # Calculate the stats of an episode
         cum_ret = [np.sum(ep_rews) for ep_rews in batch_return]
+        mean_ep_time = np.mean(time)
         mean_ep_len = np.mean(batch_lens)
         mean_ep_rew = np.mean(cum_ret)
         # calculate standard deviation (spred of distribution)
@@ -631,6 +640,7 @@ class PPO_PolicyGradient_V2:
         self.stats_data['experiment'].append(exp_name)
         self.stats_data['mean episodic length'].append(mean_ep_len)
         self.stats_data['mean episodic returns'].append(mean_ep_rew)
+        self.stats_data['mean episodic runtime'].append(mean_ep_time)
         self.stats_data['std episodic returns'].append(std_ep_rew)
         self.stats_data['timestep'].append(steps)
 
@@ -639,6 +649,7 @@ class PPO_PolicyGradient_V2:
             'train/timesteps': steps,
             'train/mean policy loss': mean_p_loss,
             'train/mean value loss': mean_v_loss,
+            'train/mean episode runtime': mean_ep_time,
             'train/mean episode length': mean_ep_len,
             'train/mean episode returns': mean_ep_rew,
             'train/std episode returns': std_ep_rew
@@ -721,9 +732,9 @@ def create_path(path: str) -> None:
     if not os.path.exists(path):
         os.makedirs(path)
 
-def load_model(path, model):
+def load_model(path, model, device='cpu'):
     checkpoint = torch.load(path)
-    model.load_state_dict(checkpoint['model_state_dict'])
+    model.load_state_dict(checkpoint['model_state_dict'], map_location=device)
     return model
 
 def simulate_rollout(policy_net, env, render=True):
@@ -776,9 +787,10 @@ def _log_summary(ep_len, ep_ret, ep_num):
         logging.info(f"--------------------------------------------")
         logging.info('\n')
 
-def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iterations,
+def train(env, in_dim, out_dim, total_training_steps, max_batch_size, episode_iterations,
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
-          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, log_video=False, ppo_version='v2'):
+          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, 
+          log_video=False, ppo_version='v2', device='cpu'):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = None
     if ppo_version == 'v2':
@@ -786,9 +798,9 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -800,15 +812,16 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device)
     else:
         agent = PPO_PolicyGradient_V1(
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -820,15 +833,16 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device)
     # run training for a total amount of steps
     agent.learn()
 
-def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False):
+def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False, device='cpu'):
     """Test the policy network (actor)"""
     # load model and test it
     policy_net = PolicyNet(in_dim, out_dim)
-    policy_net = load_model(path, policy_net)
+    policy_net = load_model(path, policy_net, device)
     
     for ep_num, (ep_len, ep_ret) in enumerate(simulate_rollout(policy_net, env, render)):
         _log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)
@@ -843,9 +857,9 @@ def hyperparam_tuning(config=None):
                 env,
                 in_dim=config.obs_dim, 
                 out_dim=config.act_dim,
-                total_timesteps=config.total_timesteps,
+                total_training_steps=config.total_training_steps,
                 max_batch_size=config.max_batch_size,
-                trajectory_iterations=config.trajectory_iterations,
+                episode_iterations=config.episode_iterations,
                 noptepochs=config.noptepochs,
                 gae_lambda = config.gae_lambda,
                 gamma=config.gamma,
@@ -874,19 +888,19 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_timesteps = 3_500_000     # time steps regarding batches collected and train agent
-    max_batch_size = 1000      # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 2048    # number of batches per episode
-    noptepochs = 12                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gae_lambda = 0.95               # trajectory discount for the general advantage estimation (GAE)
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-8             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
-    env_number = 1                  # number of actors
-    seed = 42                       # seed gym, env, torch, numpy 
+    total_training_steps = 5000     # time steps regarding batches collected and train agent
+    max_batch_size = 1000                # max number of episode samples to be sampled per time step. 
+    episode_iterations = 2048         # number of batches per episode
+    noptepochs = 12                      # Number of epochs per time step to optimize the neural networks
+    learning_rate_p = 1e-4               # learning rate for policy network
+    learning_rate_v = 1e-3               # learning rate for value network
+    gae_lambda = 0.95                    # episode discount for the general advantage estimation (GAE)
+    gamma = 0.99                         # discount factor
+    adam_epsilon = 1e-8                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
+    epsilon = 0.2                        # clipping factor
+    env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
+    env_number = 1                       # number of actors
+    seed = 42                            # seed gym, env, torch, numpy 
     
     # setup for torch save models and rendering
     render = True
@@ -940,9 +954,9 @@ if __name__ == '__main__':
             config={ # stores hyperparams in job
                 'env name': env_name,
                 'env number': env_number,
-                'total number of steps': total_timesteps,
+                'total number of steps': total_training_steps,
                 'max sampled trajectories': max_batch_size,
-                'batches per episode': trajectory_iterations,
+                'batches per episode': episode_iterations,
                 'number of epochs for update': noptepochs,
                 'input layer size': obs_dim,
                 'output layer size': act_dim,
@@ -968,9 +982,9 @@ if __name__ == '__main__':
         train(env,
             in_dim=obs_dim, 
             out_dim=act_dim,
-            total_timesteps=total_timesteps,
+            total_training_steps=total_training_steps,
             max_batch_size=max_batch_size,
-            trajectory_iterations=trajectory_iterations,
+            episode_iterations=episode_iterations,
             noptepochs=noptepochs,
             learning_rate_p=learning_rate_p, 
             learning_rate_v=learning_rate_v,
@@ -982,12 +996,13 @@ if __name__ == '__main__':
             save_steps=save_steps,
             csv_writer=csv_writer,
             stats_plotter=stats_plotter,
-            log_video=args.video)
+            log_video=args.video,
+            device=device)
     
     elif args.test:
         logging.info('Evaluation model...')
-        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
-        test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
+        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth' # TODO: define path
+        test(PATH, env, in_dim=obs_dim, out_dim=act_dim, device=device)
     
     elif args.hyperparam:
         # hyperparameter tuning with sweeps
diff --git a/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png b/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png
deleted file mode 100644
index 66bc6854..00000000
Binary files a/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png and /dev/null differ
diff --git a/PPO/ppo_torch/wrapper/stats_logger.py b/PPO/ppo_torch/wrapper/stats_logger.py
index 8fa3c60b..ab640d5b 100644
--- a/PPO/ppo_torch/wrapper/stats_logger.py
+++ b/PPO/ppo_torch/wrapper/stats_logger.py
@@ -4,6 +4,10 @@ import seaborn as sns
 import matplotlib.pyplot as plt
 import os
 
+# switch style
+sns.set_theme()
+sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
+
 class StatsLogger:
     """ Log all statistical values over the runtime
     """
@@ -43,19 +47,20 @@ class StatsPlotter:
         df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
         return df
 
-    def plot(self, dataframe, kind='line', hue='', x='x-label', y='y-label', title='title', upper_bound=None, lower_bound=None):
+    def plot(self, dataframe, x='x-label', y='y-label', title='title', upper_bound=0, lower_bound=-1800):
         """ Create a lineplot with seaborn.
             Doc: https://seaborn.pydata.org/tutorial/introduction
         """
-        line_plot = sns.relplot(
-            data=dataframe, kind=kind,
-            x=x, y=y, hue=hue, height=7, aspect=.7)
-        line_plot.set(title=title)
-        # draw a horizontal line
-        if upper_bound:
-            line_plot.axhline(upper_bound)
-        if lower_bound:
-            line_plot.axhline(lower_bound)
+        line_plot = sns.relplot(data=dataframe, x=x, y=y, hue=y, kind='line')
+
+        (line_plot
+            # draw line into log
+            .map(plt.axhline, y=upper_bound, color=".7", dashes=(2, 1), zorder=0)
+            .map(plt.axhline, y=lower_bound, color=".7", dashes=(2, 1), zorder=0)
+            .set_axis_labels("Timestep", "Mean Episodic Return")
+            .set_titles("exp: {title}")
+            .tight_layout(w_pad=0))
+
         # plot the file to given destination
         file_path = self.result_path + self.img_name
         line_plot.figure.savefig(file_path)
