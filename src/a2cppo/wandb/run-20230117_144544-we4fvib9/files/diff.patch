diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index c6cc461f..4a3763da 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -1021,22 +1021,22 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 2_000_000     # time steps regarding batches collected and train agent
-    batch_size = 512                     # max number of episode samples to be sampled per time step. 
+    total_training_steps = 2_500_000     # time steps regarding batches collected and train agent
+    batch_size = 1024                    # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
     n_optepochs = 32                     # Number of epochs per time step to optimize the neural networks
     learning_rate_p = 1e-4               # learning rate for policy network
-    learning_rate_v = 1e-3               # learning rate for value network
+    learning_rate_v = 1e-5               # learning rate for value network
     gae_lambda = 0.95                    # factor for trade-off of bias vs variance for GAE
-    gamma = 0.99                         # discount factor
-    adam_epsilon = 1e-8                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
+    gamma = 0.96                         # discount factor
+    adam_epsilon = 1e-7                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
     epsilon = 0.2                        # clipping factor
     clip_range_vf = 0.2                  # clipping factor for the value loss function. Depends on reward scaling.
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0', 'takeoff-aviary-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_adv = True                # wether to normalize the advantage estimate
-    normalize_ret = True                 # wether to normalize the return function
+    normalize_adv = True                 # wether to normalize the advantage estimate
+    normalize_ret = False                # wether to normalize the return function
     
     # setup for torch save models and rendering
     render_video = False
@@ -1165,14 +1165,14 @@ if __name__ == '__main__':
         """automatically run through multiple train runs"""
         
         param_dict = {
-                # 'learning rate (policy net)': [1e-5, 1e-4, 1e-3, 1e-2],
-                # 'learning rate (value net)': [1e-5, 1e-4, 1e-3, 1e-2],
-                # 'gamma (discount)': [0.95, 0.96, 0.97, 0.98, 0.99],
-                # 'epsilon (clip_range)': [0.1, 0.2, 0.3],
-                # 'gae lambda (GAE)': [0.9, 0.93, 0.95, 0.96, 0.99, 1.0],
-                # 'epsilon (adam optimizer)': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],
-                'number of epochs for update': [8, 16, 32, 64, 128],
-                'max sampled trajectories': [32, 64, 128, 256, 512, 1024]
+                'learning rate (policy net)': [1e-5, 1e-4, 1e-3, 1e-2],
+                'learning rate (value net)': [1e-5, 1e-4, 1e-3, 1e-2],
+                'gamma (discount)': [0.95, 0.96, 0.97, 0.98, 0.99],
+                'epsilon (clip_range)': [0.1, 0.2, 0.3],
+                'gae lambda (GAE)': [0.9, 0.93, 0.95, 0.96, 0.97, 0.99, 1.0],
+                'epsilon (adam optimizer)': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],
+                'number of epochs for update': [8, 16, 32, 64, 128, 256],
+                'max sampled trajectories': [32, 64, 128, 256, 512, 1024, 2048, 4096]
         }
 
         # iterate over settings
@@ -1187,7 +1187,7 @@ if __name__ == '__main__':
                             env = make_env(env_name, seed=seed)
 
                             # Settup project
-                            exp_name = f"exp_name: {env_name}_{CURR_DATE} ({key}: {val[i]})"
+                            exp_name = f"exp_name: {env_name}_{CURR_DATE} (GAE, {key}: {val[i]})"
                             exp_folder_name = f'{LOG_PATH}exp_{env_name}_{CURR_TIME}'
                             create_path(exp_folder_name)
                             # create model folder
