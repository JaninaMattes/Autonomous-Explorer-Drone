diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index ebfe0bbc..9bc3e497 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -21,6 +21,10 @@ import sys
 import wandb
 from wrapper.stats_logger import CSVWriter
 
+# hyperparameter tuning
+import optuna
+from optuna.integration.wandb import WeightsAndBiasesCallback
+
 # Paths and other constants
 MODEL_PATH = './models/'
 LOG_PATH = './log/'
@@ -28,18 +32,19 @@ VIDEO_PATH = './video/'
 
 CURR_DATE = datetime.today().strftime('%Y-%m-%d')
 
+
 ####################
 ####### TODO #######
 ####################
 
 # Hint: Please if working on it mark a todo as (done) if done
 # 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Check calculation of rewards --> correct mean reward over episodes? 
 # 3) Check calculation of advantage 
 
 ####################
 ####################
 
+
 class Net(nn.Module):
     def __init__(self) -> None:
         super(Net, self).__init__()
@@ -102,8 +107,64 @@ class PolicyNet(Net):
 ####################
 ####################
 
+class PPO_PolicyGradient_V1:
 
-class PPO_PolicyGradient:
+    def __init__(self, 
+        env, 
+        in_dim, 
+        out_dim,
+        total_steps,
+        max_trajectory_size,
+        trajectory_iterations,
+        noptepochs=5,
+        lr_p=1e-3,
+        lr_v=1e-3,
+        gae_lambda=0.95,
+        gamma=0.99,
+        epsilon=0.22,
+        adam_eps=1e-5,
+        render=10,
+        save_model=10,
+        csv_writer=None,
+        log_video=False) -> None:
+        
+        # hyperparams
+        self.in_dim = in_dim
+        self.out_dim = out_dim
+        self.total_steps = total_steps
+        self.max_trajectory_size = max_trajectory_size
+        self.trajectory_iterations = trajectory_iterations
+        self.noptepochs = noptepochs
+        self.lr_p = lr_p
+        self.lr_v = lr_v
+        self.gamma = gamma
+        self.epsilon = epsilon
+        self.adam_eps = adam_eps
+        self.gae_lambda = gae_lambda
+
+        # environment
+        self.env = env
+        self.render_steps = render
+        self.save_model = save_model
+
+        # track video of gym
+        self.log_video = log_video
+
+        # keep track of rewards per episode
+        self.ep_returns = deque(maxlen=max_trajectory_size)
+        self.csv_writer = csv_writer
+        self.stats_data = {'mean episodic length': [], 'mean episodic rewards': [], 'timestep': []}
+
+        # add net for actor and critic
+        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
+        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic) -  (value-based method) "How good the action taken is."
+
+        # add optimizer for actor and critic
+        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
+        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
+
+
+class PPO_PolicyGradient_V2:
     """ Proximal Policy Optimization (PPO) is an online policy gradient method.
         As an online policy method it updates the policy and then discards the experience (no replay buffer).
         Thus the agent does well in environments with dense reward signals.
@@ -128,7 +189,7 @@ class PPO_PolicyGradient:
         gamma=0.99,
         epsilon=0.22,
         adam_eps=1e-5,
-        render=1,
+        render=10,
         save_model=10,
         csv_writer=None,
         log_video=False) -> None:
@@ -217,56 +278,12 @@ class PPO_PolicyGradient:
         return torch.tensor(cum_returns, dtype=torch.float)
 
     def advantage_estimate_(self, returns, values, normalized=True):
+        """Calculate delta, which is defined by delta = r - v """
         advantages = returns - values # delta = r - v
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages
 
-    def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
-        """ Calculate advantage as a weighted average of A_t
-                - advantage measures if an action is better or worse than the policy's default behavior
-                - GAE allows to balance bias and variance through a weighted average of A_t
-
-                - gamma (dicount factor): allows reduce variance by downweighting rewards that correspond to delayed effects
-                - done (Tensor): boolean flag for end of episode. TODO: Q&A
-        """
-        # general advantage estimage paper: https://arxiv.org/pdf/1506.02438.pdf
-        # general advantage estimage other: https://nn.labml.ai/rl/ppo/gae.html
-
-        advantages = []
-        returns = []
-        values = values.detach().numpy()
-        for rewards in reversed(batch_rewards): # reversed order
-            prev_advantage = 0
-            discounted_reward = 0
-            last_value = values[-1] # V(s_t+1)
-            for i in reversed(range(len(rewards))):
-                # TODO: Q&A handling of special cases GAE(γ, 0) and GAE(γ, 1)
-                # bei Vetorisierung, bei kurzen Episoden (done flag)
-                # mask if episode completed after step i 
-                mask = 1.0 - dones[i] 
-                last_value = last_value * mask
-                prev_advantage = prev_advantage * mask
-
-                # TD residual of V with discount gamma
-                # δ_t = − V(s_t) + r_t + γ * V(s_t+1)
-                # TODO: Delta Könnte als advantage ausreichen, r - v könnte ausreichen
-                delta = - values[i] + rewards[i] + (self.gamma * last_value)
-                # discounted sum of Bellman residual term
-                # A_t = δ_t + γ * λ * A(t+1)
-                prev_advantage = delta + self.gamma * self.gae_lambda * prev_advantage
-                discounted_reward = rewards[i] + (self.gamma * discounted_reward)
-                returns.insert(0, discounted_reward) # reverse it again
-                advantages.insert(0, prev_advantage) # reverse it again
-                # store current value as V(s_t+1)
-                last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
-        returns = torch.tensor(np.array(returns), dtype=torch.float)
-        if normalized:
-            advantages = self.normalize_adv(advantages)
-        return advantages, returns
-
-
     def advantage_estimate(self, next_obs, obs, batch_rewards, dones, normalized=True):
         """Calculating advantage estimate using TD error.
             - done: only get reward at end of episode, not disounted next state value
@@ -293,15 +310,24 @@ class PPO_PolicyGradient:
             - advantage measures if an action is better or worse than the policy's default behavior
             - want to find the maximum Advantage representing the benefit of choosing a specific action
         """
-        # STEP 5: compute advantage estimates A_t at step t
+        # check if tensor and convert to numpy
+        if torch.is_tensor(batch_rewards):
+            batch_rewards = batch_rewards.detach().numpy()
+        if torch.is_tensor(values):
+            values = values.detach().numpy()
+
+        # STEP 4: compute G(t) = R(t) + gamma * R(t-1)
+        # STEP 5: compute advantage estimates δ_t = − V(s_t) + r_t
         cum_returns = []
         advantages = []
         for rewards in reversed(batch_rewards): # reversed order
             discounted_reward = 0
             for i in reversed(range(len(rewards))):
                 discounted_reward = rewards[i] + (self.gamma * discounted_reward)
-                advantage = discounted_reward - values[i] # delta = r - v
-                advantages.insert(0, advantage)
+                # hinweis @Thomy Delta Könnte als advantage ausreichen
+                # δ_t = − V(s_t) + r_t
+                delta = discounted_reward - values[i] # delta = r - v
+                advantages.insert(0, delta)
                 cum_returns.insert(0, discounted_reward) # reverse it again
         cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
         advantages = torch.tensor(np.array(advantages), dtype=torch.float)
@@ -309,6 +335,7 @@ class PPO_PolicyGradient:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
 
+
     def generalized_advantage_estimate_2(self, obs, next_obs, batch_rewards, dones, normalized=True):
         """ Generalized Advantage Estimate calculation
             - GAE defines advantage as a weighted average of A_t
@@ -343,6 +370,52 @@ class PPO_PolicyGradient:
             advantages = self.normalize_adv(advantages)
         return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
 
+
+    def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
+        """ Calculate advantage as a weighted average of A_t
+                - advantage measures if an action is better or worse than the policy's default behavior
+                - GAE allows to balance bias and variance through a weighted average of A_t
+
+                - gamma (dicount factor): allows reduce variance by downweighting rewards that correspond to delayed effects
+                - done (Tensor): boolean flag for end of episode. TODO: Q&A
+        """
+        # general advantage estimage paper: https://arxiv.org/pdf/1506.02438.pdf
+        # general advantage estimage other: https://nn.labml.ai/rl/ppo/gae.html
+
+        advantages = []
+        returns = []
+        values = values.detach().numpy()
+        for rewards in reversed(batch_rewards): # reversed order
+            prev_advantage = 0
+            discounted_reward = 0
+            last_value = values[-1] # V(s_t+1)
+            for i in reversed(range(len(rewards))):
+                # TODO: Q&A handling of special cases GAE(γ, 0) and GAE(γ, 1)
+                # bei Vetorisierung, bei kurzen Episoden (done flag)
+                # mask if episode completed after step i 
+                mask = 1.0 - dones[i] 
+                last_value = last_value * mask
+                prev_advantage = prev_advantage * mask
+
+                # TD residual of V with discount gamma
+                # δ_t = − V(s_t) + r_t + γ * V(s_t+1)
+                # TODO: Delta Könnte als advantage ausreichen, r - v könnte ausreichen
+                delta = - values[i] + rewards[i] + (self.gamma * last_value)
+                # discounted sum of Bellman residual term
+                # A_t = δ_t + γ * λ * A(t+1)
+                prev_advantage = delta + self.gamma * self.gae_lambda * prev_advantage
+                discounted_reward = rewards[i] + (self.gamma * discounted_reward)
+                returns.insert(0, discounted_reward) # reverse it again
+                advantages.insert(0, prev_advantage) # reverse it again
+                # store current value as V(s_t+1)
+                last_value = values[i]
+        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        returns = torch.tensor(np.array(returns), dtype=torch.float)
+        if normalized:
+            advantages = self.normalize_adv(advantages)
+        return advantages, returns
+
+
     def normalize_adv(self, advantages):
         return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
     
@@ -389,7 +462,7 @@ class PPO_PolicyGradient:
                         
                 # STEP 3: collecting set of trajectories D_k by running action 
                 # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
+                __obs, reward, done, info = self.env.step(action)
 
                 # collection of trajectories in batches
                 trajectory_obs.append(obs)
@@ -450,9 +523,9 @@ class PPO_PolicyGradient:
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            cum_returns = self.cummulative_return(rewards)
-            advantages = self.advantage_estimate_(cum_returns, values.detach())
-            # advantages, cum_returns = self.generalized_advantage_estimate_1(rewards, values.detach())
+            # cum_returns = self.cummulative_return(rewards)
+            # advantages = self.advantage_estimate_(cum_returns, values.detach())
+            advantages, cum_returns = self.generalized_advantage_estimate_1(rewards, values.detach())
 
             # update network params 
             for _ in range(self.noptepochs):
@@ -546,6 +619,8 @@ def arg_parser():
         help="if toggled, run model in training mode")
     parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="if toggled, run model in testing mode")
+    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, log hyperparameters")
     parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
         help="the name of this experiment")
     parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
@@ -658,7 +733,7 @@ def train(env, in_dim, out_dim, total_steps, max_trajectory_size, trajectory_ite
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
           adam_epsilon, render_steps, save_steps, csv_writer, log_video=False):
     """Train the policy network (actor) and the value network (critic) with PPO"""
-    agent = PPO_PolicyGradient(
+    agent = PPO_PolicyGradient_V2(
                 env, 
                 in_dim=in_dim, 
                 out_dim=out_dim,
@@ -692,6 +767,27 @@ def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False)
         if log_video:
             wandb.log({"test/video": wandb.Video(VIDEO_PATH, caption='episode: '+str(ep_num), fps=4, format="gif"), "step": ep_num})
 
+def hyperparam_tuning(config=None):
+    # set config
+    with wandb.init(config=config):
+        agent = PPO_PolicyGradient_V2(
+                env,
+                in_dim=config.obs_dim, 
+                out_dim=config.act_dim,
+                total_steps=config.total_steps,
+                max_trajectory_size=config.max_trajectory_size,
+                trajectory_iterations=config.trajectory_iterations,
+                noptepochs=config.noptepochs,
+                gae_lambda = config.gae_lambda,
+                gamma=config.gamma,
+                epsilon=config.epsilon,
+                adam_eps=config.adam_epsilon,
+                lr_p=config.learning_rate_p,
+                lr_v=config.learning_rate_v)
+    
+        # run training for a total amount of steps
+        agent.learn()
+
 
 if __name__ == '__main__':
     
@@ -767,10 +863,10 @@ if __name__ == '__main__':
 
     # Monitoring with W&B
     wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
+            project=f'drone-mechanics-ppo-OpenAIGym',
+            entity='drone-mechanics',
+            sync_tensorboard=True,
+            config={ # stores hyperparams in job
             'total number of steps': total_steps,
             'max sampled trajectories': max_trajectory_size,
             'batches per episode': trajectory_iterations,
@@ -782,15 +878,13 @@ if __name__ == '__main__':
             'epsilon (adam optimizer)': adam_epsilon,
             'gamma (discount)': gamma,
             'epsilon (clipping)': epsilon,
+            'gae lambda (GAE)': gae_lambda,
             'seed': seed
         },
-    name=f"exp_name: {env_name}_{CURR_DATE}",
-    monitor_gym=True,
-    save_code=True
-    )
-
-    # monitor gym
-    wandb.gym.monitor()
+            name=f"exp_name: {env_name}_{CURR_DATE}",
+            monitor_gym=True,
+            save_code=True
+        )
 
     if args.train:
         logging.info('Training model...')
@@ -817,6 +911,42 @@ if __name__ == '__main__':
         PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
         test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
     
+    elif args.hyperparam:
+        logging.info('Hyperparameter tuning...')
+        # sweep config
+        sweep_config = {
+            'method': 'bayes'
+            }
+        metric = {
+            'name': 'mean_ep_rews',
+            'goal': 'maximize'   
+            }
+        parameters_dict = {
+            'learning_rate_p': {
+                'values': [1e-5, 1e-4, 1e-3, 1e-2]
+            },
+            'learning_rate_v': {
+                'values': [1e-5, 1e-4, 1e-3, 1e-2]
+            },
+            'gamma': {
+                'values': [0.95, 0.96, 0.97, 0.98, 0.99]
+            },
+            'adam_epsilon': {
+                'values': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
+            },
+            'epsilon': {
+                'values': [0.1, 0.2, 0.25, 0.3, 0.4]
+            },
+            }
+
+        # set defined parameters
+        sweep_config['parameters'] = parameters_dict
+        sweep_config['metric'] = metric
+
+        # run sweep with sweep controller
+        sweep_id = wandb.sweep(sweep_config, project="ppo-OpenAIGym-hyperparam-tuning")
+        wandb.agent(sweep_id, hyperparam_tuning, count=5)
+
     else:
         assert("Needs training (--train) or testing (--test) flag set!")
 
