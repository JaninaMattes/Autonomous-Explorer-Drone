diff --git a/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv b/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
index 1f53afe7..121ddc6d 100644
--- a/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
+++ b/PPO/ppo_torch/log/exp_Pendulum-v1_20230112-230348/Pendulum-v1_20230112-230348.csv
@@ -401,3 +401,56 @@
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",880000,0.035997,200.0,11,-833.371375,-1006.385232,-748.792749,84.414518
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",882200,0.035903,200.0,11,-858.969178,-992.490244,-737.422276,88.048019
 0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",884400,0.035936,200.0,11,-833.454353,-1076.09056,-630.415911,117.493608
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",886600,0.038754,200.0,11,-857.078007,-980.811077,-752.516913,71.570946
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",888800,0.038257,200.0,11,-876.095189,-1105.845301,-746.1719,122.322252
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",891000,0.03673,200.0,11,-839.162794,-969.127477,-738.130164,72.744673
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",893200,0.036408,200.0,11,-842.19847,-1061.209549,-624.200784,119.360579
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",895400,0.036677,200.0,11,-917.76437,-1022.485926,-752.777606,91.843686
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",897600,0.036618,200.0,11,-870.279145,-1077.013351,-663.909212,135.130288
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",899800,0.039166,200.0,11,-818.156357,-1083.651878,-627.972492,132.034869
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",902000,0.037677,200.0,11,-780.094618,-967.940425,-629.884505,110.910147
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",904200,0.036356,200.0,11,-753.906084,-971.151297,-517.400992,118.363642
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",906400,0.036633,200.0,11,-845.319073,-1042.529031,-627.870358,124.470976
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",908600,0.03635,200.0,11,-809.9515,-1007.868424,-652.024548,103.093718
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",910800,0.036115,200.0,11,-870.078368,-1158.810354,-748.115539,136.552901
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",913000,0.039106,200.0,11,-893.785059,-1034.406016,-665.398243,96.955644
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",915200,0.037282,200.0,11,-894.055817,-1092.717992,-750.782596,124.781175
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",917400,0.036134,200.0,11,-895.442175,-1006.91564,-768.010515,77.671126
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",919600,0.036313,200.0,11,-861.203935,-977.75027,-720.897296,77.066715
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",921800,0.036191,200.0,11,-859.540887,-1037.434027,-638.688797,104.619191
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",924000,0.036389,200.0,11,-810.79916,-1010.469437,-632.240027,137.22701
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",926200,0.039543,200.0,11,-854.306071,-1230.008874,-632.043718,148.938971
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",928400,0.037522,200.0,11,-865.472109,-1136.622667,-645.062059,132.914213
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",930600,0.036315,200.0,11,-844.78535,-1141.099626,-636.891747,142.123732
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",932800,0.037085,200.0,11,-795.260196,-890.409398,-508.622309,110.27884
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",935000,0.036921,200.0,11,-786.2414,-946.581973,-657.388571,73.849423
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",937200,0.036463,200.0,11,-854.245854,-1106.55928,-694.647837,124.879004
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",939400,0.039078,200.0,11,-856.084112,-1016.192372,-498.359577,132.074899
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",941600,0.037322,200.0,11,-805.215081,-977.443023,-634.704329,97.374283
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",943800,0.037056,200.0,11,-810.465872,-964.729632,-624.223022,109.68309
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",946000,0.037802,200.0,11,-906.576345,-1103.605694,-753.411969,108.749588
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",948200,0.038704,200.0,11,-806.098964,-955.124984,-660.799117,82.460255
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",950400,0.038542,200.0,11,-798.920901,-966.351483,-657.195904,85.468548
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",952600,0.039449,200.0,11,-821.929234,-897.724971,-739.409329,68.437043
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",954800,0.037567,200.0,11,-866.603211,-1057.149498,-744.717274,96.52451
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",957000,0.038671,200.0,11,-855.263938,-1033.412353,-629.336216,151.418982
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",959200,0.03735,200.0,11,-784.942257,-1047.490388,-624.223858,125.519983
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",961400,0.038646,200.0,11,-794.168942,-1022.534528,-622.399333,109.357904
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",963600,0.038163,200.0,11,-873.476111,-1065.73287,-751.61934,108.826687
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",965800,0.040247,200.0,11,-842.248058,-1128.518475,-632.253506,133.30544
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",968000,0.04183,200.0,11,-875.157421,-1133.79506,-686.700712,142.863003
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",970200,0.036948,200.0,11,-795.079274,-965.433539,-631.628935,85.458706
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",972400,0.038347,200.0,11,-850.973606,-1150.810043,-630.95158,133.111151
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",974600,0.037154,200.0,11,-922.554718,-1175.163883,-749.419444,109.723578
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",976800,0.038381,200.0,11,-816.554044,-945.983093,-630.646995,87.212889
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",979000,0.038924,200.0,11,-827.563709,-1010.481885,-669.048055,99.560132
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",981200,0.038884,200.0,11,-808.271064,-978.510334,-626.974863,106.880283
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",983400,0.036206,200.0,11,-854.834684,-1042.752105,-514.831189,151.580617
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",985600,0.037356,200.0,11,-845.963295,-881.016269,-738.150227,40.801569
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",987800,0.036428,200.0,11,-823.854304,-1040.046524,-621.108176,112.620593
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",990000,0.038333,200.0,11,-823.652742,-975.010229,-637.735261,95.782509
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",992200,0.039628,200.0,11,-812.199598,-959.24321,-636.647322,95.722072
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",994400,0.03642,200.0,11,-841.928176,-1105.321961,-703.533603,134.140078
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",996600,0.036456,200.0,11,-770.435797,-936.660575,-635.10654,70.62929
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",998800,0.037045,200.0,11,-779.643216,-1041.303149,-633.472395,117.670294
+0,"exp_name: Pendulum-v1_2023-01-12 (seperate network, actor-critic advantage)",1001000,0.037158,200.0,11,-803.910809,-956.557849,-641.002719,83.079681
diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index fba62e12..406f1d14 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -220,8 +220,8 @@ class PPO_PolicyGradient_V2:
         device='cpu',
         exp_path='./log/',
         exp_name='PPO_V2_experiment',
-        normalize_advantage=False,
-        normalize_returns=False) -> None:
+        normalize_adv=False,
+        normalize_ret=False) -> None:
         
         # hyperparams
         self.in_dim = in_dim
@@ -242,8 +242,8 @@ class PPO_PolicyGradient_V2:
         self.render_steps = render
         self.save_model = save_model
         self.device = device
-        self.normalize_advantage = normalize_advantage
-        self.normalize_returns = normalize_returns
+        self.normalize_adv = normalize_adv
+        self.normalize_ret = normalize_ret
 
         # keep track of information
         self.exp_path = exp_path
@@ -318,7 +318,7 @@ class PPO_PolicyGradient_V2:
                 cum_returns.insert(0, discounted_reward) # reverse it again
         return torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
 
-    def advantage_estimate(self, batch_rewards, values, normalize_advantage=False, normalized_return=False):
+    def advantage_estimate(self, batch_rewards, values, normalize_adv=False, normalized_return=False):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
             - bias-variance: TD has low variance, but IS biased
@@ -332,7 +332,7 @@ class PPO_PolicyGradient_V2:
         # TD error: A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
         cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
         advantages = cum_returns - values
-        if normalize_advantage:
+        if normalize_adv:
             advantages = self.normalize_adv(advantages)
         if normalized_return:
             cum_returns = self.normalize_ret(cum_returns)
@@ -628,7 +628,7 @@ class PPO_PolicyGradient_V2:
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_returns)
+            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_adv, normalized_ret=self.normalize_ret)
 
             # update network params 
             for _ in range(self.noptepochs):
@@ -739,28 +739,18 @@ def arg_parser():
     parser = argparse.ArgumentParser()
     # fmt: off
     parser = argparse.ArgumentParser()
-    parser.add_argument("--video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False,
-        help="if toggled, capture video of run")
-    parser.add_argument("--train", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
-        help="if toggled, run model in training mode")
-    parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False,
-        help="if toggled, run model in testing mode")
-    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
-        help="if toggled, log hyperparameters")
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False, help="if toggled, capture video of run")
+    parser.add_argument("--train", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True, help="if toggled, run model in training mode")
+    parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=False, help="if toggled, run model in testing mode")
+    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True, help="if toggled, log hyperparameters")
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"), help="the name of this experiment")
+    parser.add_argument("--project-name", type=str, default='OpenAIGym-PPO', help="the name of this project") 
+    parser.add_argument("--gym-id", type=str, default="Pendulum-v1", help="the id of the gym environment")
+    parser.add_argument("--learning-rate", type=float, default=3e-4, help="the learning rate of the optimizer")
+    parser.add_argument("--seed", type=int, default=1, help="seed of the experiment")
+    parser.add_argument("--total-timesteps", type=int, default=2000000, help="total timesteps of the experiments")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True, help="if toggled, cuda will be enabled by default")
     
     # Parse arguments if they are given
     args = parser.parse_args()
@@ -858,7 +848,7 @@ def _log_summary(ep_len, ep_ret, ep_num):
 def train(env, in_dim, out_dim, total_training_steps, max_batch_size, n_rollout_steps,
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
           adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter,
-          normalize_advantage=False, normalize_returns=False,
+          normalize_adv=False, normalize_ret=False,
           log_video=False, ppo_version='v2', device='cpu', exp_path='./log/', exp_name='PPO-experiment'):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = None
@@ -877,8 +867,8 @@ def train(env, in_dim, out_dim, total_training_steps, max_batch_size, n_rollout_
                     gamma=gamma,
                     epsilon=epsilon,
                     adam_eps=adam_epsilon,
-                    normalize_advantage=normalize_advantage,
-                    normalize_returns=normalize_returns,
+                    normalize_adv=normalize_adv,
+                    normalize_ret=normalize_ret,
                     render=render_steps,
                     save_model=save_steps,
                     csv_writer=csv_writer,
@@ -960,7 +950,7 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 1_000_000     # time steps regarding batches collected and train agent
+    total_training_steps = 3_000_000     # time steps regarding batches collected and train agent
     max_batch_size = 512                 # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
     noptepochs = 12                      # Number of epochs per time step to optimize the neural networks
@@ -974,8 +964,8 @@ if __name__ == '__main__':
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_advantage = True           # wether to normalize the advantage estimate
-    normalize_returns = False            # wether to normalize the return function
+    normalize_adv = False                # wether to normalize the advantage estimate
+    normalize_ret = False                # wether to normalize the return function
     # setup for torch save models and rendering
     render = True
     render_steps = 10
@@ -1032,7 +1022,7 @@ if __name__ == '__main__':
 
     # Monitoring with W&B
     wandb.init(
-            project=f'drone-mechanics-ppo-OpenAIGym',
+            project=args.project_name,
             entity='drone-mechanics',
             sync_tensorboard=True,
             config={ # stores hyperparams in job
@@ -1054,8 +1044,8 @@ if __name__ == '__main__':
                 'gamma (discount)': gamma,
                 'epsilon (clipping)': epsilon,
                 'gae lambda (GAE)': gae_lambda,
-                'normalize advantage': normalize_advantage,
-                'normalize return': normalize_returns,
+                'normalize advantage': normalize_adv,
+                'normalize return': normalize_ret,
                 'seed': seed,
                 'experiment path': exp_folder_name,
                 'experiment name': args.exp_name
@@ -1080,8 +1070,8 @@ if __name__ == '__main__':
             gamma=gamma,
             epsilon=epsilon,
             adam_epsilon=adam_epsilon,
-            normalize_advantage=normalize_advantage,
-            normalize_returns=normalize_returns,
+            normalize_adv=normalize_adv,
+            normalize_ret=normalize_ret,
             render_steps=render_steps,
             save_steps=save_steps,
             csv_writer=csv_writer,
