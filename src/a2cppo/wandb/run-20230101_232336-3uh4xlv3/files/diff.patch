diff --git a/.gitignore b/.gitignore
index 2d5f4324..b9ee61da 100644
--- a/.gitignore
+++ b/.gitignore
@@ -37,7 +37,7 @@ sysinfo.txt
 
 # Monitoring
 */wandb/*
-ppo_impl/ppo_torch/wandb/*
+PPO/ppo_torch/wandb/
 
 # Checkpoints
 # IPython notebook checkpoints
diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index 617da7b1..fa8e5a6e 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -224,6 +224,7 @@ class PPO_PolicyGradient:
 
         advantages = []
         returns = []
+        values = values.detach().numpy()
         for rewards in reversed(batch_rewards): # reversed order
             prev_advantage = 0
             discounted_reward = 0
@@ -248,10 +249,12 @@ class PPO_PolicyGradient:
                 advantages.insert(0, prev_advantage) # reverse it again
                 # store current value as V(s_t+1)
                 last_value = values[i]
-        advantages = np.array(advantages)
+        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        returns = torch.tensor(np.array(returns), dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
+            #returns = self.normalize_ret(returns)
+        return advantages, returns
 
 
     def advantage_estimate(self, next_obs, obs, batch_rewards, dones, normalized=True):
@@ -322,7 +325,10 @@ class PPO_PolicyGradient:
 
     def normalize_adv(self, advantages):
         return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        
+    
+    def normalize_ret(self, returns):
+        return (returns - returns.mean()) / returns.std()
+
     def finish_episode(self):
         pass 
 
@@ -424,10 +430,10 @@ class PPO_PolicyGradient:
             steps += np.sum(batch_lens)
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
-            values, curr_log_probs , _ = self.get_values(obs, actions)
-            # cum_returns = self.cummulative_return(batch_rewards)
-            # advantages = self.advantage_estimate(cum_returns, values.detach())
-            advantages, cum_returns = self.generalized_advantage_estimate_3(batch_rewards, values, dones)
+            values, _ , _ = self.get_values(obs, actions)
+            cum_returns = self.cummulative_return(batch_rewards)
+            advantages = self.generalized_advantage_estimate_1(cum_returns, values.detach())
+            #advantages, cum_returns = self.generalized_advantage_estimate_3(batch_rewards, values, dones)
             #advantages = self.generalized_advantage_estimate_1(cum_returns, values)
 
             # update network params 
@@ -439,7 +445,7 @@ class PPO_PolicyGradient:
                 policy_losses.append(policy_loss.detach().numpy())
                 value_losses.append(value_loss.detach().numpy())
 
-            self.log_stats(policy_losses, value_losses, cum_returns, batch_lens, steps)
+            self.log_stats(policy_losses, value_losses, batch_rewards, batch_lens, steps)
 
             # store model in checkpoints
             if steps % self.save_model == 0:
@@ -466,6 +472,8 @@ class PPO_PolicyGradient:
 
     def log_stats(self, p_losses, v_losses, batch_return, batch_lens, steps):
         """Calculate stats and log to W&B, CSV, logger """
+        if torch.is_tensor(batch_return):
+            batch_return = batch_return.detach().numpy()
         # calculate stats
         mean_p_loss = np.mean([np.sum(loss) for loss in p_losses])
         mean_v_loss = np.mean([np.sum(loss) for loss in v_losses])
@@ -598,7 +606,7 @@ if __name__ == '__main__':
     seed = 42                       # seed gym, env, torch, numpy 
     
     # setup for torch save models and rendering
-    render_steps = 100
+    render_steps = 10
     save_steps = 100
 
     # Configure logger
