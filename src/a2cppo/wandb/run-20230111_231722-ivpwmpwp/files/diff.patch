diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index c22fb37e..b4eee5df 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -61,7 +61,7 @@ class ValueNet(Net):
     
     def forward(self, obs):
         if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
+            obs = torch.tensor(obs, device=self.device, dtype=torch.float)
         x = self.relu(self.layer1(obs))
         x = self.relu(self.layer2(x))
         out = self.layer3(x) # head has linear activation
@@ -83,7 +83,7 @@ class PolicyNet(Net):
 
     def forward(self, obs):
         if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
+            obs = torch.tensor(obs, device=self.device, dtype=torch.float)
         x = self.relu(self.layer1(obs))
         x = self.relu(self.layer2(x))
         out = self.layer3(x) # head has linear activation (continuous space)
@@ -114,9 +114,9 @@ class PPO_PolicyGradient_V1:
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -128,14 +128,15 @@ class PPO_PolicyGradient_V1:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -148,6 +149,7 @@ class PPO_PolicyGradient_V1:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
         # track video of gym
         self.log_video = log_video
@@ -183,9 +185,9 @@ class PPO_PolicyGradient_V2:
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -197,14 +199,15 @@ class PPO_PolicyGradient_V2:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -217,6 +220,7 @@ class PPO_PolicyGradient_V2:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
         # track video of gym
         self.log_video = log_video
@@ -229,9 +233,10 @@ class PPO_PolicyGradient_V2:
                            'timestep': [], 
                            'mean episodic length': [], 
                            'mean episodic returns': [], 
-                           'info/mean episodic returns': [], 
-                           'info/mean episodic length': [], 
-                           'info/mean episodic time': []
+                           'std episodic returns': [], 
+                        #    'info/mean episodic returns': [], 
+                        #    'info/mean episodic length': [], 
+                        #    'info/mean episodic time': []
                         }
 
         # add net for actor and critic
@@ -283,38 +288,33 @@ class PPO_PolicyGradient_V2:
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        return torch.tensor(np.array(cum_returns), dtype=torch.float)
+        return torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
 
     def advantage_estimate(self, batch_rewards, values, normalized=True):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
+            - bias-variance: TD has low variance, but IS biased
             - dones: only get reward at end of episode, not disounted next state value
         """
-        # check if tensor and convert to numpy
-        if torch.is_tensor(batch_rewards):
-            batch_rewards = batch_rewards.detach().numpy()
-        if torch.is_tensor(values):
-            values = values.detach().numpy()
-
         advantages = []
-        last_value = values[-1]
+        cum_returns = []
         for rewards in reversed(batch_rewards):  # reversed order
-            for i in reversed(range(len(rewards))):
-                # TD error: A(s,a) = r + gamma * V(s_t+1) - V(s_t)
-                temp_pred = gamma * last_value - values[i]
-                advantage = rewards[i] + temp_pred
-                advantages.insert(0, advantage)
-                last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+            for reward in reversed(rewards):
+                cum_returns.insert(0, reward) # reverse it again
+        # TD error: A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = cum_returns - values
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return advantages
+        return advantages, cum_returns
 
     def generalized_advantage_estimate_0(self, batch_rewards, values, normalized=True):
         """ Generalized Advantage Estimate calculation
             Calculate delta, which is defined by delta = r - v 
         """
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
         for rewards in reversed(batch_rewards): # reversed order
@@ -322,8 +322,9 @@ class PPO_PolicyGradient_V2:
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = cum_returns - values # delta = r - v
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        # delta = G(t) - V(s_t)
+        advantages = cum_returns - values 
         # normalize for more stability
         if normalized:
             advantages = self.normalize_adv(advantages)
@@ -356,8 +357,8 @@ class PPO_PolicyGradient_V2:
                 cum_returns.insert(0, discounted_reward) # reverse it again
 
         # convert numpy to torch tensor
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
@@ -395,7 +396,9 @@ class PPO_PolicyGradient_V2:
         advantages = np.array(advantages)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
+        return advantages, returns
 
 
     def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
@@ -436,8 +439,9 @@ class PPO_PolicyGradient_V2:
                 advantages.insert(0, prev_advantage) # reverse it again
                 # store current value as V(s_t+1)
                 last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
-        returns = torch.tensor(np.array(returns), dtype=torch.float)
+
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, returns
@@ -452,26 +456,26 @@ class PPO_PolicyGradient_V2:
     def finish_episode(self):
         pass 
 
-    def collect_rollout(self, n_step=1, render=False):
+    def collect_rollout(self, n_steps=1, render=False):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        step, trajectory_rewards = 0, []
+        step, episode_rewards = 0, []
 
         # collect trajectories
-        trajectory_obs = []
-        trajectory_nextobs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_dones = []
+        episode_obs = []
+        episode_nextobs = []
+        episode_actions = []
+        episode_action_probs = []
+        episode_dones = []
         batch_rewards = []
         batch_lens = []
 
         # Run Monte Carlo simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
-        while step < n_step:
+        while step < n_steps:
             
             # rewards collected per episode
-            trajectory_rewards, done = [], False 
+            episode_rewards, done = [], False 
             obs = self.env.reset()
 
             # Run episode for a fixed amount of timesteps
@@ -492,41 +496,43 @@ class PPO_PolicyGradient_V2:
                 __obs, reward, done, info = self.env.step(action)
 
                 # collection of trajectories in batches
-                trajectory_obs.append(obs)
-                trajectory_nextobs.append(__obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_dones.append(done)
+                episode_obs.append(obs)
+                episode_nextobs.append(__obs)
+                episode_actions.append(action)
+                episode_action_probs.append(log_probability)
+                episode_rewards.append(reward)
+                episode_dones.append(done)
                     
                 obs = __obs
 
                 # break out of loop if episode is terminated
                 if done:
-                    info_episode = info[0]['episode']
-                    info_episode_r = info_episode['r']
-                    info_episode_l = info_episode['l']
-                    info_episode_t = info_episode['t']
-                    self.stats_data['info/mean episodic returns'].append(info_episode_r)
-                    self.stats_data['info/mean episodic length'].append(info_episode_l)
-                    self.stats_data['info/mean episodic time'].append(info_episode_t)
-
-                    wandb.log({
-                        'info/mean episodic returns': info_episode_r,
-                        'info/mean episodic length': info_episode_l,
-                        'info/mean episodic time': info_episode_t
-                    })
+                    # logging of stats
+                    # info_episode = info[0]['episode']
+                    # info_episode_r = info_episode['r']
+                    # info_episode_l = info_episode['l']
+                    # info_episode_t = info_episode['t']
+                    # self.stats_data['info/mean episodic returns'].append(info_episode_r)
+                    # self.stats_data['info/mean episodic length'].append(info_episode_l)
+                    # self.stats_data['info/mean episodic time'].append(info_episode_t)
+
+                    # wandb.log({
+                    #     'info/mean episodic returns': info_episode_r,
+                    #     'info/mean episodic length': info_episode_l,
+                    #     'info/mean episodic time': info_episode_t
+                    # })
+
                     break
 
             batch_lens.append(ep_t + 1) # as we started at 0
-            batch_rewards.append(trajectory_rewards)
+            batch_rewards.append(episode_rewards)
 
         # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        next_obs = torch.tensor(np.array(trajectory_nextobs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        action_log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        dones = torch.tensor(np.array(trajectory_dones), dtype=torch.float)
+        obs = torch.tensor(np.array(episode_obs), device=self.device, dtype=torch.float)
+        next_obs = torch.tensor(np.array(episode_nextobs), device=self.device, dtype=torch.float)
+        actions = torch.tensor(np.array(episode_actions), device=self.device, dtype=torch.float)
+        action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
+        dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
         return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens
                 
@@ -552,20 +558,20 @@ class PPO_PolicyGradient_V2:
         """"""
         steps = 0
 
-        while steps < self.total_timesteps:
+        while training_steps < self.total_training_steps:
             policy_losses, value_losses = [], []
-            # Collect trajectory
+            # Collect episode
             # STEP 3: simulate and collect trajectories --> the following values are all per batch
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens = self.collect_rollout(n_step=self.trajectory_iterations)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens = self.collect_rollout(n_steps=self.episode_iterations)
 
             # timesteps simulated so far for batch collection
-            steps += np.sum(ep_lens)
+            training_steps += np.sum(ep_lens)
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            cum_returns = self.cummulative_return(rewards)
+            #cum_returns = self.cummulative_return(rewards)
             # advantages = self.advantage_estimate_(cum_returns, values.detach())
-            #advantages, cum_returns = self.generalized_advantage_estimate_0(rewards, values.detach())
+            # advantages, cum_returns = self.generalized_advantage_estimate_0(rewards, values.detach())
             advantages = self.advantage_estimate(rewards, values)
             # update network params 
             for _ in range(self.noptepochs):
@@ -721,9 +727,9 @@ def create_path(path: str) -> None:
     if not os.path.exists(path):
         os.makedirs(path)
 
-def load_model(path, model):
+def load_model(path, model, device='cpu'):
     checkpoint = torch.load(path)
-    model.load_state_dict(checkpoint['model_state_dict'])
+    model.load_state_dict(checkpoint['model_state_dict'], map_location=device)
     return model
 
 def simulate_rollout(policy_net, env, render=True):
@@ -776,9 +782,10 @@ def _log_summary(ep_len, ep_ret, ep_num):
         logging.info(f"--------------------------------------------")
         logging.info('\n')
 
-def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iterations,
+def train(env, in_dim, out_dim, total_training_steps, max_batch_size, episode_iterations,
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
-          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, log_video=False, ppo_version='v2'):
+          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, 
+          log_video=False, ppo_version='v2', device='cpu'):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = None
     if ppo_version == 'v2':
@@ -786,9 +793,9 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -800,15 +807,16 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device)
     else:
         agent = PPO_PolicyGradient_V1(
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -820,15 +828,16 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device)
     # run training for a total amount of steps
     agent.learn()
 
-def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False):
+def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False, device='cpu'):
     """Test the policy network (actor)"""
     # load model and test it
     policy_net = PolicyNet(in_dim, out_dim)
-    policy_net = load_model(path, policy_net)
+    policy_net = load_model(path, policy_net, device)
     
     for ep_num, (ep_len, ep_ret) in enumerate(simulate_rollout(policy_net, env, render)):
         _log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)
@@ -843,9 +852,9 @@ def hyperparam_tuning(config=None):
                 env,
                 in_dim=config.obs_dim, 
                 out_dim=config.act_dim,
-                total_timesteps=config.total_timesteps,
+                total_training_steps=config.total_training_steps,
                 max_batch_size=config.max_batch_size,
-                trajectory_iterations=config.trajectory_iterations,
+                episode_iterations=config.episode_iterations,
                 noptepochs=config.noptepochs,
                 gae_lambda = config.gae_lambda,
                 gamma=config.gamma,
@@ -874,19 +883,19 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_timesteps = 3_500_000     # time steps regarding batches collected and train agent
-    max_batch_size = 1000      # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 2048    # number of batches per episode
-    noptepochs = 12                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gae_lambda = 0.95               # trajectory discount for the general advantage estimation (GAE)
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-8             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
-    env_number = 1                  # number of actors
-    seed = 42                       # seed gym, env, torch, numpy 
+    total_training_steps = 3_500_000     # time steps regarding batches collected and train agent
+    max_batch_size = 1000                # max number of episode samples to be sampled per time step. 
+    episode_iterations = 2048         # number of batches per episode
+    noptepochs = 12                      # Number of epochs per time step to optimize the neural networks
+    learning_rate_p = 1e-4               # learning rate for policy network
+    learning_rate_v = 1e-3               # learning rate for value network
+    gae_lambda = 0.95                    # episode discount for the general advantage estimation (GAE)
+    gamma = 0.99                         # discount factor
+    adam_epsilon = 1e-8                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
+    epsilon = 0.2                        # clipping factor
+    env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
+    env_number = 1                       # number of actors
+    seed = 42                            # seed gym, env, torch, numpy 
     
     # setup for torch save models and rendering
     render = True
@@ -940,9 +949,9 @@ if __name__ == '__main__':
             config={ # stores hyperparams in job
                 'env name': env_name,
                 'env number': env_number,
-                'total number of steps': total_timesteps,
+                'total number of steps': total_training_steps,
                 'max sampled trajectories': max_batch_size,
-                'batches per episode': trajectory_iterations,
+                'batches per episode': episode_iterations,
                 'number of epochs for update': noptepochs,
                 'input layer size': obs_dim,
                 'output layer size': act_dim,
@@ -968,9 +977,9 @@ if __name__ == '__main__':
         train(env,
             in_dim=obs_dim, 
             out_dim=act_dim,
-            total_timesteps=total_timesteps,
+            total_training_steps=total_training_steps,
             max_batch_size=max_batch_size,
-            trajectory_iterations=trajectory_iterations,
+            episode_iterations=episode_iterations,
             noptepochs=noptepochs,
             learning_rate_p=learning_rate_p, 
             learning_rate_v=learning_rate_v,
@@ -982,12 +991,13 @@ if __name__ == '__main__':
             save_steps=save_steps,
             csv_writer=csv_writer,
             stats_plotter=stats_plotter,
-            log_video=args.video)
+            log_video=args.video,
+            device=device)
     
     elif args.test:
         logging.info('Evaluation model...')
-        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
-        test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
+        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth' # TODO: define path
+        test(PATH, env, in_dim=obs_dim, out_dim=act_dim, device=device)
     
     elif args.hyperparam:
         # hyperparameter tuning with sweeps
