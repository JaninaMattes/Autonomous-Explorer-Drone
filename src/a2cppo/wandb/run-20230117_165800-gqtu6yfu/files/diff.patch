diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index f5908cb7..a0c179c0 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -680,8 +680,8 @@ class PPO_PolicyGradient_V2:
             # STEP 4-5: Calculate cummulated reward and advantage at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
             # Calculate advantage function
-            # advantages, cum_returns = self.advantage_reinforce(rewards, normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            advantages, cum_returns = self.advantage_reinforce(rewards, normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            # advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             # advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             # advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             
@@ -1035,7 +1035,7 @@ if __name__ == '__main__':
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0', 'takeoff-aviary-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_adv = True                 # wether to normalize the advantage estimate
+    normalize_adv = False                 # wether to normalize the advantage estimate
     normalize_ret = True                # wether to normalize the return function
     
     # setup for torch save models and rendering
