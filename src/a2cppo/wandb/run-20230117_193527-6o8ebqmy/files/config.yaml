wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.13.8
    code_path: code/PPO/ppo_torch/ppo_continuous.py
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.10.8
    start_time: 1673980527.561241
    t:
      1:
      - 1
      - 35
      - 55
      2:
      - 1
      - 35
      - 55
      3:
      - 3
      - 13
      - 16
      - 23
      - 35
      4: 3.10.8
      5: 0.13.8
      8:
      - 4
      - 5
action space:
  desc: null
  value:
  - 1
action space lower bound:
  desc: null
  value: -2.0
action space upper bound:
  desc: null
  value: 2.0
batches per episode:
  desc: null
  value: 2048
env name:
  desc: null
  value: Pendulum-v1
env number:
  desc: null
  value: 1
epsilon (adam optimizer):
  desc: null
  value: 1.0e-07
epsilon (clip_range):
  desc: null
  value: 0.2
experiment name:
  desc: null
  value: (actor-critic advantage, new hyperparams, normalize adv)
experiment path:
  desc: null
  value: ./log/exp_Pendulum-v1_20230117-193526
gae lambda (GAE):
  desc: null
  value: 0.9
gamma (discount):
  desc: null
  value: 0.96
input layer size:
  desc: null
  value: 3
learning rate (policy net):
  desc: null
  value: 0.0001
learning rate (value net):
  desc: null
  value: 0.001
max sampled trajectories:
  desc: null
  value: 1024
normalize advantage:
  desc: null
  value: true
normalize return:
  desc: null
  value: false
number of epochs for update:
  desc: null
  value: 32
observation space:
  desc: null
  value:
  - 3
output layer size:
  desc: null
  value: 1
seed:
  desc: null
  value: 42
total_training_steps:
  desc: null
  value: 1200000
