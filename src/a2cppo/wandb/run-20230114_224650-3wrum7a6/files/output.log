INFO:root:Training model...
INFO:root:Collecting batch trajectories...
INFO:root:Updating network parameter...
Traceback (most recent call last):
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 1098, in <module>
    train(env,
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 939, in train
    agent.learn()
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 667, in learn
    policy_loss, value_loss = self.train(values, cum_returns, advantages, batch_log_probs, curr_log_probs, self.epsilon)
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 629, in train
    policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 107, in loss
    clip_1 = ratio * advantages
RuntimeError: The size of tensor a (2200) must match the size of tensor b (200) at non-singleton dimension 0