INFO:root:Training model...
INFO:root:Collecting batch trajectories...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
INFO:root:Updating network parameter...
Traceback (most recent call last):
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 971, in <module>
    train(env,
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 828, in train
    agent.learn()
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 583, in learn
    self.log_stats(policy_losses, value_losses, rewards, ep_lens, steps)
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 637, in log_stats
    self.stats_data['std episodic returns'].append(std_ep_rew)
KeyError: 'std episodic returns'