diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index f5908cb7..6b6cd8f2 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -14,6 +14,11 @@ from stable_baselines3 import PPO
 # gym environment
 import gym
 
+# video logging
+from matplotlib import animation
+import matplotlib.pyplot as plt
+import numpy as np
+
 # logging python
 import logging
 import sys
@@ -250,6 +255,7 @@ class PPO_PolicyGradient_V2:
         csv_writer=None,
         stats_plotter=None,
         log_video=False,
+        video_log_steps=100,
         device='cpu',
         exp_path='./log/',
         exp_name='PPO_V2_experiment',
@@ -286,6 +292,7 @@ class PPO_PolicyGradient_V2:
         self.exp_name = exp_name
         # track video of gym
         self.log_video = log_video
+        self.video_log_steps = video_log_steps
 
         # keep track of rewards per episode
         self.ep_returns = deque(maxlen=batch_size)
@@ -565,7 +572,7 @@ class PPO_PolicyGradient_V2:
     def collect_rollout(self, n_steps=1):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        t_step, rewards = 0, []
+        t_step, rewards, frames = 0, [], deque(maxlen=24) # 4 fps - 6 sec
 
         # log time
         episode_time = []
@@ -584,7 +591,7 @@ class PPO_PolicyGradient_V2:
         while t_step < n_steps:
             
             # rewards collected
-            rewards, done = [], False 
+            rewards, done, frames = [], False, []
             obs = self.env.reset()
 
             # measure time elapsed for one episode
@@ -596,7 +603,7 @@ class PPO_PolicyGradient_V2:
             for t_batch in range(0, self.batch_size):
                 # render gym envs
                 if self.render_video and t_batch % self.render_steps == 0:
-                    self.env.render()
+                    frames.append(self.env.render(mode="rgb_array"))
                 
                 t_step += 1 
 
@@ -642,7 +649,7 @@ class PPO_PolicyGradient_V2:
         action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
         dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time)
+        return obs, next_obs, actions, action_log_probs, dones, episode_rewards, episode_lens, np.array(episode_time), frames
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -672,7 +679,7 @@ class PPO_PolicyGradient_V2:
             # Collect data over one episode
             # Episode = recording of actions and states that an agent performed from a start state to an end state
             # STEP 3: simulate and collect trajectories --> the following values are all per batch over one episode
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.n_rollout_steps)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time, frames = self.collect_rollout(n_steps=self.n_rollout_steps)
 
             # experiences simulated so far
             training_steps += np.sum(ep_lens)
@@ -681,9 +688,9 @@ class PPO_PolicyGradient_V2:
             values, _ , _ = self.get_values(obs, actions)
             # Calculate advantage function
             # advantages, cum_returns = self.advantage_reinforce(rewards, normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            # advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             # advantages, cum_returns = self.advantage_TD_actor_critic(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
-            # advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
+            advantages, cum_returns = self.generalized_advantage_estimate(rewards, values.detach(), normalized_adv=self.normalize_advantage, normalized_ret=self.normalize_return)
             
             # update network params 
             for _ in range(self.n_optepochs):
@@ -696,7 +703,7 @@ class PPO_PolicyGradient_V2:
 
             # log all statistical values to CSV
             self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time, done_so_far, exp_name=self.exp_name)
-            
+
             # increment for each iteration
             done_so_far += 1
 
@@ -729,6 +736,16 @@ class PPO_PolicyGradient_V2:
                     for value in self.stats_data.values():
                         del value[:]
 
+                # Log to video
+                if self.render_video and done_so_far % self.video_log_steps == 0:
+                    filename='pendulum_v1.gif'
+                    self.save_frames_as_gif(frames, self.exp_path, filename)
+                    wandb.log({
+                        "train/video": wandb.Video(os.path.join(self.exp_path, filename), 
+                        caption='episode: '+str(done_so_far), 
+                        fps=4, format="gif"), "step": done_so_far
+                        })
+
         # Finalize and plot stats
         if self.stats_plotter:
             df = self.stats_plotter.read_csv() # read all files in folder
@@ -745,6 +762,20 @@ class PPO_PolicyGradient_V2:
             # Save any files starting with "ppo"
             wandb.save(os.path.join(wandb.run.dir, "ppo*"))
 
+    def save_frames_as_gif(self, frames, path='./', filename='pendulum_v1.gif'):
+
+        #Mess with this to change frame size
+        plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)
+
+        patch = plt.imshow(frames[0])
+        plt.axis('off')
+
+        def animate(i):
+            patch.set_data(frames[i])
+
+        anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)
+        video_path = os.path.join(path, filename)
+        anim.save(video_path, writer='imagemagick', fps=60)
 
     def log_stats(self, p_losses, v_losses, batch_return, episode_lens, training_steps, time, done_so_far, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
@@ -917,7 +948,7 @@ def _log_summary(ep_len, ep_ret, ep_num):
 def train(env, in_dim, out_dim, total_training_steps, batch_size=512, n_rollout_steps=2048,
           n_optepochs=32, learning_rate_p=1e-4, learning_rate_v=1e-3, gae_lambda=0.95, gamma=0.99, epsilon=0.2,
           adam_epsilon=1e-8, render_steps=10, render_video=False, save_steps=10, csv_writer=None, stats_plotter=None,
-          normalize_adv=False, normalize_ret=False, log_video=False, ppo_version='v2', 
+          normalize_adv=False, normalize_ret=False, log_video=False, video_log_steps=1000, ppo_version='v2', 
           device='cpu', exp_path='./log/', exp_name='PPO-experiment'):
     """Train the policy network (actor) and the value network (critic) with PPO (clip version)"""
     agent = None
@@ -944,6 +975,7 @@ def train(env, in_dim, out_dim, total_training_steps, batch_size=512, n_rollout_
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
                     log_video=log_video,
+                    video_log_steps=video_log_steps,
                     device=device,
                     exp_path=exp_path,
                     exp_name=exp_name)
@@ -1021,25 +1053,26 @@ if __name__ == '__main__':
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_training_steps = 1_500_000     # time steps regarding batches collected and train agent
+    total_training_steps = 1_200_000     # time steps regarding batches collected and train agent
     batch_size = 1024                    # max number of episode samples to be sampled per time step. 
     n_rollout_steps = 2048               # number of batches per episode, or experiences to collect per environment
-    n_optepochs = 32                     # Number of epochs per time step to optimize the neural networks
+    n_optepochs = 64                     # Number of epochs per time step to optimize the neural networks
     learning_rate_p = 1e-4               # learning rate for policy network
     learning_rate_v = 1e-3               # learning rate for value network
-    gae_lambda = 0.9                     # factor for trade-off of bias vs variance for GAE
-    gamma = 0.96                         # discount factor
+    gae_lambda = 0.94                    # factor for trade-off of bias vs variance for GAE
+    gamma = 0.95                         # discount factor
     adam_epsilon = 1e-7                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
     epsilon = 0.2                        # clipping factor
     clip_range_vf = 0.2                  # clipping factor for the value loss function. Depends on reward scaling.
     env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0', 'takeoff-aviary-v0'
     env_number = 1                       # number of actors
     seed = 42                            # seed gym, env, torch, numpy 
-    normalize_adv = True                 # wether to normalize the advantage estimate
+    normalize_adv = False                 # wether to normalize the advantage estimate
     normalize_ret = True                # wether to normalize the return function
     
     # setup for torch save models and rendering
     render_video = False
+    video_log_steps = 1000
     render_steps = 10
     save_steps = 100
 
@@ -1152,6 +1185,7 @@ if __name__ == '__main__':
             csv_writer=csv_writer,
             stats_plotter=stats_plotter,
             log_video=args.video,
+            video_log_steps=video_log_steps,
             device=device,
             exp_path=exp_folder_name,
             exp_name=exp_name)
