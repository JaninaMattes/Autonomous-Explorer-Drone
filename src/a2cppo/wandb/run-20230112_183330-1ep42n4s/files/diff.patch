diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index c22fb37e..2ab5ff44 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -1,4 +1,5 @@
 from collections import deque
+import time
 import torch
 from torch import nn
 from torch.optim import Adam
@@ -31,7 +32,9 @@ LOG_PATH = './log/'
 VIDEO_PATH = './video/'
 RESULTS_PATH = './results/'
 
+# get current date and time
 CURR_DATE = datetime.today().strftime('%Y-%m-%d')
+CURR_TIME = datetime.now().strftime("%Y%m%d-%H%M%S")
 
 
 ####################
@@ -68,8 +71,11 @@ class ValueNet(Net):
         return out
     
     def loss(self, values, returns):
-        """Objective function defined by mean-squared error"""
-        # return 0.5 * ((rewards - values)**2).mean() # MSE loss
+        """ Objective function defined by mean-squared error.
+            ValueNet is approximated via regression.
+            Regression target y(t) is defined by Bellman equation or G(t) sample return
+        """
+        # return 0.5 * ((returns - values)**2).mean() # MSE loss
         return nn.MSELoss()(values, returns)
 
 class PolicyNet(Net):
@@ -96,7 +102,8 @@ class PolicyNet(Net):
                 - The minimum is taken, so that the gradient will pull π_new towards π_OLD 
                   if the ratio is not between 1-ϵ and 1+ϵ.
         """
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
+        # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
+        ratio = torch.exp(curr_log_probs - batch_log_probs)
         clip_1 = ratio * advantages
         clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
         policy_loss = (-torch.min(clip_1, clip_2)) # negative as Adam mins loss, but we want to max it
@@ -114,9 +121,9 @@ class PPO_PolicyGradient_V1:
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -128,14 +135,15 @@ class PPO_PolicyGradient_V1:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -148,6 +156,7 @@ class PPO_PolicyGradient_V1:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
         # track video of gym
         self.log_video = log_video
@@ -156,7 +165,16 @@ class PPO_PolicyGradient_V1:
         self.ep_returns = deque(maxlen=max_batch_size)
         self.csv_writer = csv_writer
         self.stats_plotter = stats_plotter
-        self.stats_data = {'experiment': [], 'timestep': [], 'mean episodic length': [], 'mean episodic returns': [], 'std episodic returns': []}
+        self.stats_data = {
+            'experiment': [], 
+            'timestep': [],
+            'mean episodic runtime': [],
+            'mean episodic length': [], 
+            'mean episodic returns': [],
+            'min episodic returns': [],
+            'max episodic returns': [],
+            'std episodic returns': [],
+            }
 
         # add net for actor and critic
         self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
@@ -177,15 +195,15 @@ class PPO_PolicyGradient_V2:
     # Further reading
     # PPO experiments: https://nn.labml.ai/rl/ppo/experiment.html
     #                  https://nn.labml.ai/rl/ppo/index.html
-    # PPO explained: https://huggingface.co/blog/deep-rl-ppo
+    # PPO explained:   https://huggingface.co/blog/deep-rl-ppo
 
     def __init__(self, 
         env, 
         in_dim, 
         out_dim,
-        total_timesteps,
+        total_training_steps,
         max_batch_size,
-        trajectory_iterations,
+        episode_iterations,
         noptepochs=5,
         lr_p=1e-3,
         lr_v=1e-3,
@@ -197,14 +215,17 @@ class PPO_PolicyGradient_V2:
         save_model=10,
         csv_writer=None,
         stats_plotter=None,
-        log_video=False) -> None:
+        log_video=False,
+        device='cpu',
+        exp_path='./log/',
+        exp_name='PPO_V2_experiment') -> None:
         
         # hyperparams
         self.in_dim = in_dim
         self.out_dim = out_dim
-        self.total_timesteps = total_timesteps
+        self.total_training_steps = total_training_steps
         self.max_batch_size = max_batch_size
-        self.trajectory_iterations = trajectory_iterations
+        self.episode_iterations = episode_iterations
         self.noptepochs = noptepochs
         self.lr_p = lr_p
         self.lr_v = lr_v
@@ -217,7 +238,11 @@ class PPO_PolicyGradient_V2:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+        self.device = device
 
+        # keep track of information
+        self.exp_path = exp_path
+        self.exp_name = exp_name
         # track video of gym
         self.log_video = log_video
 
@@ -225,14 +250,16 @@ class PPO_PolicyGradient_V2:
         self.ep_returns = deque(maxlen=max_batch_size)
         self.csv_writer = csv_writer
         self.stats_plotter = stats_plotter
-        self.stats_data = {'experiment': [], 
-                           'timestep': [], 
-                           'mean episodic length': [], 
-                           'mean episodic returns': [], 
-                           'info/mean episodic returns': [], 
-                           'info/mean episodic length': [], 
-                           'info/mean episodic time': []
-                        }
+        self.stats_data = {
+            'experiment': [], 
+            'timestep': [],
+            'mean episodic runtime': [],
+            'mean episodic length': [], 
+            'mean episodic returns': [],
+            'min episodic returns': [],
+            'max episodic returns': [],
+            'std episodic returns': [],
+            }
 
         # add net for actor and critic
         self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor) - (policy-based method) "How the agent behaves"
@@ -283,47 +310,62 @@ class PPO_PolicyGradient_V2:
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        return torch.tensor(np.array(cum_returns), dtype=torch.float)
+        return torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
 
     def advantage_estimate(self, batch_rewards, values, normalized=True):
         """ Calculating advantage estimate using TD error (Temporal Difference Error).
             TD Error can be used as an estimator for Advantage function,
+            - bias-variance: TD has low variance, but IS biased
             - dones: only get reward at end of episode, not disounted next state value
         """
-        # check if tensor and convert to numpy
-        if torch.is_tensor(batch_rewards):
-            batch_rewards = batch_rewards.detach().numpy()
-        if torch.is_tensor(values):
-            values = values.detach().numpy()
-
         advantages = []
-        last_value = values[-1]
+        cum_returns = []
         for rewards in reversed(batch_rewards):  # reversed order
-            for i in reversed(range(len(rewards))):
-                # TD error: A(s,a) = r + gamma * V(s_t+1) - V(s_t)
-                temp_pred = gamma * last_value - values[i]
-                advantage = rewards[i] + temp_pred
-                advantages.insert(0, advantage)
-                last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+            for reward in reversed(rewards):
+                cum_returns.insert(0, reward) # reverse it again
+        # TD error: A(s,a) = r + (gamma * V(s_t+1)) - V(s_t)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = cum_returns - values
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return advantages
+        return advantages, cum_returns
 
-    def generalized_advantage_estimate_0(self, batch_rewards, values, normalized=True):
-        """ Generalized Advantage Estimate calculation
+    def advantage_reinforce(self, batch_rewards, values, normalized=True):
+        """ Advantage Reinforce A(s_t, a_t) = G(t)
+        """
+        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # G(t) is the total disounted reward
+        # return value: G(t) = R(t) + gamma * R(t-1)
+        cum_returns = []
+        for rewards in reversed(batch_rewards): # reversed order
+            discounted_reward = 0
+            for reward in reversed(rewards):
+                # R + discount * estimated return from the next step taking action a'
+                discounted_reward = reward + (self.gamma * discounted_reward)
+                cum_returns.insert(0, discounted_reward) # reverse it again
+        advantages = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        # normalize for more stability
+        if normalized:
+            advantages = self.normalize_adv(advantages)
+        return advantages, cum_returns
+
+    def advantage_actor_critic(self, batch_rewards, values, normalized=True):
+        """ Advantage Actor-Critic A(s_t, a_t) = G(t) - V(s_t)
             Calculate delta, which is defined by delta = r - v 
         """
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
+        # G(t) is the total disounted reward
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
         for rewards in reversed(batch_rewards): # reversed order
             discounted_reward = 0
             for reward in reversed(rewards):
+                # R + discount * estimated return from the next step taking action a'
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = cum_returns - values # delta = r - v
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        # delta = G(t) - V(s_t)
+        advantages = cum_returns - values 
         # normalize for more stability
         if normalized:
             advantages = self.normalize_adv(advantages)
@@ -356,8 +398,8 @@ class PPO_PolicyGradient_V2:
                 cum_returns.insert(0, discounted_reward) # reverse it again
 
         # convert numpy to torch tensor
-        cum_returns = torch.tensor(np.array(cum_returns), dtype=torch.float)
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        cum_returns = torch.tensor(np.array(cum_returns), device=self.device, dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, cum_returns
@@ -385,7 +427,8 @@ class PPO_PolicyGradient_V2:
                 # STEP 5: compute advantage estimates A_t at step t
                 mask = (1.0 - dones[i])
                 gamma = self.gamma * mask
-                td_error = rewards[i] + gamma * ns_values[i] - s_values[i]
+                td_target = rewards[i] + (gamma * ns_values[i])
+                td_error = td_target - s_values[i]
                 # A_t = δ_t + γ * λ * A(t+1)
                 prev_advantage = td_error + gamma * self.gae_lambda * prev_advantage
                 returns_current = rewards[i] + gamma * returns_current
@@ -395,7 +438,9 @@ class PPO_PolicyGradient_V2:
         advantages = np.array(advantages)
         if normalized:
             advantages = self.normalize_adv(advantages)
-        return torch.tensor(np.array(advantages), dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
+        return advantages, returns
 
 
     def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
@@ -436,8 +481,9 @@ class PPO_PolicyGradient_V2:
                 advantages.insert(0, prev_advantage) # reverse it again
                 # store current value as V(s_t+1)
                 last_value = values[i]
-        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
-        returns = torch.tensor(np.array(returns), dtype=torch.float)
+
+        advantages = torch.tensor(np.array(advantages), device=self.device, dtype=torch.float)
+        returns = torch.tensor(np.array(returns), device=self.device, dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages, returns
@@ -452,36 +498,43 @@ class PPO_PolicyGradient_V2:
     def finish_episode(self):
         pass 
 
-    def collect_rollout(self, n_step=1, render=False):
+    def collect_rollout(self, n_steps=1, render=False):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        step, trajectory_rewards = 0, []
+        t_step, episode_rewards = 0, []
+
+        # log time
+        episode_time = []
 
         # collect trajectories
-        trajectory_obs = []
-        trajectory_nextobs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_dones = []
+        episode_obs = []
+        episode_nextobs = []
+        episode_actions = []
+        episode_action_probs = []
+        episode_dones = []
         batch_rewards = []
         batch_lens = []
 
         # Run Monte Carlo simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
-        while step < n_step:
+        while t_step < n_steps:
             
             # rewards collected per episode
-            trajectory_rewards, done = [], False 
+            episode_rewards, done = [], False 
             obs = self.env.reset()
 
+            # measure time elapsed for one episode
+            # torch.cuda.synchronize()
+            start_epoch = time.time()
+
             # Run episode for a fixed amount of timesteps
             # to keep rollout size fixed and episodes independent
-            for ep_t in range(0, self.max_batch_size):
+            for t_episode in range(0, self.max_batch_size):
                 # render gym envs
-                if render and ep_t % self.render_steps == 0:
+                if render and t_episode % self.render_steps == 0:
                     self.env.render()
                 
-                step += 1 
+                t_step += 1 
 
                 # action logic 
                 # sampled via policy which defines behavioral strategy of an agent
@@ -489,46 +542,40 @@ class PPO_PolicyGradient_V2:
                         
                 # STEP 3: collecting set of trajectories D_k by running action 
                 # that was sampled from policy in environment
-                __obs, reward, done, info = self.env.step(action)
+                __obs, reward, done, truncated = self.env.step(action)
 
                 # collection of trajectories in batches
-                trajectory_obs.append(obs)
-                trajectory_nextobs.append(__obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_dones.append(done)
+                episode_obs.append(obs)
+                episode_nextobs.append(__obs)
+                episode_actions.append(action)
+                episode_action_probs.append(log_probability)
+                episode_rewards.append(reward)
+                episode_dones.append(done)
                     
                 obs = __obs
 
                 # break out of loop if episode is terminated
-                if done:
-                    info_episode = info[0]['episode']
-                    info_episode_r = info_episode['r']
-                    info_episode_l = info_episode['l']
-                    info_episode_t = info_episode['t']
-                    self.stats_data['info/mean episodic returns'].append(info_episode_r)
-                    self.stats_data['info/mean episodic length'].append(info_episode_l)
-                    self.stats_data['info/mean episodic time'].append(info_episode_t)
-
-                    wandb.log({
-                        'info/mean episodic returns': info_episode_r,
-                        'info/mean episodic length': info_episode_l,
-                        'info/mean episodic time': info_episode_t
-                    })
+                if done or truncated:
                     break
+            
+            # stop time per episode
+            # Waits for everything to finish running
+            # torch.cuda.synchronize()
+            end_epoch = time.time()
+            time_elapsed = end_epoch - start_epoch
+            episode_time.append(time_elapsed)
 
-            batch_lens.append(ep_t + 1) # as we started at 0
-            batch_rewards.append(trajectory_rewards)
+            batch_lens.append(t_episode + 1) # as we started at 0
+            batch_rewards.append(episode_rewards)
 
         # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        next_obs = torch.tensor(np.array(trajectory_nextobs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        action_log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        dones = torch.tensor(np.array(trajectory_dones), dtype=torch.float)
+        obs = torch.tensor(np.array(episode_obs), device=self.device, dtype=torch.float)
+        next_obs = torch.tensor(np.array(episode_nextobs), device=self.device, dtype=torch.float)
+        actions = torch.tensor(np.array(episode_actions), device=self.device, dtype=torch.float)
+        action_log_probs = torch.tensor(np.array(episode_action_probs), device=self.device, dtype=torch.float)
+        dones = torch.tensor(np.array(episode_dones), device=self.device, dtype=torch.float)
 
-        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens
+        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens, np.array(episode_time)
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -550,23 +597,21 @@ class PPO_PolicyGradient_V2:
 
     def learn(self):
         """"""
-        steps = 0
+        training_steps = 0
 
-        while steps < self.total_timesteps:
+        while training_steps < self.total_training_steps:
             policy_losses, value_losses = [], []
-            # Collect trajectory
+            # Collect episode
             # STEP 3: simulate and collect trajectories --> the following values are all per batch
-            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens = self.collect_rollout(n_step=self.trajectory_iterations)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, ep_lens, ep_time = self.collect_rollout(n_steps=self.episode_iterations)
 
             # timesteps simulated so far for batch collection
-            steps += np.sum(ep_lens)
+            training_steps += np.sum(ep_lens)
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            cum_returns = self.cummulative_return(rewards)
-            # advantages = self.advantage_estimate_(cum_returns, values.detach())
-            #advantages, cum_returns = self.generalized_advantage_estimate_0(rewards, values.detach())
-            advantages = self.advantage_estimate(rewards, values)
+            advantages, cum_returns = self.advantage_actor_critic(rewards, values.detach())
+
             # update network params 
             for _ in range(self.noptepochs):
                 # STEP 6-7: calculate loss and update weights
@@ -577,21 +622,22 @@ class PPO_PolicyGradient_V2:
                 value_losses.append(value_loss.detach().numpy())
 
             # log all statistical values to CSV
-            self.log_stats(policy_losses, value_losses, rewards, ep_lens, steps)
+            self.log_stats(policy_losses, value_losses, rewards, ep_lens, training_steps, ep_time, exp_name=self.exp_name)
 
             # store model in checkpoints
-            if steps % self.save_model == 0:
+            if training_steps % self.save_model == 0:
                 env_name = self.env.unwrapped.spec.id
-                policy_net_name = f'{MODEL_PATH}{env_name}_{CURR_DATE}_policyNet.pth'
-                value_net_name = f'{MODEL_PATH}{env_name}_{CURR_DATE}_valueNet.pth'
+                model_path = os.path.join(LOG_PATH, MODEL_PATH)
+                policy_net_name = f'{model_path}{env_name}_{CURR_TIME}_policyNet.pth'
+                value_net_name = f'{model_path}{env_name}_{CURR_TIME}_valueNet.pth'
                 torch.save({
-                    'epoch': steps,
+                    'epoch': training_steps,
                     'model_state_dict': self.policy_net.state_dict(),
                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
                     'loss': policy_loss,
                     }, policy_net_name)
                 torch.save({
-                    'epoch': steps,
+                    'epoch': training_steps,
                     'model_state_dict': self.value_net.state_dict(),
                     'optimizer_state_dict': self.value_net_optim.state_dict(),
                     'loss': value_loss,
@@ -609,50 +655,59 @@ class PPO_PolicyGradient_V2:
 
         # Finalize and plot stats
         if self.stats_plotter:
-            df = self.stats_plotter.read_csv(CURR_DATE) # just select files from current date
-            self.stats_plotter.plot(df, kind='line', hue='std episodic returns', x='timestep', y='mean episodic returns', title=env_name)
+            df = self.stats_plotter.read_csv() # read all files in folder
+            self.stats_plotter.plot_seaborn(df, x='timestep', y='mean episodic returns', hue='experiment', title=f'{env_name}')
 
-    def log_stats(self, p_losses, v_losses, batch_return, batch_lens, steps, exp_name='experiment'):
+    def log_stats(self, p_losses, v_losses, batch_return, batch_lens, training_steps, time, exp_name='experiment'):
         """Calculate stats and log to W&B, CSV, logger """
         if torch.is_tensor(batch_return):
             batch_return = batch_return.detach().numpy()
         # calculate stats
-        mean_p_loss = np.mean([np.sum(loss) for loss in p_losses])
-        mean_v_loss = np.mean([np.sum(loss) for loss in v_losses])
+        mean_p_loss = round(np.mean([np.sum(loss) for loss in p_losses]), 6)
+        mean_v_loss = round(np.mean([np.sum(loss) for loss in v_losses]), 6)
 
         # Calculate the stats of an episode
         cum_ret = [np.sum(ep_rews) for ep_rews in batch_return]
-        mean_ep_len = np.mean(batch_lens)
-        mean_ep_rew = np.mean(cum_ret)
+        mean_ep_time = round(np.mean(time), 6) 
+        mean_ep_len = round(np.mean(batch_lens), 6)
+
+        # statistical values for return
+        mean_ep_ret = round(np.mean(cum_ret), 6)
+        max_ep_ret = round(np.max(cum_ret), 6)
+        min_ep_ret = round(np.min(cum_ret), 6)
+
         # calculate standard deviation (spred of distribution)
-        std_ep_rew = np.std(cum_ret)
+        std_ep_rew = round(np.std(cum_ret), 6)
 
         # Log stats to CSV file
         self.stats_data['experiment'].append(exp_name)
         self.stats_data['mean episodic length'].append(mean_ep_len)
-        self.stats_data['mean episodic returns'].append(mean_ep_rew)
+        self.stats_data['mean episodic returns'].append(mean_ep_ret)
+        self.stats_data['min episodic returns'].append(min_ep_ret)
+        self.stats_data['max episodic returns'].append(max_ep_ret)
+        self.stats_data['mean episodic runtime'].append(mean_ep_time)
         self.stats_data['std episodic returns'].append(std_ep_rew)
-        self.stats_data['timestep'].append(steps)
+        self.stats_data['timestep'].append(training_steps)
 
         # Monitoring via W&B
         wandb.log({
-            'train/timesteps': steps,
+            'train/timesteps': training_steps,
             'train/mean policy loss': mean_p_loss,
             'train/mean value loss': mean_v_loss,
-            'train/mean episode length': mean_ep_len,
-            'train/mean episode returns': mean_ep_rew,
-            'train/std episode returns': std_ep_rew
+            'train/mean episode returns': mean_ep_ret,
+            'train/std episode returns': std_ep_rew,
+            'train/mean episode runtime': mean_ep_time,
+            'train/mean episode length': mean_ep_len
         })
 
         logging.info('\n')
-        logging.info(f'------------ Episode: {steps} --------------')
-        logging.info(f"Mean return:          {mean_ep_rew}")
+        logging.info(f'------------ Episode: {training_steps} --------------')
+        logging.info(f"Mean return:          {mean_ep_ret}")
         logging.info(f"Mean policy loss:     {mean_p_loss}")
         logging.info(f"Mean value loss:      {mean_v_loss}")
         logging.info('--------------------------------------------')
         logging.info('\n')
 
-
 ####################
 ####################
 
@@ -721,9 +776,9 @@ def create_path(path: str) -> None:
     if not os.path.exists(path):
         os.makedirs(path)
 
-def load_model(path, model):
+def load_model(path, model, device='cpu'):
     checkpoint = torch.load(path)
-    model.load_state_dict(checkpoint['model_state_dict'])
+    model.load_state_dict(checkpoint['model_state_dict'], map_location=device)
     return model
 
 def simulate_rollout(policy_net, env, render=True):
@@ -776,9 +831,10 @@ def _log_summary(ep_len, ep_ret, ep_num):
         logging.info(f"--------------------------------------------")
         logging.info('\n')
 
-def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iterations,
+def train(env, in_dim, out_dim, total_training_steps, max_batch_size, episode_iterations,
           noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
-          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, log_video=False, ppo_version='v2'):
+          adam_epsilon, render_steps, save_steps, csv_writer, stats_plotter, 
+          log_video=False, ppo_version='v2', device='cpu', exp_path='./log/', exp_name='PPO-experiment'):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = None
     if ppo_version == 'v2':
@@ -786,9 +842,9 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -800,15 +856,18 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device,
+                    exp_path=exp_path,
+                    exp_name=exp_name)
     else:
         agent = PPO_PolicyGradient_V1(
                     env, 
                     in_dim=in_dim, 
                     out_dim=out_dim,
-                    total_timesteps=total_timesteps,
+                    total_training_steps=total_training_steps,
                     max_batch_size=max_batch_size,
-                    trajectory_iterations=trajectory_iterations,
+                    episode_iterations=episode_iterations,
                     noptepochs=noptepochs,
                     lr_p=learning_rate_p,
                     lr_v=learning_rate_v,
@@ -820,15 +879,16 @@ def train(env, in_dim, out_dim, total_timesteps, max_batch_size, trajectory_iter
                     save_model=save_steps,
                     csv_writer=csv_writer,
                     stats_plotter=stats_plotter,
-                    log_video=log_video)
+                    log_video=log_video,
+                    device=device)
     # run training for a total amount of steps
     agent.learn()
 
-def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False):
+def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False, device='cpu'):
     """Test the policy network (actor)"""
     # load model and test it
     policy_net = PolicyNet(in_dim, out_dim)
-    policy_net = load_model(path, policy_net)
+    policy_net = load_model(path, policy_net, device)
     
     for ep_num, (ep_len, ep_ret) in enumerate(simulate_rollout(policy_net, env, render)):
         _log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)
@@ -843,9 +903,9 @@ def hyperparam_tuning(config=None):
                 env,
                 in_dim=config.obs_dim, 
                 out_dim=config.act_dim,
-                total_timesteps=config.total_timesteps,
+                total_training_steps=config.total_training_steps,
                 max_batch_size=config.max_batch_size,
-                trajectory_iterations=config.trajectory_iterations,
+                episode_iterations=config.episode_iterations,
                 noptepochs=config.noptepochs,
                 gae_lambda = config.gae_lambda,
                 gamma=config.gamma,
@@ -869,24 +929,23 @@ if __name__ == '__main__':
     # check if path exists otherwise create
     if args.video:
         create_path(VIDEO_PATH)
-    create_path(MODEL_PATH)
     create_path(LOG_PATH)
     create_path(RESULTS_PATH)
     
     # Hyperparameter
-    total_timesteps = 3_500_000     # time steps regarding batches collected and train agent
-    max_batch_size = 1000      # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 2048    # number of batches per episode
-    noptepochs = 12                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gae_lambda = 0.95               # trajectory discount for the general advantage estimation (GAE)
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-8             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
-    env_number = 1                  # number of actors
-    seed = 42                       # seed gym, env, torch, numpy 
+    total_training_steps = 1000      # time steps regarding batches collected and train agent
+    max_batch_size = 1000                # max number of episode samples to be sampled per time step. 
+    episode_iterations = 2048            # number of batches per episode
+    noptepochs = 12                      # Number of epochs per time step to optimize the neural networks
+    learning_rate_p = 1e-4               # learning rate for policy network
+    learning_rate_v = 1e-3               # learning rate for value network
+    gae_lambda = 0.95                    # episode discount for the general advantage estimation (GAE)
+    gamma = 0.99                         # discount factor
+    adam_epsilon = 1e-8                  # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8 - Andrychowicz, et al. (2021)  uses 0.9
+    epsilon = 0.2                        # clipping factor
+    env_name = 'Pendulum-v1'             # name of OpenAI gym environment other: 'Pendulum-v1' , 'MountainCarContinuous-v0'
+    env_number = 1                       # number of actors
+    seed = 42                            # seed gym, env, torch, numpy 
     
     # setup for torch save models and rendering
     render = True
@@ -928,9 +987,19 @@ if __name__ == '__main__':
     logging.info(f'upper bound for env observation: {env.observation_space.high}')
     logging.info(f'lower bound for env observation: {env.observation_space.low}')
 
+    # create folder for project
+    exp_name = f"exp_name: {env_name}_{CURR_DATE} {args.exp_name}"
+    exp_folder_name = f'{LOG_PATH}exp_{env_name}_{CURR_TIME}'
+    create_path(exp_folder_name)
+    # create model folder
+    model_path = f'{LOG_PATH}{MODEL_PATH}'
+    create_path(model_path)
+
     # create CSV writer
-    csv_writer = CSVWriter(f"{LOG_PATH}{env_name}_{CURR_DATE}.csv")
-    stats_plotter = StatsPlotter(LOG_PATH, f'{env_name}_{CURR_DATE}.png', RESULTS_PATH)
+    csv_file = os.path.join(exp_folder_name, f'{env_name}_{CURR_TIME}.csv')
+    png_file = os.path.join(exp_folder_name, f'{env_name}_{CURR_TIME}.png')
+    csv_writer = CSVWriter(csv_file)
+    stats_plotter = StatsPlotter(csv_folder_path=exp_folder_name, file_name_and_path=png_file)
 
     # Monitoring with W&B
     wandb.init(
@@ -940,9 +1009,9 @@ if __name__ == '__main__':
             config={ # stores hyperparams in job
                 'env name': env_name,
                 'env number': env_number,
-                'total number of steps': total_timesteps,
+                'total number of steps': total_training_steps,
                 'max sampled trajectories': max_batch_size,
-                'batches per episode': trajectory_iterations,
+                'batches per episode': episode_iterations,
                 'number of epochs for update': noptepochs,
                 'input layer size': obs_dim,
                 'output layer size': act_dim,
@@ -956,9 +1025,11 @@ if __name__ == '__main__':
                 'gamma (discount)': gamma,
                 'epsilon (clipping)': epsilon,
                 'gae lambda (GAE)': gae_lambda,
-                'seed': seed
+                'seed': seed,
+                'experiment path': exp_folder_name,
+                'experiment name': args.exp_name
             },
-            name=f"exp_name: {env_name}_{CURR_DATE} {args.exp_name}", # needs flag --exp-name
+            name=exp_name, # needs flag --exp-name
             monitor_gym=True,
             save_code=True
         )
@@ -968,9 +1039,9 @@ if __name__ == '__main__':
         train(env,
             in_dim=obs_dim, 
             out_dim=act_dim,
-            total_timesteps=total_timesteps,
+            total_training_steps=total_training_steps,
             max_batch_size=max_batch_size,
-            trajectory_iterations=trajectory_iterations,
+            episode_iterations=episode_iterations,
             noptepochs=noptepochs,
             learning_rate_p=learning_rate_p, 
             learning_rate_v=learning_rate_v,
@@ -982,12 +1053,15 @@ if __name__ == '__main__':
             save_steps=save_steps,
             csv_writer=csv_writer,
             stats_plotter=stats_plotter,
-            log_video=args.video)
+            log_video=args.video,
+            device=device,
+            exp_path=exp_folder_name,
+            exp_name=exp_name)
     
     elif args.test:
         logging.info('Evaluation model...')
-        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
-        test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
+        PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth' # TODO: define path
+        test(PATH, env, in_dim=obs_dim, out_dim=act_dim, device=device)
     
     elif args.hyperparam:
         # hyperparameter tuning with sweeps
diff --git a/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png b/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png
deleted file mode 100644
index 66bc6854..00000000
Binary files a/PPO/ppo_torch/results/Pendulum-v1_2023-01-10.png and /dev/null differ
diff --git a/PPO/ppo_torch/wrapper/stats_logger.py b/PPO/ppo_torch/wrapper/stats_logger.py
index 8fa3c60b..fe01fe73 100644
--- a/PPO/ppo_torch/wrapper/stats_logger.py
+++ b/PPO/ppo_torch/wrapper/stats_logger.py
@@ -4,6 +4,10 @@ import seaborn as sns
 import matplotlib.pyplot as plt
 import os
 
+# switch style
+sns.set_theme()
+sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
+
 class StatsLogger:
     """ Log all statistical values over the runtime
     """
@@ -21,10 +25,9 @@ class StatsLogger:
 
 class StatsPlotter:
     """Plotting collected network statistics as graphs."""
-    def __init__(self, csv_path, img_name, results_path) -> None:
-        self.csv_path = csv_path
-        self.img_name = img_name
-        self.result_path = results_path
+    def __init__(self, csv_folder_path, file_name_and_path) -> None:
+        self.csv_folder_path = csv_folder_path
+        self.file_name_and_path = file_name_and_path
     
     def get_files(self, path):
         """Get all files from a path"""
@@ -35,30 +38,46 @@ class StatsPlotter:
             files.append(df)
         return files
 
-    def read_csv(self, date:str):
+    def read_csv(self):
         """ Use file path to collect all csv files.
             Concats all files and returns a pandas dataframe.
         """
-        all_files = glob.glob(os.path.join(self.csv_path, f"*{date}*.csv"))
+        # find all csv files in this folder
+        csv_file = os.path.join(self.csv_folder_path, f"*.csv")
+        all_files = glob.glob(csv_file)
         df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)
         return df
 
-    def plot(self, dataframe, kind='line', hue='', x='x-label', y='y-label', title='title', upper_bound=None, lower_bound=None):
+    def plot_seaborn_fill(self, dataframe):
+        g = sns.FacetGrid(dataframe, row='Filename', height=3.0, aspect=4) 
+        g = g.map(plt.plot, 'Time(min)', 'Smoothed Humidity') 
+        x = plt.gca().axes.get_xlim() 
+        
+        for ax in g.axes: 
+            ax[0].fill_between(x, y1=85, y2=90, color=sns.xkcd_rgb['blue'], alpha=0.2) 
+    
+    def plot_seaborn(self, dataframe, x='x-label', y='y-label', hue='hue', title='title', upper_bound=0, lower_bound=-1800):
         """ Create a lineplot with seaborn.
             Doc: https://seaborn.pydata.org/tutorial/introduction
         """
-        line_plot = sns.relplot(
-            data=dataframe, kind=kind,
-            x=x, y=y, hue=hue, height=7, aspect=.7)
-        line_plot.set(title=title)
-        # draw a horizontal line
-        if upper_bound:
-            line_plot.axhline(upper_bound)
-        if lower_bound:
-            line_plot.axhline(lower_bound)
+        line_plot = sns.relplot(data=dataframe, x=x, y=y, hue=hue, kind='line')
+
+        (line_plot
+            # draw line into log
+            .map(plt.axhline, y=upper_bound, color=".7", dashes=(2, 1), zorder=0)
+            .map(plt.axhline, y=lower_bound, color=".7", dashes=(2, 1), zorder=0)
+            .set_axis_labels("Timestep", "Mean Episodic Return")
+            .set_titles(f"{title}")
+            .tight_layout(w_pad=0))
+
         # plot the file to given destination
-        file_path = self.result_path + self.img_name
-        line_plot.figure.savefig(file_path)
+        line_plot.figure.savefig(self.file_name_and_path)
+    
+    def plot_matplot(self, x_values, y_values, y_lower, y_upper):
+        """ Create a matplot lineplot with filling between ."""
+        plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) # standard deviation
+        plt.plot(x_values, y_values) # plotted mean 
+        plt.show()
 
 class CSVWriter:
     """Log the network outputs via pandas to a CSV file.
