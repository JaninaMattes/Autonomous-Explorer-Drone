diff --git a/PPO/ppo_torch/log/Pendulum-v1_2023-01-03.csv b/PPO/ppo_torch/log/Pendulum-v1_2023-01-03.csv
index 007c91c3..4c0829ba 100644
--- a/PPO/ppo_torch/log/Pendulum-v1_2023-01-03.csv
+++ b/PPO/ppo_torch/log/Pendulum-v1_2023-01-03.csv
@@ -1677,3 +1677,53 @@
 0,200.0,-1310.8017574946643,7800
 0,200.0,-1239.0310718626833,10400
 0,200.0,-1049.056217407961,13000
+0,200.0,-1277.9901390931113,2600
+0,200.0,-1326.9692516560017,5200
+0,200.0,-1310.7406571912868,7800
+0,200.0,-1239.0813488062604,10400
+0,200.0,-1048.1619964680572,13000
+0,200.0,-1368.8421347084598,15600
+0,200.0,-1204.848486092831,18200
+0,200.0,-1150.4603318777515,20800
+0,200.0,-1263.2929661048677,23400
+0,200.0,-1133.5620103328815,26000
+0,200.0,-1352.7517539292417,28600
+0,200.0,-1065.7074632337788,31200
+0,200.0,-1221.3015274181687,33800
+0,200.0,-1054.2160679749252,36400
+0,200.0,-1189.2470844194183,39000
+0,200.0,-1240.9052256525824,41600
+0,200.0,-1208.072508721985,44200
+0,200.0,-1246.7505550935914,46800
+0,200.0,-1199.7333024764098,49400
+0,200.0,-1203.4404807123765,52000
+0,200.0,-1044.2891770316314,54600
+0,200.0,-1125.783846067617,57200
+0,200.0,-1175.8231602377655,59800
+0,200.0,-1134.2043836517644,62400
+0,200.0,-1210.7404587671865,65000
+0,200.0,-1256.0249148677708,67600
+0,200.0,-1154.7490265844945,70200
+0,200.0,-1130.2467766771022,72800
+0,200.0,-1293.907845731711,75400
+0,200.0,-1239.059705885478,78000
+0,200.0,-1315.2441244661156,80600
+0,200.0,-1280.0179276422148,83200
+0,200.0,-1234.4820413675507,85800
+0,200.0,-1325.7545154930801,88400
+0,200.0,-1414.192745205547,91000
+0,200.0,-1287.5623951318507,93600
+0,200.0,-1254.1888564279068,96200
+0,200.0,-1147.3061996594688,98800
+0,200.0,-1283.9449235522222,101400
+0,200.0,-1231.6859641964677,104000
+0,200.0,-1182.2488032693761,106600
+0,200.0,-1268.3635996476337,109200
+0,200.0,-1243.1274164516942,111800
+0,200.0,-1279.4890211246925,114400
+0,200.0,-1265.6183029937288,117000
+0,200.0,-1288.2368538141955,119600
+0,200.0,-1263.579023644027,122200
+0,200.0,-1254.636865254602,124800
+0,200.0,-1173.4692293317955,127400
+0,200.0,-1299.739601994589,130000
diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index 3500fb60..f89c8f6a 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -131,7 +131,7 @@ class PPO_PolicyGradient:
         render=1,
         save_model=10,
         csv_writer=None,
-        log_video=True) -> None:
+        log_video=False) -> None:
         
         # hyperparams
         self.in_dim = in_dim
@@ -151,6 +151,8 @@ class PPO_PolicyGradient:
         self.env = env
         self.render_steps = render
         self.save_model = save_model
+
+        # track video of gym
         self.log_video = log_video
 
         # keep track of rewards per episode
@@ -214,6 +216,12 @@ class PPO_PolicyGradient:
             cum_returns = (cum_returns - cum_returns.mean()) / cum_returns.std()
         return torch.tensor(cum_returns, dtype=torch.float)
 
+    def advantage_estimate_(self, returns, values, normalized=True):
+        advantages = returns - values # delta = r - v
+        if normalized:
+            advantages = self.normalize_adv(advantages)
+        return advantages
+
     def generalized_advantage_estimate_3(self, batch_rewards, values, dones, normalized=True):
         """ Calculate advantage as a weighted average of A_t
                 - advantage measures if an action is better or worse than the policy's default behavior
@@ -256,7 +264,6 @@ class PPO_PolicyGradient:
         returns = torch.tensor(np.array(returns), dtype=torch.float)
         if normalized:
             advantages = self.normalize_adv(advantages)
-            #returns = self.normalize_ret(returns)
         return advantages, returns
 
 
@@ -405,11 +412,10 @@ class PPO_PolicyGradient:
         obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
         next_obs = torch.tensor(np.array(trajectory_nextobs), dtype=torch.float)
         actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
+        action_log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
         dones = torch.tensor(np.array(trajectory_dones), dtype=torch.float)
-        rewards = batch_rewards
 
-        return obs, next_obs, actions, log_probs, dones, rewards, batch_lens
+        return obs, next_obs, actions, action_log_probs, dones, batch_rewards, batch_lens
                 
 
     def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
@@ -437,25 +443,27 @@ class PPO_PolicyGradient:
             policy_losses, value_losses = [], []
             # Collect trajectory
             # STEP 3: simulate and collect trajectories --> the following values are all per batch
-            obs, next_obs, actions, log_probs, dones, batch_rewards, batch_lens = self.collect_rollout(n_step=self.trajectory_iterations)
+            obs, next_obs, actions, batch_log_probs, dones, rewards, batch_lens = self.collect_rollout(n_step=self.trajectory_iterations)
 
             # timesteps simulated so far for batch collection
             steps += np.sum(batch_lens)
 
             # STEP 4-5: Calculate cummulated reward and GAE at timestep t_step
             values, _ , _ = self.get_values(obs, actions)
-            advantages, cum_returns = self.generalized_advantage_estimate_1(batch_rewards, values.detach())
+            cum_returns = self.cummulative_return(rewards)
+            advantages = self.advantage_estimate_(cum_returns, values.detach())
+            # advantages, cum_returns = self.generalized_advantage_estimate_1(rewards, values.detach())
 
             # update network params 
             for _ in range(self.noptepochs):
                 # STEP 6-7: calculate loss and update weights
                 values, curr_log_probs, _ = self.get_values(obs, actions)
-                policy_loss, value_loss = self.train(values, cum_returns, advantages, log_probs, curr_log_probs, self.epsilon)
+                policy_loss, value_loss = self.train(values, cum_returns, advantages, batch_log_probs, curr_log_probs, self.epsilon)
                 
                 policy_losses.append(policy_loss.detach().numpy())
                 value_losses.append(value_loss.detach().numpy())
 
-            self.log_stats(policy_losses, value_losses, batch_rewards, batch_lens, steps)
+            self.log_stats(policy_losses, value_losses, rewards, batch_lens, steps)
 
             # store model in checkpoints
             if steps % self.save_model == 0:
@@ -476,12 +484,6 @@ class PPO_PolicyGradient:
 
                 if wandb:
                     wandb.save(policy_net_name)
-                    
-                    if self.log_video:
-                        file_name = VIDEO_PATH + f'{env_name}.mp4'
-                        wandb.log({
-                            "train/video": wandb.Video(file_name, caption='episode: '+str(steps), fps=4, format="mp4"), 
-                            "step": steps})
 
                 # Log to CSV
                 if self.csv_writer is not None:
@@ -650,23 +652,9 @@ def _log_summary(ep_len, ep_ret, ep_num):
         logging.info(f"--------------------------------------------")
         logging.info('\n')
 
-def train(env, 
-          in_dim, 
-          out_dim, 
-          total_steps, 
-          max_trajectory_size, 
-          trajectory_iterations,
-          noptepochs,
-          learning_rate_p,
-          learning_rate_v,
-          gae_lambda,
-          gamma,
-          epsilon,
-          adam_epsilon,
-          render_steps,
-          save_steps,
-          csv_writer,
-          log_video):
+def train(env, in_dim, out_dim, total_steps, max_trajectory_size, trajectory_iterations,
+          noptepochs, learning_rate_p, learning_rate_v, gae_lambda, gamma, epsilon,
+          adam_epsilon, render_steps, save_steps, csv_writer, log_video=False):
     """Train the policy network (actor) and the value network (critic) with PPO"""
     agent = PPO_PolicyGradient(
                 env, 
@@ -712,11 +700,13 @@ if __name__ == '__main__':
     args = arg_parser()
 
     # check if path exists otherwise create
+    if args.video:
+        create_path(VIDEO_PATH)
     create_path(MODEL_PATH)
     create_path(LOG_PATH)
     
     # Hyperparameter
-    total_steps = 1_000_000         # time steps regarding batches collected and train agent
+    total_steps = 10_000_000         # time steps regarding batches collected and train agent
     max_trajectory_size = 1000      # max number of trajectory samples to be sampled per time step. 
     trajectory_iterations = 2408    # number of batches of episodes
     noptepochs = 12                 # Number of epochs per time step to optimize the neural networks
@@ -740,16 +730,6 @@ if __name__ == '__main__':
     
     # seed gym, torch and numpy
     env = make_env(env_name, seed=seed)
-    if args.video:
-        create_path(VIDEO_PATH)
-        #record video for the 100th episodes
-        env = gym.wrappers.RecordVideo(
-            env, 
-            VIDEO_PATH, 
-            step_trigger=lambda x: x == save_steps, 
-            video_length=6, 
-            name_prefix=env_name)
-
     torch.manual_seed(seed)
     np.random.seed(seed)
     torch.backends.cudnn.deterministic = args.torch_deterministic
@@ -804,10 +784,12 @@ if __name__ == '__main__':
         },
     name=f"exp_name: {env_name}_{CURR_DATE}",
     monitor_gym=True,
-    save_code=True,
-    resume=True # resume runs
+    save_code=True
     )
 
+    # log code
+    wandb.run.log_code(".")
+
     # monitor gym
     wandb.gym.monitor()
 
@@ -830,10 +812,12 @@ if __name__ == '__main__':
             save_steps=save_steps,
             csv_writer=csv_writer,
             log_video=args.video)
+    
     elif args.test:
         logging.info('Evaluation model...')
         PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
         test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
+    
     else:
         assert("Needs training (--train) or testing (--test) flag set!")
 
diff --git a/PPO/ppo_torch/video/Pendulum-v1-step-100.meta.json b/PPO/ppo_torch/video/Pendulum-v1-step-100.meta.json
deleted file mode 100644
index 403124e0..00000000
--- a/PPO/ppo_torch/video/Pendulum-v1-step-100.meta.json
+++ /dev/null
@@ -1 +0,0 @@
-{"step_id": 100, "episode_id": 0, "content_type": "video/mp4"}
\ No newline at end of file
diff --git a/PPO/ppo_torch/video/Pendulum-v1-step-100.mp4 b/PPO/ppo_torch/video/Pendulum-v1-step-100.mp4
deleted file mode 100644
index dfbfe4cc..00000000
Binary files a/PPO/ppo_torch/video/Pendulum-v1-step-100.mp4 and /dev/null differ
