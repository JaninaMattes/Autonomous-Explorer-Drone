diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index ebfe0bbc..1422931e 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -21,6 +21,10 @@ import sys
 import wandb
 from wrapper.stats_logger import CSVWriter
 
+# hyperparameter tuning
+import optuna
+from optuna.integration.wandb import WeightsAndBiasesCallback
+
 # Paths and other constants
 MODEL_PATH = './models/'
 LOG_PATH = './log/'
@@ -28,18 +32,19 @@ VIDEO_PATH = './video/'
 
 CURR_DATE = datetime.today().strftime('%Y-%m-%d')
 
+
 ####################
 ####### TODO #######
 ####################
 
 # Hint: Please if working on it mark a todo as (done) if done
 # 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Check calculation of rewards --> correct mean reward over episodes? 
 # 3) Check calculation of advantage 
 
 ####################
 ####################
 
+
 class Net(nn.Module):
     def __init__(self) -> None:
         super(Net, self).__init__()
@@ -546,6 +551,8 @@ def arg_parser():
         help="if toggled, run model in training mode")
     parser.add_argument("--test", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="if toggled, run model in testing mode")
+    parser.add_argument("--hyperparam", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, log hyperparameters")
     parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
         help="the name of this experiment")
     parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
@@ -693,6 +700,7 @@ def test(path, env, in_dim, out_dim, steps=10_000, render=True, log_video=False)
             wandb.log({"test/video": wandb.Video(VIDEO_PATH, caption='episode: '+str(ep_num), fps=4, format="gif"), "step": ep_num})
 
 
+
 if __name__ == '__main__':
     
     """ Classic control gym environments 
@@ -816,7 +824,22 @@ if __name__ == '__main__':
         logging.info('Evaluation model...')
         PATH = './models/Pendulum-v1_2023-01-01_policyNet.pth'
         test(PATH, env, in_dim=obs_dim, out_dim=act_dim)
-    
+    elif args.hyperparam:
+        # hyperparameter tuning
+        wandb_kwargs = {"project": "drone-mechanics-ppo-HyperparamTuning"}
+        wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)
+        
+        @wandbc.track_in_wandb()
+        def objective(trial):
+            x = trial.suggest_float("x", -10, 10)
+            wandb.log({"power": 2, "base of metric": x - 2})
+
+            return (x - 2) ** 2
+
+
+        study = optuna.create_study()
+        study.optimize(objective, n_trials=10, callbacks=[wandbc])
+
     else:
         assert("Needs training (--train) or testing (--test) flag set!")
 
