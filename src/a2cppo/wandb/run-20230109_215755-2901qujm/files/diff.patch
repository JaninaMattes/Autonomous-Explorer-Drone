diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index 4e1ffa29..0ee6f728 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -269,8 +269,31 @@ class PPO_PolicyGradient_V2:
         action, log_prob, entropy = self.get_action(action_dist)
         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
 
-    def cummulative_return(self, batch_rewards, normalized=False):
-        """Calculate cummulative rewards with discount factor gamma."""
+    def advantage_estimate(self, batch_rewards, values, dones, normalized=True):
+        """ Calculating advantage estimate using TD error (Temporal Difference Error).
+            TD Error can be used as an estimator for Advantage function,
+            - dones: only get reward at end of episode, not disounted next state value
+        """
+        advantages = []
+        returns = []
+        for rewards in reversed(batch_rewards): # reversed order
+            last_value = values[-1] # V(s_t+1)
+            for i in reversed(range(len(rewards))):
+                # TD error: A(s,a) = r + gamma * V(s_t) - V(s) 
+                advantage = rewards[i] + (self.gamma *  values[i]) - last_value
+                advantages.insert(0, advantage)
+                returns.insert(0, rewards[i])
+        advantages = torch.tensor(np.array(advantages), dtype=torch.float)
+        returns = torch.tensor(np.array(returns), dtype=torch.float)
+        # normalize for more stability
+        if normalized:
+            advantages = self.normalize_adv(advantages)
+        return advantages, returns
+
+    def generalized_advantage_estimate_0(self, batch_rewards, values, normalized=True):
+        """ Generalized Advantage Estimate calculation
+            Calculate delta, which is defined by delta = r - v 
+        """
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
         # return value: G(t) = R(t) + gamma * R(t-1)
         cum_returns = []
@@ -279,36 +302,11 @@ class PPO_PolicyGradient_V2:
             for reward in reversed(rewards):
                 discounted_reward = reward + (self.gamma * discounted_reward)
                 cum_returns.insert(0, discounted_reward) # reverse it again
-        if normalized:
-            cum_returns = (cum_returns - cum_returns.mean()) / cum_returns.std()
-        return torch.tensor(cum_returns, dtype=torch.float)
-
-    def advantage_estimate_(self, returns, values, normalized=True):
-        """Calculate delta, which is defined by delta = r - v """
-        advantages = returns - values # delta = r - v
+        advantages = cum_returns - values # delta = r - v
         if normalized:
             advantages = self.normalize_adv(advantages)
         return advantages
 
-    def advantage_estimate(self, next_obs, obs, batch_rewards, normalized=True):
-        """ Calculating advantage estimate using TD error.
-            - done: only get reward at end of episode, not disounted next state value
-        """
-        advantages = []
-        returns = []
-        
-        for rewards in reversed(batch_rewards): # reversed order
-            discounted_return = 0
-            for i in reversed(range(len(rewards))):
-                discounted_return = rewards[i] + (self.gamma * discounted_return)
-                advantage = discounted_return + (self.gamma *  self.get_value(next_obs[i])) - self.get_value(obs[i])
-                advantages.insert(0, advantage)
-                returns.insert(0, discounted_return)
-        advantages = np.array(advantages)
-        if normalized:
-            advantages = self.normalize_adv(advantages)
-        return torch.tensor(advantages, dtype=torch.float), torch.tensor(np.array(returns), dtype=torch.float)
-
     def generalized_advantage_estimate_1(self, batch_rewards, values, normalized=True):
         """ Generalized Advantage Estimate calculation
             - GAE defines advantage as a weighted average of A_t
