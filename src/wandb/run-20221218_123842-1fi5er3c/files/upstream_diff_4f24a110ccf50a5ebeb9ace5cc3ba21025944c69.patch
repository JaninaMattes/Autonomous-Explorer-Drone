diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index ca4ac79..88baa13 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -126,7 +126,7 @@ class PPO_PolicyGradient:
         self.env = env
 
         # keep track of rewards per episode
-        self.ep_returns = []
+        self.ep_returns = deque(maxlen=max_trajectory_size)
 
         # add net for actor and critic
         self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
@@ -160,9 +160,6 @@ class PPO_PolicyGradient:
         log_prob = dist.log_prob(actions)
         return values, log_prob
 
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
     def step(self, obs):
         """ Given an observation, get action and probabilities from policy network (actor)"""
         action_dist = self.get_continuous_policy(obs) 
@@ -192,6 +189,9 @@ class PPO_PolicyGradient:
     def generalized_advantage_estimate(self):
         pass
     
+    def finish_episode(self):
+        pass 
+
     def collect_rollout(self, n_step=1, render=True):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
@@ -204,23 +204,25 @@ class PPO_PolicyGradient:
         batch_rewards = []
         batch_lens = []
 
-        # Run simulation for n timesteps per batch
+        # Run Monte Carlo simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
         while step < n_step:
             
+            # rewards collected per episode
+            ep_rewards, done = [], False 
             obs = self.env.reset()
-            done = False
 
             # Run episode for a fixed amount of timesteps
-            # to keep rollout size fixed
-            for ep_t in range(self.max_trajectory_size):
-                # render gym env
+            # to keep rollout size fixed and episodes independent
+            for ep_t in range(0, self.max_trajectory_size):
+                # render gym envs
                 if render:
                     self.env.render(mode='human')
                 
                 step += 1 
 
-                # action logic
+                # action logic 
+                # sampled via policy which defines behavioral strategy of an agent
                 action, log_probability, _ = self.step(obs)
                         
                 # STEP 3: collecting set of trajectories D_k by running action 
@@ -237,25 +239,9 @@ class PPO_PolicyGradient:
 
                 # break out of loop if episode is terminated
                 if done:
-                    # # STEP 4: Calculate cummulated reward
-                    # total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-                    # # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    # # calculate advantage in different ways --> GAE can be done later
-                    # # keep the rollout size fixed --> episodes should stay independent
-                    # # log when episode end
-                    # advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), \
-                    #                                     np.array(trajectory_values, dtype=np.float32))
-                    # trajectory_advantages = advantage
-
-                    # self.num_episodes += 1
-                    
-                    # # reset values
-                    # trajectory_values = []
-                    # obs = self.env.reset()
-                    ep_rewards = []
                     break
             
-            batch_lens.append(ep_t + 1)
+            batch_lens.append(ep_t + 1) # as we started at 0
             batch_rewards.append(ep_rewards)
 
         # convert trajectories to torch tensors
@@ -321,7 +307,7 @@ class PPO_PolicyGradient:
                 policy_loss, value_loss = self.train(values, cum_return, advantages, log_probs, curr_log_probs, self.epsilon)
 
             logging.info('###########################################')
-            logging.info(f"Mean cummulative reward: {mean_reward}")
+            logging.info(f"Mean return: {mean_reward}")
             logging.info(f"Policy loss: {policy_loss}")
             logging.info(f"Value loss: {value_loss}")
             logging.info(f"Time step: {steps}")
@@ -416,11 +402,11 @@ if __name__ == '__main__':
     args = arg_parser()
     # Hyperparameter
     unity_file_name = ''            # name of unity environment
-    total_steps = 1000              # time steps to train agent
-    max_trajectory_size = 1600      # max number of trajectory samples to be sampled per time step. 
+    total_steps = 30000000          # time steps to train agent
+    max_trajectory_size = 1000      # max number of trajectory samples to be sampled per time step. 
     trajectory_iterations = 4600    # number of batches of episodes
-    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
+    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
+    learning_rate_p = 1e-3          # learning rate for policy network
     learning_rate_v = 1e-3          # learning rate for value network
     gamma = 0.99                    # discount factor
     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
@@ -477,7 +463,8 @@ if __name__ == '__main__':
             'learning rate (value net)': learning_rate_v,
             'epsilon (adam optimizer)': adam_epsilon,
             'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
+            'epsilon (clipping)': epsilon,
+            'seerd': seed
         },
     name=f"{env_name}__{current_time}",
     # monitor_gym=True,
@@ -498,7 +485,7 @@ if __name__ == '__main__':
                 epsilon=epsilon,
                 adam_eps=adam_epsilon)
     
-    # run training
+    # run training for a total amount of steps
     agent.learn()
     logging.info('### Done ###')
 
diff --git a/wandb/debug-cli.janinaalicamattes.log b/PPO/ppo_torch/wandb/debug-cli.janinaalicamattes.log
similarity index 100%
rename from wandb/debug-cli.janinaalicamattes.log
rename to PPO/ppo_torch/wandb/debug-cli.janinaalicamattes.log
diff --git a/PPO/ppo_torch/wandb/debug-internal.log b/PPO/ppo_torch/wandb/debug-internal.log
new file mode 120000
index 0000000..21779c4
--- /dev/null
+++ b/PPO/ppo_torch/wandb/debug-internal.log
@@ -0,0 +1 @@
+run-20221218_123842-1fi5er3c/logs/debug-internal.log
\ No newline at end of file
diff --git a/PPO/ppo_torch/wandb/debug.log b/PPO/ppo_torch/wandb/debug.log
new file mode 120000
index 0000000..b8cbab8
--- /dev/null
+++ b/PPO/ppo_torch/wandb/debug.log
@@ -0,0 +1 @@
+run-20221218_123842-1fi5er3c/logs/debug.log
\ No newline at end of file
diff --git a/PPO/ppo_torch/wandb/latest-run b/PPO/ppo_torch/wandb/latest-run
new file mode 120000
index 0000000..cd830a6
--- /dev/null
+++ b/PPO/ppo_torch/wandb/latest-run
@@ -0,0 +1 @@
+run-20221218_123842-1fi5er3c
\ No newline at end of file
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch/ppo_continuous.py
similarity index 82%
rename from wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch/ppo_continuous.py
rename to PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch/ppo_continuous.py
index 2a80dba..009c2b1 100644
--- a/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch/ppo_continuous.py
@@ -56,11 +56,10 @@ class ValueNet(Net):
         out = self.layer3(x) # head has linear activation
         return out
     
-    def loss(self, obs, rewards):
+    def loss(self, values, returns):
         """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
+        # return 0.5 * ((rewards - values)**2).mean() # MSE loss
+        return nn.MSELoss()(values, returns)
 
 class PolicyNet(Net):
     """Setup Policy Network (Actor)"""
@@ -126,6 +125,9 @@ class PPO_PolicyGradient:
         # environment
         self.env = env
 
+        # keep track of rewards per episode
+        self.ep_returns = []
+
         # add net for actor and critic
         self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
         self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
@@ -158,29 +160,28 @@ class PPO_PolicyGradient:
         log_prob = dist.log_prob(actions)
         return values, log_prob
 
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
     def step(self, obs):
         """ Given an observation, get action and probabilities from policy network (actor)"""
         action_dist = self.get_continuous_policy(obs) 
         action, log_prob, entropy = self.get_action(action_dist)
         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
 
-    def cummulative_reward(self, rewards):
+    def cummulative_reward(self, batch_rewards):
+        """Calculate cummulative rewards with discount factor gamma."""
         # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
         # G(t) = R(t) + gamma * R(t-1)
         cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
+        for rewards in reversed(batch_rewards):
+            discounted_reward = 0
+            for reward in reversed(rewards):
+                discounted_reward = reward + (self.gamma * discounted_reward)
+                cum_rewards.insert(0, discounted_reward)
+        return torch.tensor(cum_rewards, dtype=torch.float)
+
+    def advantage_estimate(self, returns, values, normalized=True):
         """Simplest advantage calculation"""
         # STEP 5: compute advantage estimates A_t at step t
-        advantages = rewards - values
+        advantages = returns - values
         if normalized:
             advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
         return advantages
@@ -188,72 +189,82 @@ class PPO_PolicyGradient:
     def generalized_advantage_estimate(self):
         pass
     
-    def collect_rollout(self, obs, n_step=1, render=True):
+    def collect_rollout(self, n_step=1, render=True):
         """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
         
-        self.num_episodes = 0
-        done = False
+        step, ep_rewards = 0, []
 
         # collect trajectories
         trajectory_obs = []
         trajectory_actions = []
         trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
+        batch_rewards = []
+        batch_lens = []
 
-        # Run an episode 
+        # Run simulation for n timesteps per batch
         logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
+        while step < n_step:
+            
+            obs = self.env.reset()
+            done = False
 
-            while True: 
-                # render gym env
+            # Run episode for a fixed amount of timesteps
+            # to keep rollout size fixed and episodes independent
+            for ep_t in range(self.max_trajectory_size):
+                # render gym envs
                 if render:
                     self.env.render(mode='human')
-                    
+                
+                step += 1 
+
                 # action logic
                 action, log_probability, _ = self.step(obs)
                         
                 # STEP 3: collecting set of trajectories D_k by running action 
                 # that was sampled from policy in environment
                 __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
 
                 # collection of trajectories in batches
                 trajectory_obs.append(obs)
                 trajectory_actions.append(action)
                 trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
+                ep_rewards.append(reward)
                     
                 obs = __obs
 
                 # break out of loop if episode is terminated
                 if done:
-                    # STEP 4: Calculate cummulated reward
-                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-                    trajectory_advantages = advantage
-
-                    self.num_episodes += 1
-                    
-                    # reset values
-                    trajectory_values = []
+                    # # reset values
                     obs = self.env.reset()
+                    ep_rewards = []
                     break
-        
+            
+            batch_lens.append(ep_t + 1)
+            batch_rewards.append(ep_rewards)
+
         # convert trajectories to torch tensors
         obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
         actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
         log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
+        
+        # STEP 4: Calculate cummulated reward
+        cummulative_reward = self.cummulative_reward(batch_rewards)
+        cummulative_reward = torch.tensor(np.array(cummulative_reward), dtype=torch.float)
+
+        # Calculate the stats
+        mean_ep_lens = np.mean(batch_lens)
+        mean_ep_rews = np.mean([np.sum(ep_rews) for ep_rews in batch_rewards])
+
+        # Log stats
+        wandb.log({
+            "train/mean episode length": mean_ep_lens,
+            "train/mean episode returns": mean_ep_rews,
+        })
 
-        return obs, actions, log_probs, rewards, advantages
+        return obs, actions, log_probs, cummulative_reward, batch_lens, mean_ep_rews
                 
 
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
+    def train(self, values, returns, advantages, batch_log_probs, curr_log_probs, epsilon):
         """Calculate loss and update weights of both networks."""
         logging.info("Updating network parameter...")
         # loss of the policy network
@@ -264,7 +275,7 @@ class PPO_PolicyGradient:
 
         # loss of the value network
         self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
+        value_loss = self.value_net.loss(values, returns) # TODO: discounted return 
         value_loss.backward()
         self.value_net_optim.step()
 
@@ -272,22 +283,27 @@ class PPO_PolicyGradient:
 
     def learn(self):
         """"""
-        best_mean_reward = 0
-        for steps in range(self.total_steps):
+        steps, best_mean_reward = 0, 0
+
+        while steps < self.total_steps:
         
-            next_obs = self.env.reset()
             # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
+            # STEP 3-4: simulate and collect trajectories --> the following values are all per batch
+            obs, actions, log_probs, cum_return, batch_lens, mean_reward = self.collect_rollout(n_step=self.trajectory_iterations)
             
-            # calculate mean reward per episode
-            mean_reward = sum(rewards) / self.num_episodes # mean return
+            # timesteps for batch collection
+            steps += np.sum(batch_lens)
+
+            # STEP 5: compute advantage estimates A_t at timestep t_step
+            values, _ = self.get_values(obs, actions)
+            advantages = self.advantage_estimate(cum_return, values.detach())
             
+            # TODO: fix frequency of the updates --> improve the algorithm 
             for _ in range(self.num_epochs):
-                _, curr_log_probs = self.get_values(obs, actions)
                 # STEP 6-7: calculate loss and update weights
-                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-                
+                values, curr_log_probs = self.get_values(obs, actions)
+                policy_loss, value_loss = self.train(values, cum_return, advantages, log_probs, curr_log_probs, self.epsilon)
+
             logging.info('###########################################')
             logging.info(f"Mean cummulative reward: {mean_reward}")
             logging.info(f"Policy loss: {policy_loss}")
@@ -297,10 +313,9 @@ class PPO_PolicyGradient:
             
             # logging for monitoring in W&B
             wandb.log({
-                'time/step': steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
+                'train/step': steps,
+                'train/policy loss': policy_loss,
+                'train/value loss': value_loss})
             
             # store model in checkpoints
             if mean_reward > best_mean_reward:
@@ -386,8 +401,8 @@ if __name__ == '__main__':
     # Hyperparameter
     unity_file_name = ''            # name of unity environment
     total_steps = 1000              # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
+    max_trajectory_size = 1600      # max number of trajectory samples to be sampled per time step. 
+    trajectory_iterations = 4600    # number of batches of episodes
     num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
     learning_rate_p = 1e-4          # learning rate for policy network
     learning_rate_v = 1e-3          # learning rate for value network
diff --git a/wandb/run-20221214_224254-1omblhza/files/conda-environment.yaml b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml
similarity index 100%
rename from wandb/run-20221214_224254-1omblhza/files/conda-environment.yaml
rename to PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/config.yaml b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml
similarity index 93%
rename from wandb/run-20221215_125246-1ml5dns8/files/config.yaml
rename to PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml
index 78a605c..26f2ecc 100644
--- a/wandb/run-20221215_125246-1ml5dns8/files/config.yaml
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml
@@ -9,7 +9,7 @@ _wandb:
     is_jupyter_run: false
     is_kaggle_kernel: false
     python_version: 3.10.8
-    start_time: 1671105166.986829
+    start_time: 1671210643.920539
     t:
       1:
       - 1
@@ -29,7 +29,7 @@ _wandb:
       - 5
 batches per episode:
   desc: null
-  value: 10
+  value: 4600
 epsilon (adam optimizer):
   desc: null
   value: 1.0e-05
@@ -50,7 +50,7 @@ learning rate (value net):
   value: 0.001
 max sampled trajectories:
   desc: null
-  value: 10000
+  value: 1600
 number of epochs for update:
   desc: null
   value: 20
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/diff.patch b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/diff.patch
new file mode 100644
index 0000000..afc73a3
--- /dev/null
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/diff.patch
@@ -0,0 +1,76 @@
+diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
+index ca4ac79..009c2b1 100644
+--- a/PPO/ppo_torch/ppo_continuous.py
++++ b/PPO/ppo_torch/ppo_continuous.py
+@@ -160,9 +160,6 @@ class PPO_PolicyGradient:
+         log_prob = dist.log_prob(actions)
+         return values, log_prob
+ 
+-    def get_value(self, obs):
+-        return self.value_net(obs).squeeze()
+-
+     def step(self, obs):
+         """ Given an observation, get action and probabilities from policy network (actor)"""
+         action_dist = self.get_continuous_policy(obs) 
+@@ -212,9 +209,9 @@ class PPO_PolicyGradient:
+             done = False
+ 
+             # Run episode for a fixed amount of timesteps
+-            # to keep rollout size fixed
++            # to keep rollout size fixed and episodes independent
+             for ep_t in range(self.max_trajectory_size):
+-                # render gym env
++                # render gym envs
+                 if render:
+                     self.env.render(mode='human')
+                 
+@@ -237,21 +234,8 @@ class PPO_PolicyGradient:
+ 
+                 # break out of loop if episode is terminated
+                 if done:
+-                    # # STEP 4: Calculate cummulated reward
+-                    # total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
+-                    # # STEP 5: compute advantage estimates A_t at timestep num_steps
+-                    # # calculate advantage in different ways --> GAE can be done later
+-                    # # keep the rollout size fixed --> episodes should stay independent
+-                    # # log when episode end
+-                    # advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), \
+-                    #                                     np.array(trajectory_values, dtype=np.float32))
+-                    # trajectory_advantages = advantage
+-
+-                    # self.num_episodes += 1
+-                    
+                     # # reset values
+-                    # trajectory_values = []
+-                    # obs = self.env.reset()
++                    obs = self.env.reset()
+                     ep_rewards = []
+                     break
+             
+diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
+index 5b5538e..a0306f6 120000
+--- a/wandb/debug-internal.log
++++ b/wandb/debug-internal.log
+@@ -1 +1 @@
+-run-20221215_130627-g5gz7u2b/logs/debug-internal.log
+\ No newline at end of file
++run-20221215_231144-3fnhylxq/logs/debug-internal.log
+\ No newline at end of file
+diff --git a/wandb/debug.log b/wandb/debug.log
+index 3348e0e..21432b6 120000
+--- a/wandb/debug.log
++++ b/wandb/debug.log
+@@ -1 +1 @@
+-run-20221215_130627-g5gz7u2b/logs/debug.log
+\ No newline at end of file
++run-20221215_231144-3fnhylxq/logs/debug.log
+\ No newline at end of file
+diff --git a/wandb/latest-run b/wandb/latest-run
+index 59dc0e4..bba22d0 120000
+--- a/wandb/latest-run
++++ b/wandb/latest-run
+@@ -1 +1 @@
+-run-20221215_130627-g5gz7u2b
+\ No newline at end of file
++run-20221215_231144-3fnhylxq
+\ No newline at end of file
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
new file mode 100644
index 0000000..df1fc5f
--- /dev/null
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
@@ -0,0 +1,10 @@
+
+INFO:root:Collecting batch trajectories...
+Traceback (most recent call last):
+  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 486, in <module>
+    agent.learn()
+  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 299, in learn
+    advantages = self.advantage_estimate(cum_return, values.detach())
+  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 184, in advantage_estimate
+    advantages = returns - values
+RuntimeError: The size of tensor a (4400) must match the size of tensor b (4600) at non-singleton dimension 0
\ No newline at end of file
diff --git a/wandb/run-20221214_224254-1omblhza/files/requirements.txt b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/requirements.txt
similarity index 100%
rename from wandb/run-20221214_224254-1omblhza/files/requirements.txt
rename to PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/requirements.txt
diff --git a/wandb/run-20221214_224254-1omblhza/files/wandb-metadata.json b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-metadata.json
similarity index 82%
rename from wandb/run-20221214_224254-1omblhza/files/wandb-metadata.json
rename to PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-metadata.json
index 78cbcb6..dfb60b2 100644
--- a/wandb/run-20221214_224254-1omblhza/files/wandb-metadata.json
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-metadata.json
@@ -1,8 +1,8 @@
 {
     "os": "macOS-13.0.1-arm64-arm-64bit",
     "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T21:42:54.974347",
-    "startedAt": "2022-12-14T21:42:54.242314",
+    "heartbeatAt": "2022-12-16T17:10:44.774749",
+    "startedAt": "2022-12-16T17:10:43.901262",
     "docker": null,
     "cuda": null,
     "args": [],
@@ -11,7 +11,7 @@
     "codePath": "PPO/ppo_torch/ppo_continuous.py",
     "git": {
         "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
+        "commit": "4f24a110ccf50a5ebeb9ace5cc3ba21025944c69"
     },
     "email": "janina.alica.mattes@gmail.com",
     "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
@@ -22,7 +22,7 @@
     "cpu_count_logical": 8,
     "disk": {
         "total": 460.4317207336426,
-        "used": 8.218391418457031
+        "used": 8.218402862548828
     },
     "gpuapple": {
         "type": "arm",
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json
new file mode 100644
index 0000000..e82cbb8
--- /dev/null
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json
@@ -0,0 +1 @@
+{"train/mean episode length": 200.0, "train/mean episode returns": -1199.110207787845, "_timestamp": 1671210653.462142, "_runtime": 9.541603088378906, "_step": 0, "_wandb": {"runtime": 8}}
\ No newline at end of file
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug-internal.log b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug-internal.log
new file mode 100644
index 0000000..7d94462
--- /dev/null
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug-internal.log
@@ -0,0 +1,187 @@
+2022-12-16 18:10:43,964 INFO    StreamThr :1849 [internal.py:wandb_internal():87] W&B internal server running at pid: 1849, started at: 2022-12-16 18:10:43.963335
+2022-12-16 18:10:43,966 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: status
+2022-12-16 18:10:43,967 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: status
+2022-12-16 18:10:43,968 DEBUG   SenderThread:1849 [sender.py:send():303] send: header
+2022-12-16 18:10:43,968 INFO    WriterThread:1849 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/run-11mxelcn.wandb
+2022-12-16 18:10:43,969 DEBUG   SenderThread:1849 [sender.py:send():303] send: run
+2022-12-16 18:10:44,508 INFO    SenderThread:1849 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files
+2022-12-16 18:10:44,508 INFO    SenderThread:1849 [sender.py:_start_run_threads():928] run started: 11mxelcn with start time 1671210643.920539
+2022-12-16 18:10:44,508 DEBUG   SenderThread:1849 [sender.py:send():303] send: summary
+2022-12-16 18:10:44,509 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
+2022-12-16 18:10:44,510 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: check_version
+2022-12-16 18:10:44,510 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: check_version
+2022-12-16 18:10:44,767 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: run_start
+2022-12-16 18:10:44,773 DEBUG   HandlerThread:1849 [system_info.py:__init__():31] System info init
+2022-12-16 18:10:44,773 DEBUG   HandlerThread:1849 [system_info.py:__init__():46] System info init done
+2022-12-16 18:10:44,774 INFO    HandlerThread:1849 [system_monitor.py:start():150] Starting system monitor
+2022-12-16 18:10:44,774 INFO    SystemMonitor:1849 [system_monitor.py:_start():116] Starting system asset monitoring threads
+2022-12-16 18:10:44,774 INFO    HandlerThread:1849 [system_monitor.py:probe():171] Collecting system info
+2022-12-16 18:10:44,774 DEBUG   HandlerThread:1849 [system_info.py:probe():195] Probing system
+2022-12-16 18:10:44,774 INFO    SystemMonitor:1849 [interfaces.py:start():168] Started cpu
+2022-12-16 18:10:44,774 INFO    SystemMonitor:1849 [interfaces.py:start():168] Started disk
+2022-12-16 18:10:44,776 INFO    SystemMonitor:1849 [interfaces.py:start():168] Started gpuapple
+2022-12-16 18:10:44,777 INFO    SystemMonitor:1849 [interfaces.py:start():168] Started memory
+2022-12-16 18:10:44,777 INFO    SystemMonitor:1849 [interfaces.py:start():168] Started network
+2022-12-16 18:10:44,781 DEBUG   HandlerThread:1849 [system_info.py:_probe_git():180] Probing git
+2022-12-16 18:10:44,796 DEBUG   HandlerThread:1849 [system_info.py:_probe_git():188] Probing git done
+2022-12-16 18:10:44,796 DEBUG   HandlerThread:1849 [system_info.py:probe():241] Probing system done
+2022-12-16 18:10:44,796 DEBUG   HandlerThread:1849 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-16T17:10:44.774749', 'startedAt': '2022-12-16T17:10:43.901262', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '4f24a110ccf50a5ebeb9ace5cc3ba21025944c69'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218402862548828}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
+2022-12-16 18:10:44,796 INFO    HandlerThread:1849 [system_monitor.py:probe():181] Finished collecting system info
+2022-12-16 18:10:44,796 INFO    HandlerThread:1849 [system_monitor.py:probe():184] Publishing system info
+2022-12-16 18:10:44,796 DEBUG   HandlerThread:1849 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
+2022-12-16 18:10:44,797 DEBUG   HandlerThread:1849 [system_info.py:_save_pip():67] Saving pip packages done
+2022-12-16 18:10:44,797 DEBUG   HandlerThread:1849 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
+2022-12-16 18:10:45,514 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/requirements.txt
+2022-12-16 18:10:45,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml
+2022-12-16 18:10:45,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json
+2022-12-16 18:10:46,148 DEBUG   HandlerThread:1849 [system_info.py:_save_conda():86] Saving conda packages done
+2022-12-16 18:10:46,148 DEBUG   HandlerThread:1849 [system_info.py:_save_code():89] Saving code
+2022-12-16 18:10:46,154 DEBUG   HandlerThread:1849 [system_info.py:_save_code():110] Saving code done
+2022-12-16 18:10:46,154 DEBUG   HandlerThread:1849 [system_info.py:_save_patches():127] Saving git patches
+2022-12-16 18:10:46,213 DEBUG   HandlerThread:1849 [system_info.py:_save_patches():169] Saving git patches done
+2022-12-16 18:10:46,213 INFO    HandlerThread:1849 [system_monitor.py:probe():186] Finished publishing system info
+2022-12-16 18:10:46,241 DEBUG   SenderThread:1849 [sender.py:send():303] send: files
+2022-12-16 18:10:46,241 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
+2022-12-16 18:10:46,242 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
+2022-12-16 18:10:46,242 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file diff.patch with policy now
+2022-12-16 18:10:46,246 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: stop_status
+2022-12-16 18:10:46,246 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: stop_status
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-metadata.json
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/diff.patch
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch/ppo_continuous.py
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch
+2022-12-16 18:10:46,515 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code
+2022-12-16 18:10:46,560 DEBUG   SenderThread:1849 [sender.py:send():303] send: telemetry
+2022-12-16 18:10:46,568 INFO    Thread-17 :1849 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpciit97zywandb/a25wzarl-code/PPO/ppo_torch/ppo_continuous.py
+2022-12-16 18:10:47,027 INFO    Thread-16 :1849 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpciit97zywandb/256u6b3n-wandb-metadata.json
+2022-12-16 18:10:47,029 INFO    Thread-18 :1849 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpciit97zywandb/2aw61p3q-diff.patch
+2022-12-16 18:10:47,518 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
+2022-12-16 18:10:49,526 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
+2022-12-16 18:10:53,463 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: partial_history
+2022-12-16 18:10:53,464 DEBUG   SenderThread:1849 [sender.py:send():303] send: history
+2022-12-16 18:10:53,464 DEBUG   SenderThread:1849 [sender.py:send():303] send: summary
+2022-12-16 18:10:53,465 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
+2022-12-16 18:10:53,494 DEBUG   SenderThread:1849 [sender.py:send():303] send: exit
+2022-12-16 18:10:53,494 INFO    SenderThread:1849 [sender.py:send_exit():442] handling exit code: 1
+2022-12-16 18:10:53,494 INFO    SenderThread:1849 [sender.py:send_exit():444] handling runtime: 8
+2022-12-16 18:10:53,495 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
+2022-12-16 18:10:53,495 INFO    SenderThread:1849 [sender.py:send_exit():450] send defer
+2022-12-16 18:10:53,495 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,495 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 0
+2022-12-16 18:10:53,495 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,495 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 0
+2022-12-16 18:10:53,495 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 1
+2022-12-16 18:10:53,495 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,495 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 1
+2022-12-16 18:10:53,495 INFO    HandlerThread:1849 [system_monitor.py:finish():160] Stopping system monitor
+2022-12-16 18:10:53,515 DEBUG   SystemMonitor:1849 [system_monitor.py:_start():130] Starting system metrics aggregation loop
+2022-12-16 18:10:53,515 DEBUG   SystemMonitor:1849 [system_monitor.py:_start():137] Finished system metrics aggregation loop
+2022-12-16 18:10:53,515 DEBUG   SystemMonitor:1849 [system_monitor.py:_start():141] Publishing last batch of metrics
+2022-12-16 18:10:53,516 INFO    HandlerThread:1849 [interfaces.py:finish():175] Joined cpu
+2022-12-16 18:10:53,516 INFO    HandlerThread:1849 [interfaces.py:finish():175] Joined disk
+2022-12-16 18:10:53,547 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json
+2022-12-16 18:10:53,710 INFO    HandlerThread:1849 [interfaces.py:finish():175] Joined gpuapple
+2022-12-16 18:10:53,710 INFO    HandlerThread:1849 [interfaces.py:finish():175] Joined memory
+2022-12-16 18:10:53,710 INFO    HandlerThread:1849 [interfaces.py:finish():175] Joined network
+2022-12-16 18:10:53,711 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,711 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 1
+2022-12-16 18:10:53,711 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 2
+2022-12-16 18:10:53,711 DEBUG   SenderThread:1849 [sender.py:send():303] send: telemetry
+2022-12-16 18:10:53,711 DEBUG   SenderThread:1849 [sender.py:send():303] send: stats
+2022-12-16 18:10:53,711 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,711 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 2
+2022-12-16 18:10:53,711 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,711 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 2
+2022-12-16 18:10:53,711 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 3
+2022-12-16 18:10:53,711 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,711 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 3
+2022-12-16 18:10:53,712 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 3
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 4
+2022-12-16 18:10:53,712 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,712 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 4
+2022-12-16 18:10:53,712 DEBUG   SenderThread:1849 [sender.py:send():303] send: summary
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
+2022-12-16 18:10:53,712 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 4
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 5
+2022-12-16 18:10:53,712 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,712 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 5
+2022-12-16 18:10:53,712 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:53,712 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 5
+2022-12-16 18:10:53,997 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 6
+2022-12-16 18:10:53,997 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:53,998 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 6
+2022-12-16 18:10:54,001 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:54,001 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 6
+2022-12-16 18:10:54,499 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: poll_exit
+2022-12-16 18:10:54,547 INFO    Thread-13 :1849 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml
+2022-12-16 18:10:55,505 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 7
+2022-12-16 18:10:55,505 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: poll_exit
+2022-12-16 18:10:55,506 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:55,507 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 7
+2022-12-16 18:10:55,507 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:55,507 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 7
+2022-12-16 18:10:55,507 INFO    SenderThread:1849 [dir_watcher.py:finish():362] shutting down directory watcher
+2022-12-16 18:10:55,556 INFO    SenderThread:1849 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
+2022-12-16 18:10:55,557 INFO    SenderThread:1849 [dir_watcher.py:finish():392] scan: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files
+2022-12-16 18:10:55,557 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml conda-environment.yaml
+2022-12-16 18:10:55,557 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml config.yaml
+2022-12-16 18:10:55,560 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/diff.patch diff.patch
+2022-12-16 18:10:55,561 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log output.log
+2022-12-16 18:10:55,563 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/requirements.txt requirements.txt
+2022-12-16 18:10:55,567 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-metadata.json wandb-metadata.json
+2022-12-16 18:10:55,567 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json wandb-summary.json
+2022-12-16 18:10:55,572 INFO    SenderThread:1849 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/code/PPO/ppo_torch/ppo_continuous.py code/PPO/ppo_torch/ppo_continuous.py
+2022-12-16 18:10:55,572 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 8
+2022-12-16 18:10:55,572 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:55,573 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 8
+2022-12-16 18:10:55,573 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:55,573 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 8
+2022-12-16 18:10:55,573 INFO    SenderThread:1849 [file_pusher.py:finish():168] shutting down file pusher
+2022-12-16 18:10:56,417 INFO    Thread-19 :1849 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/conda-environment.yaml
+2022-12-16 18:10:56,419 INFO    Thread-23 :1849 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/wandb-summary.json
+2022-12-16 18:10:56,422 INFO    Thread-21 :1849 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/output.log
+2022-12-16 18:10:56,442 INFO    Thread-22 :1849 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/requirements.txt
+2022-12-16 18:10:56,444 INFO    Thread-20 :1849 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/files/config.yaml
+2022-12-16 18:10:56,649 INFO    Thread-12 (_thread_body):1849 [sender.py:transition_state():463] send defer: 9
+2022-12-16 18:10:56,650 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:56,650 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 9
+2022-12-16 18:10:56,650 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:56,650 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 9
+2022-12-16 18:10:56,651 INFO    SenderThread:1849 [file_pusher.py:join():173] waiting for file pusher
+2022-12-16 18:10:56,651 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 10
+2022-12-16 18:10:56,651 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:56,651 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 10
+2022-12-16 18:10:56,651 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:56,651 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 10
+2022-12-16 18:10:56,836 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 11
+2022-12-16 18:10:56,837 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:56,837 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 11
+2022-12-16 18:10:56,837 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:56,837 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 11
+2022-12-16 18:10:56,838 INFO    SenderThread:1849 [sender.py:transition_state():463] send defer: 12
+2022-12-16 18:10:56,838 DEBUG   SenderThread:1849 [sender.py:send():303] send: final
+2022-12-16 18:10:56,839 DEBUG   SenderThread:1849 [sender.py:send():303] send: footer
+2022-12-16 18:10:56,839 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: defer
+2022-12-16 18:10:56,839 INFO    HandlerThread:1849 [handler.py:handle_request_defer():162] handle defer: 12
+2022-12-16 18:10:56,839 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: defer
+2022-12-16 18:10:56,839 INFO    SenderThread:1849 [sender.py:send_request_defer():459] handle sender defer: 12
+2022-12-16 18:10:56,841 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: poll_exit
+2022-12-16 18:10:56,841 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: poll_exit
+2022-12-16 18:10:56,842 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: server_info
+2022-12-16 18:10:56,842 DEBUG   SenderThread:1849 [sender.py:send_request():317] send_request: server_info
+2022-12-16 18:10:56,844 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: get_summary
+2022-12-16 18:10:56,845 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: sampled_history
+2022-12-16 18:10:57,331 INFO    MainThread:1849 [wandb_run.py:_footer_history_summary_info():3408] rendering history
+2022-12-16 18:10:57,332 INFO    MainThread:1849 [wandb_run.py:_footer_history_summary_info():3440] rendering summary
+2022-12-16 18:10:57,332 INFO    MainThread:1849 [wandb_run.py:_footer_sync_info():3364] logging synced files
+2022-12-16 18:10:57,333 DEBUG   HandlerThread:1849 [handler.py:handle_request():139] handle_request: shutdown
+2022-12-16 18:10:57,333 INFO    HandlerThread:1849 [handler.py:finish():814] shutting down handler
+2022-12-16 18:10:57,844 INFO    WriterThread:1849 [datastore.py:close():279] close: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/run-11mxelcn.wandb
+2022-12-16 18:10:58,336 INFO    SenderThread:1849 [sender.py:finish():1331] shutting down sender
+2022-12-16 18:10:58,336 INFO    SenderThread:1849 [file_pusher.py:finish():168] shutting down file pusher
+2022-12-16 18:10:58,337 INFO    SenderThread:1849 [file_pusher.py:join():173] waiting for file pusher
+2022-12-16 18:10:58,575 INFO    MainThread:1849 [internal.py:handle_exit():77] Internal process exited
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug.log b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug.log
new file mode 100644
index 0000000..26d24ed
--- /dev/null
+++ b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug.log
@@ -0,0 +1,26 @@
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_setup.py:_flush():68] Configure stats pid to 1840
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/settings
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug.log
+2022-12-16 18:10:43,904 INFO    MainThread:1840 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/logs/debug-internal.log
+2022-12-16 18:10:43,905 INFO    MainThread:1840 [wandb_init.py:init():516] calling init triggers
+2022-12-16 18:10:43,905 INFO    MainThread:1840 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
+config: {'total number of steps': 1000, 'max sampled trajectories': 1600, 'batches per episode': 4600, 'number of epochs for update': 20, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
+2022-12-16 18:10:43,905 INFO    MainThread:1840 [wandb_init.py:init():569] starting backend
+2022-12-16 18:10:43,905 INFO    MainThread:1840 [wandb_init.py:init():573] setting up manager
+2022-12-16 18:10:43,915 INFO    MainThread:1840 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
+2022-12-16 18:10:43,920 INFO    MainThread:1840 [wandb_init.py:init():580] backend started and connected
+2022-12-16 18:10:43,923 INFO    MainThread:1840 [wandb_init.py:init():658] updated telemetry
+2022-12-16 18:10:43,935 INFO    MainThread:1840 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
+2022-12-16 18:10:44,509 INFO    MainThread:1840 [wandb_run.py:_on_init():2006] communicating current version
+2022-12-16 18:10:44,755 INFO    MainThread:1840 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
+
+2022-12-16 18:10:44,755 INFO    MainThread:1840 [wandb_init.py:init():728] starting run threads in backend
+2022-12-16 18:10:46,244 INFO    MainThread:1840 [wandb_run.py:_console_start():1986] atexit reg
+2022-12-16 18:10:46,244 INFO    MainThread:1840 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
+2022-12-16 18:10:46,244 INFO    MainThread:1840 [wandb_run.py:_redirect():1909] Wrapping output streams.
+2022-12-16 18:10:46,245 INFO    MainThread:1840 [wandb_run.py:_redirect():1931] Redirects installed.
+2022-12-16 18:10:46,245 INFO    MainThread:1840 [wandb_init.py:init():765] run started, returning control to user process
+2022-12-16 18:10:58,344 WARNING MsgRouterThr:1840 [router.py:message_loop():77] message_loop has been closed
diff --git a/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/run-11mxelcn.wandb b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/run-11mxelcn.wandb
new file mode 100644
index 0000000..df49c03
Binary files /dev/null and b/PPO/ppo_torch/wandb/run-20221216_181043-11mxelcn/run-11mxelcn.wandb differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
deleted file mode 120000
index 5b5538e..0000000
--- a/wandb/debug-internal.log
+++ /dev/null
@@ -1 +0,0 @@
-run-20221215_130627-g5gz7u2b/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
deleted file mode 120000
index 3348e0e..0000000
--- a/wandb/debug.log
+++ /dev/null
@@ -1 +0,0 @@
-run-20221215_130627-g5gz7u2b/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
deleted file mode 120000
index 59dc0e4..0000000
--- a/wandb/latest-run
+++ /dev/null
@@ -1 +0,0 @@
-run-20221215_130627-g5gz7u2b
\ No newline at end of file
diff --git a/wandb/run-20221214_224254-1omblhza/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_224254-1omblhza/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index a88e3ad..0000000
--- a/wandb/run-20221214_224254-1omblhza/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,474 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def finish_episode(self, rewards, values):
-        # calculate stats and reset all values
-        self.ep_returns.append(sum(rewards))
-        # STEP 5: compute advantage estimates A_t at timestep num_steps
-        advantage = self.advantage_estimate(rewards, values)
-        return advantage
-
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for i in range(n_step):
-
-            # render gym env
-            if render and i % 2 == 0:
-                self.env.render(mode='human')
-                
-            # action logic
-            action, log_probability, _ = self.step(obs)
-                    
-            # STEP 3: collecting set of trajectories D_k by running action 
-            # that was sampled from policy in environment
-            __obs, reward, done, _ = self.env.step(action)
-            value = self.get_value(__obs)
-            # tracking of values
-            trajectory_obs.append(obs)
-            trajectory_actions.append(action)
-            trajectory_action_probs.append(log_probability)
-            trajectory_rewards.append(reward)
-            trajectory_values.append(value.detach())
-                
-            obs = __obs
-            self.num_steps += 1
-
-            # break out of loop if episode is terminated
-            if done:
-                adv = self.finish_episode(trajectory_rewards, trajectory_values)
-                trajectory_advantages.append(adv)
-                obs = self.env.reset()
-                break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards)
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            total_reward = sum(rewards.detach().numpy())
-            mean_reward = np.mean(total_reward)
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/total reward': total_reward,
-                'reward/mean reward': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_224254-1omblhza/files/config.yaml b/wandb/run-20221214_224254-1omblhza/files/config.yaml
deleted file mode 100644
index 6b9c277..0000000
--- a/wandb/run-20221214_224254-1omblhza/files/config.yaml
+++ /dev/null
@@ -1,58 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671054174.285209
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_224254-1omblhza/files/diff.patch b/wandb/run-20221214_224254-1omblhza/files/diff.patch
deleted file mode 100644
index c30b74c..0000000
--- a/wandb/run-20221214_224254-1omblhza/files/diff.patch
+++ /dev/null
@@ -1,544 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..a88e3ad 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -53,7 +54,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,7 +77,7 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-@@ -84,7 +85,7 @@ class PolicyNet(Net):
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
-         clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +94,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +114,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +124,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +140,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +164,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,74 +193,72 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def finish_episode(self, rewards, values):
-+        # calculate stats and reset all values
-+        self.ep_returns.append(sum(rewards))
-+        # STEP 5: compute advantage estimates A_t at timestep num_steps
-+        advantage = self.advantage_estimate(rewards, values)
-+        return advantage
-+
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for i in range(n_step):
- 
-             # render gym env
--            if render:
-+            if render and i % 2 == 0:
-                 self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
--                # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
-                 
--                # STEP 3: collecting set of trajectories D_k by running action 
--                # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
--
--                # tracking of values
--                trajectory_actions.append(action)
--                trajectory_action_probs.append(log_probability)
--                trajectory_rewards.append(reward)
-+            # action logic
-+            action, log_probability, _ = self.step(obs)
-+                    
-+            # STEP 3: collecting set of trajectories D_k by running action 
-+            # that was sampled from policy in environment
-+            __obs, reward, done, _ = self.env.step(action)
-+            value = self.get_value(__obs)
-+            # tracking of values
-+            trajectory_obs.append(obs)
-+            trajectory_actions.append(action)
-+            trajectory_action_probs.append(log_probability)
-+            trajectory_rewards.append(reward)
-+            trajectory_values.append(value.detach())
-                 
--                # break out of loop if episode is terminated
--                if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
--                    break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-+            obs = __obs
-+            self.num_steps += 1
-+
-+            # break out of loop if episode is terminated
-+            if done:
-+                adv = self.finish_episode(trajectory_rewards, trajectory_values)
-+                trajectory_advantages.append(adv)
-+                obs = self.env.reset()
-+                break
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-@@ -270,56 +272,46 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
--            
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            total_reward = sum(rewards.detach().numpy())
-+            mean_reward = np.mean(total_reward)
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
-+                'reward/total reward': total_reward,
-                 'reward/mean reward': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +342,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +384,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +435,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +456,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_224254-1omblhza/files/output.log b/wandb/run-20221214_224254-1omblhza/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_224254-1omblhza/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_224254-1omblhza/files/wandb-summary.json b/wandb/run-20221214_224254-1omblhza/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_224254-1omblhza/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_224254-1omblhza/logs/debug-internal.log b/wandb/run-20221214_224254-1omblhza/logs/debug-internal.log
deleted file mode 100644
index d7228a2..0000000
--- a/wandb/run-20221214_224254-1omblhza/logs/debug-internal.log
+++ /dev/null
@@ -1,63 +0,0 @@
-2022-12-14 22:42:54,299 INFO    StreamThr :4939 [internal.py:wandb_internal():87] W&B internal server running at pid: 4939, started at: 2022-12-14 22:42:54.297628
-2022-12-14 22:42:54,302 DEBUG   HandlerThread:4939 [handler.py:handle_request():139] handle_request: status
-2022-12-14 22:42:54,305 DEBUG   SenderThread:4939 [sender.py:send_request():317] send_request: status
-2022-12-14 22:42:54,309 INFO    WriterThread:4939 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/run-1omblhza.wandb
-2022-12-14 22:42:54,309 DEBUG   SenderThread:4939 [sender.py:send():303] send: header
-2022-12-14 22:42:54,309 DEBUG   SenderThread:4939 [sender.py:send():303] send: run
-2022-12-14 22:42:54,807 INFO    SenderThread:4939 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files
-2022-12-14 22:42:54,807 INFO    SenderThread:4939 [sender.py:_start_run_threads():928] run started: 1omblhza with start time 1671054174.285209
-2022-12-14 22:42:54,808 DEBUG   SenderThread:4939 [sender.py:send():303] send: summary
-2022-12-14 22:42:54,808 INFO    SenderThread:4939 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 22:42:54,810 DEBUG   HandlerThread:4939 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 22:42:54,810 DEBUG   SenderThread:4939 [sender.py:send_request():317] send_request: check_version
-2022-12-14 22:42:54,966 DEBUG   HandlerThread:4939 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 22:42:54,973 DEBUG   HandlerThread:4939 [system_info.py:__init__():31] System info init
-2022-12-14 22:42:54,973 DEBUG   HandlerThread:4939 [system_info.py:__init__():46] System info init done
-2022-12-14 22:42:54,973 INFO    HandlerThread:4939 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 22:42:54,974 INFO    HandlerThread:4939 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 22:42:54,974 DEBUG   HandlerThread:4939 [system_info.py:probe():195] Probing system
-2022-12-14 22:42:54,974 INFO    SystemMonitor:4939 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 22:42:54,975 INFO    SystemMonitor:4939 [interfaces.py:start():168] Started cpu
-2022-12-14 22:42:54,976 INFO    SystemMonitor:4939 [interfaces.py:start():168] Started disk
-2022-12-14 22:42:54,978 INFO    SystemMonitor:4939 [interfaces.py:start():168] Started gpuapple
-2022-12-14 22:42:54,983 INFO    SystemMonitor:4939 [interfaces.py:start():168] Started memory
-2022-12-14 22:42:54,985 DEBUG   HandlerThread:4939 [system_info.py:_probe_git():180] Probing git
-2022-12-14 22:42:54,985 INFO    SystemMonitor:4939 [interfaces.py:start():168] Started network
-2022-12-14 22:42:55,001 DEBUG   HandlerThread:4939 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 22:42:55,001 DEBUG   HandlerThread:4939 [system_info.py:probe():241] Probing system done
-2022-12-14 22:42:55,001 DEBUG   HandlerThread:4939 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T21:42:54.974347', 'startedAt': '2022-12-14T21:42:54.242314', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 22:42:55,001 INFO    HandlerThread:4939 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 22:42:55,001 INFO    HandlerThread:4939 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 22:42:55,001 DEBUG   HandlerThread:4939 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 22:42:55,002 DEBUG   HandlerThread:4939 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 22:42:55,002 DEBUG   HandlerThread:4939 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 22:42:55,816 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/conda-environment.yaml
-2022-12-14 22:42:55,817 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/requirements.txt
-2022-12-14 22:42:55,817 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/wandb-summary.json
-2022-12-14 22:42:56,092 DEBUG   HandlerThread:4939 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 22:42:56,092 DEBUG   HandlerThread:4939 [system_info.py:_save_code():89] Saving code
-2022-12-14 22:42:56,103 DEBUG   HandlerThread:4939 [system_info.py:_save_code():110] Saving code done
-2022-12-14 22:42:56,103 DEBUG   HandlerThread:4939 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 22:42:56,164 DEBUG   HandlerThread:4939 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 22:42:56,165 INFO    HandlerThread:4939 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 22:42:56,255 DEBUG   SenderThread:4939 [sender.py:send():303] send: files
-2022-12-14 22:42:56,255 INFO    SenderThread:4939 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 22:42:56,255 INFO    SenderThread:4939 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 22:42:56,256 INFO    SenderThread:4939 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 22:42:56,261 DEBUG   HandlerThread:4939 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 22:42:56,261 DEBUG   SenderThread:4939 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 22:42:56,612 DEBUG   SenderThread:4939 [sender.py:send():303] send: telemetry
-2022-12-14 22:42:56,626 INFO    Thread-23 :4939 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpecnvgcerwandb/ug2b40co-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 22:42:56,817 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/conda-environment.yaml
-2022-12-14 22:42:56,817 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/output.log
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/diff.patch
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/wandb-metadata.json
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/code
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/code/PPO
-2022-12-14 22:42:56,818 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/code/PPO/ppo_torch
-2022-12-14 22:42:57,083 INFO    Thread-22 :4939 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpecnvgcerwandb/2x0ri3q2-wandb-metadata.json
-2022-12-14 22:42:57,115 INFO    Thread-24 :4939 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpecnvgcerwandb/24mizh8r-diff.patch
-2022-12-14 22:42:58,830 INFO    Thread-19 :4939 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/files/output.log
-2022-12-14 22:43:11,620 DEBUG   HandlerThread:4939 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 22:43:11,620 DEBUG   SenderThread:4939 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_224254-1omblhza/logs/debug.log b/wandb/run-20221214_224254-1omblhza/logs/debug.log
deleted file mode 100644
index 195b3ea..0000000
--- a/wandb/run-20221214_224254-1omblhza/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 22:42:54,246 INFO    MainThread:4929 [wandb_setup.py:_flush():68] Configure stats pid to 4929
-2022-12-14 22:42:54,246 INFO    MainThread:4929 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 22:42:54,246 INFO    MainThread:4929 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 22:42:54,246 INFO    MainThread:4929 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 22:42:54,247 INFO    MainThread:4929 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 22:42:54,247 INFO    MainThread:4929 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/logs/debug.log
-2022-12-14 22:42:54,247 INFO    MainThread:4929 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_224254-1omblhza/logs/debug-internal.log
-2022-12-14 22:42:54,247 INFO    MainThread:4929 [wandb_init.py:init():516] calling init triggers
-2022-12-14 22:42:54,248 INFO    MainThread:4929 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 22:42:54,248 INFO    MainThread:4929 [wandb_init.py:init():569] starting backend
-2022-12-14 22:42:54,248 INFO    MainThread:4929 [wandb_init.py:init():573] setting up manager
-2022-12-14 22:42:54,279 INFO    MainThread:4929 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 22:42:54,284 INFO    MainThread:4929 [wandb_init.py:init():580] backend started and connected
-2022-12-14 22:42:54,291 INFO    MainThread:4929 [wandb_init.py:init():658] updated telemetry
-2022-12-14 22:42:54,303 INFO    MainThread:4929 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 22:42:54,809 INFO    MainThread:4929 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 22:42:54,935 INFO    MainThread:4929 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 22:42:54,935 INFO    MainThread:4929 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 22:42:56,261 INFO    MainThread:4929 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 22:42:56,262 INFO    MainThread:4929 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 22:42:56,262 INFO    MainThread:4929 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 22:42:56,262 INFO    MainThread:4929 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 22:42:56,263 INFO    MainThread:4929 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_224254-1omblhza/run-1omblhza.wandb b/wandb/run-20221214_224254-1omblhza/run-1omblhza.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_225344-3i0odo9b/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 8a8b3ea..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,469 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for i in range(n_step):
-
-            # render gym env
-            if render and i % 2 == 0:
-                self.env.render(mode='human')
-                
-            # action logic
-            action, log_probability, _ = self.step(obs)
-                    
-            # STEP 3: collecting set of trajectories D_k by running action 
-            # that was sampled from policy in environment
-            __obs, reward, done, _ = self.env.step(action)
-            value = self.get_value(__obs)
-            # tracking of values
-            trajectory_obs.append(obs)
-            trajectory_actions.append(action)
-            trajectory_action_probs.append(log_probability)
-            trajectory_rewards.append(reward)
-            trajectory_values.append(value.detach())
-                
-            obs = __obs
-            self.num_steps += 1
-
-            # break out of loop if episode is terminated
-            if done:
-                # calculate stats and reset
-                self.ep_returns.append(sum(rewards))
-                # STEP 5: compute advantage estimates A_t at timestep num_steps
-                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                trajectory_advantages.append(advantage)
-                obs = self.env.reset()
-                break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards)
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-            
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/conda-environment.yaml b/wandb/run-20221214_225344-3i0odo9b/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/config.yaml b/wandb/run-20221214_225344-3i0odo9b/files/config.yaml
deleted file mode 100644
index 766e7a5..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/config.yaml
+++ /dev/null
@@ -1,58 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671054824.1545
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/diff.patch b/wandb/run-20221214_225344-3i0odo9b/files/diff.patch
deleted file mode 100644
index 3229460..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/diff.patch
+++ /dev/null
@@ -1,539 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..8a8b3ea 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -53,7 +54,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,7 +77,7 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-@@ -84,7 +85,7 @@ class PolicyNet(Net):
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
-         clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +94,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +114,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +124,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +140,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +164,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,74 +193,69 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for i in range(n_step):
- 
-             # render gym env
--            if render:
-+            if render and i % 2 == 0:
-                 self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
--                # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
-                 
--                # STEP 3: collecting set of trajectories D_k by running action 
--                # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
--
--                # tracking of values
--                trajectory_actions.append(action)
--                trajectory_action_probs.append(log_probability)
--                trajectory_rewards.append(reward)
-+            # action logic
-+            action, log_probability, _ = self.step(obs)
-+                    
-+            # STEP 3: collecting set of trajectories D_k by running action 
-+            # that was sampled from policy in environment
-+            __obs, reward, done, _ = self.env.step(action)
-+            value = self.get_value(__obs)
-+            # tracking of values
-+            trajectory_obs.append(obs)
-+            trajectory_actions.append(action)
-+            trajectory_action_probs.append(log_probability)
-+            trajectory_rewards.append(reward)
-+            trajectory_values.append(value.detach())
-                 
--                # break out of loop if episode is terminated
--                if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
--                    break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-+            obs = __obs
-+            self.num_steps += 1
-+
-+            # break out of loop if episode is terminated
-+            if done:
-+                # calculate stats and reset
-+                self.ep_returns.append(sum(rewards))
-+                # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                trajectory_advantages.append(advantage)
-+                obs = self.env.reset()
-+                break
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-@@ -270,56 +269,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +337,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +379,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +430,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +451,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/output.log b/wandb/run-20221214_225344-3i0odo9b/files/output.log
deleted file mode 100644
index 6e10d51..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/output.log
+++ /dev/null
@@ -1,5 +0,0 @@
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
-  return _methods._mean(a, axis=axis, dtype=dtype,
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/requirements.txt b/wandb/run-20221214_225344-3i0odo9b/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/wandb-metadata.json b/wandb/run-20221214_225344-3i0odo9b/files/wandb-metadata.json
deleted file mode 100644
index c735a53..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T21:53:44.860463",
-    "startedAt": "2022-12-14T21:53:44.111172",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_225344-3i0odo9b/files/wandb-summary.json b/wandb/run-20221214_225344-3i0odo9b/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_225344-3i0odo9b/logs/debug-internal.log b/wandb/run-20221214_225344-3i0odo9b/logs/debug-internal.log
deleted file mode 100644
index 3e043e2..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/logs/debug-internal.log
+++ /dev/null
@@ -1,61 +0,0 @@
-2022-12-14 22:53:44,168 INFO    StreamThr :5369 [internal.py:wandb_internal():87] W&B internal server running at pid: 5369, started at: 2022-12-14 22:53:44.166963
-2022-12-14 22:53:44,172 DEBUG   HandlerThread:5369 [handler.py:handle_request():139] handle_request: status
-2022-12-14 22:53:44,174 DEBUG   SenderThread:5369 [sender.py:send_request():317] send_request: status
-2022-12-14 22:53:44,176 DEBUG   SenderThread:5369 [sender.py:send():303] send: header
-2022-12-14 22:53:44,176 INFO    WriterThread:5369 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/run-3i0odo9b.wandb
-2022-12-14 22:53:44,179 DEBUG   SenderThread:5369 [sender.py:send():303] send: run
-2022-12-14 22:53:44,692 INFO    SenderThread:5369 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files
-2022-12-14 22:53:44,692 INFO    SenderThread:5369 [sender.py:_start_run_threads():928] run started: 3i0odo9b with start time 1671054824.1545
-2022-12-14 22:53:44,693 DEBUG   SenderThread:5369 [sender.py:send():303] send: summary
-2022-12-14 22:53:44,693 INFO    SenderThread:5369 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 22:53:44,696 DEBUG   HandlerThread:5369 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 22:53:44,696 DEBUG   SenderThread:5369 [sender.py:send_request():317] send_request: check_version
-2022-12-14 22:53:44,849 DEBUG   HandlerThread:5369 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 22:53:44,857 DEBUG   HandlerThread:5369 [system_info.py:__init__():31] System info init
-2022-12-14 22:53:44,857 DEBUG   HandlerThread:5369 [system_info.py:__init__():46] System info init done
-2022-12-14 22:53:44,858 INFO    HandlerThread:5369 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 22:53:44,859 INFO    HandlerThread:5369 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 22:53:44,860 DEBUG   HandlerThread:5369 [system_info.py:probe():195] Probing system
-2022-12-14 22:53:44,860 INFO    SystemMonitor:5369 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 22:53:44,862 INFO    SystemMonitor:5369 [interfaces.py:start():168] Started cpu
-2022-12-14 22:53:44,864 INFO    SystemMonitor:5369 [interfaces.py:start():168] Started disk
-2022-12-14 22:53:44,864 INFO    SystemMonitor:5369 [interfaces.py:start():168] Started gpuapple
-2022-12-14 22:53:44,873 INFO    SystemMonitor:5369 [interfaces.py:start():168] Started memory
-2022-12-14 22:53:44,875 DEBUG   HandlerThread:5369 [system_info.py:_probe_git():180] Probing git
-2022-12-14 22:53:44,877 INFO    SystemMonitor:5369 [interfaces.py:start():168] Started network
-2022-12-14 22:53:44,893 DEBUG   HandlerThread:5369 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 22:53:44,893 DEBUG   HandlerThread:5369 [system_info.py:probe():241] Probing system done
-2022-12-14 22:53:44,893 DEBUG   HandlerThread:5369 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T21:53:44.860463', 'startedAt': '2022-12-14T21:53:44.111172', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 22:53:44,893 INFO    HandlerThread:5369 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 22:53:44,893 INFO    HandlerThread:5369 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 22:53:44,894 DEBUG   HandlerThread:5369 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 22:53:44,894 DEBUG   HandlerThread:5369 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 22:53:44,894 DEBUG   HandlerThread:5369 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 22:53:45,701 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/conda-environment.yaml
-2022-12-14 22:53:45,702 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/wandb-summary.json
-2022-12-14 22:53:45,702 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/requirements.txt
-2022-12-14 22:53:46,003 DEBUG   HandlerThread:5369 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 22:53:46,003 DEBUG   HandlerThread:5369 [system_info.py:_save_code():89] Saving code
-2022-12-14 22:53:46,010 DEBUG   HandlerThread:5369 [system_info.py:_save_code():110] Saving code done
-2022-12-14 22:53:46,010 DEBUG   HandlerThread:5369 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 22:53:46,071 DEBUG   HandlerThread:5369 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 22:53:46,072 INFO    HandlerThread:5369 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 22:53:46,162 DEBUG   SenderThread:5369 [sender.py:send():303] send: files
-2022-12-14 22:53:46,163 INFO    SenderThread:5369 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 22:53:46,163 INFO    SenderThread:5369 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 22:53:46,163 INFO    SenderThread:5369 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 22:53:46,175 DEBUG   HandlerThread:5369 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 22:53:46,175 DEBUG   SenderThread:5369 [sender.py:send():303] send: telemetry
-2022-12-14 22:53:46,177 DEBUG   SenderThread:5369 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 22:53:46,521 INFO    Thread-23 :5369 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpvekylce1wandb/18g1bxsr-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 22:53:46,700 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/conda-environment.yaml
-2022-12-14 22:53:46,700 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/diff.patch
-2022-12-14 22:53:46,700 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/wandb-metadata.json
-2022-12-14 22:53:46,700 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 22:53:46,701 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/output.log
-2022-12-14 22:53:46,701 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/code/PPO/ppo_torch
-2022-12-14 22:53:46,701 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/code/PPO
-2022-12-14 22:53:46,701 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/code
-2022-12-14 22:53:47,004 INFO    Thread-22 :5369 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpvekylce1wandb/riieu43o-wandb-metadata.json
-2022-12-14 22:53:47,028 INFO    Thread-24 :5369 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpvekylce1wandb/3igjuyis-diff.patch
-2022-12-14 22:53:48,714 INFO    Thread-19 :5369 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/files/output.log
diff --git a/wandb/run-20221214_225344-3i0odo9b/logs/debug.log b/wandb/run-20221214_225344-3i0odo9b/logs/debug.log
deleted file mode 100644
index c578a43..0000000
--- a/wandb/run-20221214_225344-3i0odo9b/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_setup.py:_flush():68] Configure stats pid to 5359
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 22:53:44,116 INFO    MainThread:5359 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/logs/debug.log
-2022-12-14 22:53:44,117 INFO    MainThread:5359 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_225344-3i0odo9b/logs/debug-internal.log
-2022-12-14 22:53:44,117 INFO    MainThread:5359 [wandb_init.py:init():516] calling init triggers
-2022-12-14 22:53:44,117 INFO    MainThread:5359 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 22:53:44,118 INFO    MainThread:5359 [wandb_init.py:init():569] starting backend
-2022-12-14 22:53:44,118 INFO    MainThread:5359 [wandb_init.py:init():573] setting up manager
-2022-12-14 22:53:44,148 INFO    MainThread:5359 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 22:53:44,154 INFO    MainThread:5359 [wandb_init.py:init():580] backend started and connected
-2022-12-14 22:53:44,161 INFO    MainThread:5359 [wandb_init.py:init():658] updated telemetry
-2022-12-14 22:53:44,174 INFO    MainThread:5359 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 22:53:44,694 INFO    MainThread:5359 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 22:53:44,818 INFO    MainThread:5359 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 22:53:44,818 INFO    MainThread:5359 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 22:53:46,168 INFO    MainThread:5359 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 22:53:46,168 INFO    MainThread:5359 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 22:53:46,168 INFO    MainThread:5359 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 22:53:46,169 INFO    MainThread:5359 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 22:53:46,169 INFO    MainThread:5359 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_225344-3i0odo9b/run-3i0odo9b.wandb b/wandb/run-20221214_225344-3i0odo9b/run-3i0odo9b.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index d6c46f1..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,469 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for i in range(n_step):
-
-            # render gym env
-            if render:
-                self.env.render(mode='human')
-                
-            # action logic
-            action, log_probability, _ = self.step(obs)
-                    
-            # STEP 3: collecting set of trajectories D_k by running action 
-            # that was sampled from policy in environment
-            __obs, reward, done, _ = self.env.step(action)
-            value = self.get_value(__obs)
-            
-            # tracking of values
-            trajectory_obs.append(obs)
-            trajectory_actions.append(action)
-            trajectory_action_probs.append(log_probability)
-            trajectory_rewards.append(reward)
-            trajectory_values.append(value.detach())
-                
-            obs = __obs
-            self.num_steps += 1
-
-            # break out of loop if episode is terminated
-            if done:
-                # calculate stats and reset
-                self.ep_returns.append(sum(rewards))
-                # STEP 5: compute advantage estimates A_t at timestep num_steps
-                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                trajectory_advantages.append(advantage)
-                obs = self.env.reset()
-                break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/conda-environment.yaml b/wandb/run-20221214_230510-1t2jf2ki/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/config.yaml b/wandb/run-20221214_230510-1t2jf2ki/files/config.yaml
deleted file mode 100644
index 1bcd74c..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671055510.384167
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/diff.patch b/wandb/run-20221214_230510-1t2jf2ki/files/diff.patch
deleted file mode 100644
index 5484aff..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/diff.patch
+++ /dev/null
@@ -1,556 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..d6c46f1 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,76 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for i in range(n_step):
- 
-             # render gym env
-             if render:
-                 self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
--                # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
--                # STEP 3: collecting set of trajectories D_k by running action 
--                # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
--
--                # tracking of values
--                trajectory_actions.append(action)
--                trajectory_action_probs.append(log_probability)
--                trajectory_rewards.append(reward)
-                 
--                # break out of loop if episode is terminated
--                if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
--                    break
-+            # action logic
-+            action, log_probability, _ = self.step(obs)
-+                    
-+            # STEP 3: collecting set of trajectories D_k by running action 
-+            # that was sampled from policy in environment
-+            __obs, reward, done, _ = self.env.step(action)
-+            value = self.get_value(__obs)
-             
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-+            # tracking of values
-+            trajectory_obs.append(obs)
-+            trajectory_actions.append(action)
-+            trajectory_action_probs.append(log_probability)
-+            trajectory_rewards.append(reward)
-+            trajectory_values.append(value.detach())
-+                
-+            obs = __obs
-+            self.num_steps += 1
-+
-+            # break out of loop if episode is terminated
-+            if done:
-+                # calculate stats and reset
-+                self.ep_returns.append(sum(rewards))
-+                # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                trajectory_advantages.append(advantage)
-+                obs = self.env.reset()
-+                break
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +269,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
--            
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +337,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +379,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +430,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +451,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/output.log b/wandb/run-20221214_230510-1t2jf2ki/files/output.log
deleted file mode 100644
index 3b9301b..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/output.log
+++ /dev/null
@@ -1,7 +0,0 @@
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
-DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/5288891/envelope/ HTTP/1.1" 200 2
-INFO:root:Collecting batch trajectories...
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
-  return _methods._mean(a, axis=axis, dtype=dtype,
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
-  ret = ret.dtype.type(ret / rcount)
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/requirements.txt b/wandb/run-20221214_230510-1t2jf2ki/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/wandb-metadata.json b/wandb/run-20221214_230510-1t2jf2ki/files/wandb-metadata.json
deleted file mode 100644
index b14425c..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:05:11.082299",
-    "startedAt": "2022-12-14T22:05:10.345768",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_230510-1t2jf2ki/files/wandb-summary.json b/wandb/run-20221214_230510-1t2jf2ki/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_230510-1t2jf2ki/logs/debug-internal.log b/wandb/run-20221214_230510-1t2jf2ki/logs/debug-internal.log
deleted file mode 100644
index 7f9e6e8..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/logs/debug-internal.log
+++ /dev/null
@@ -1,95 +0,0 @@
-2022-12-14 23:05:10,398 INFO    StreamThr :5676 [internal.py:wandb_internal():87] W&B internal server running at pid: 5676, started at: 2022-12-14 23:05:10.397063
-2022-12-14 23:05:10,401 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:05:10,403 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: status
-2022-12-14 23:05:10,407 DEBUG   SenderThread:5676 [sender.py:send():303] send: header
-2022-12-14 23:05:10,407 DEBUG   SenderThread:5676 [sender.py:send():303] send: run
-2022-12-14 23:05:10,413 INFO    WriterThread:5676 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/run-1t2jf2ki.wandb
-2022-12-14 23:05:10,924 INFO    SenderThread:5676 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files
-2022-12-14 23:05:10,924 INFO    SenderThread:5676 [sender.py:_start_run_threads():928] run started: 1t2jf2ki with start time 1671055510.384167
-2022-12-14 23:05:10,924 DEBUG   SenderThread:5676 [sender.py:send():303] send: summary
-2022-12-14 23:05:10,924 INFO    SenderThread:5676 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:05:10,927 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:05:10,927 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:05:11,074 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:05:11,081 DEBUG   HandlerThread:5676 [system_info.py:__init__():31] System info init
-2022-12-14 23:05:11,081 DEBUG   HandlerThread:5676 [system_info.py:__init__():46] System info init done
-2022-12-14 23:05:11,081 INFO    HandlerThread:5676 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:05:11,081 INFO    HandlerThread:5676 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:05:11,082 DEBUG   HandlerThread:5676 [system_info.py:probe():195] Probing system
-2022-12-14 23:05:11,082 INFO    SystemMonitor:5676 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:05:11,083 INFO    SystemMonitor:5676 [interfaces.py:start():168] Started cpu
-2022-12-14 23:05:11,084 INFO    SystemMonitor:5676 [interfaces.py:start():168] Started disk
-2022-12-14 23:05:11,084 INFO    SystemMonitor:5676 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:05:11,091 INFO    SystemMonitor:5676 [interfaces.py:start():168] Started memory
-2022-12-14 23:05:11,092 DEBUG   HandlerThread:5676 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:05:11,092 INFO    SystemMonitor:5676 [interfaces.py:start():168] Started network
-2022-12-14 23:05:11,107 DEBUG   HandlerThread:5676 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:05:11,108 DEBUG   HandlerThread:5676 [system_info.py:probe():241] Probing system done
-2022-12-14 23:05:11,108 DEBUG   HandlerThread:5676 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:05:11.082299', 'startedAt': '2022-12-14T22:05:10.345768', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:05:11,108 INFO    HandlerThread:5676 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:05:11,108 INFO    HandlerThread:5676 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:05:11,108 DEBUG   HandlerThread:5676 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:05:11,109 DEBUG   HandlerThread:5676 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:05:11,109 DEBUG   HandlerThread:5676 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:05:11,933 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/wandb-summary.json
-2022-12-14 23:05:11,933 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/conda-environment.yaml
-2022-12-14 23:05:11,933 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/requirements.txt
-2022-12-14 23:05:12,195 DEBUG   HandlerThread:5676 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:05:12,195 DEBUG   HandlerThread:5676 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:05:12,202 DEBUG   HandlerThread:5676 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:05:12,202 DEBUG   HandlerThread:5676 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:05:12,265 DEBUG   HandlerThread:5676 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:05:12,266 INFO    HandlerThread:5676 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:05:12,358 DEBUG   SenderThread:5676 [sender.py:send():303] send: files
-2022-12-14 23:05:12,358 INFO    SenderThread:5676 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:05:12,359 INFO    SenderThread:5676 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:05:12,360 INFO    SenderThread:5676 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:05:12,366 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:05:12,367 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:05:12,721 DEBUG   SenderThread:5676 [sender.py:send():303] send: telemetry
-2022-12-14 23:05:12,732 INFO    Thread-23 :5676 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpkm9ugn1dwandb/1nlge909-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:05:12,935 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/conda-environment.yaml
-2022-12-14 23:05:12,935 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:05:12,935 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/diff.patch
-2022-12-14 23:05:12,935 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/wandb-metadata.json
-2022-12-14 23:05:12,935 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO/ppo_torch
-2022-12-14 23:05:12,936 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/code
-2022-12-14 23:05:12,936 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/code/PPO
-2022-12-14 23:05:13,356 INFO    Thread-22 :5676 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpkm9ugn1dwandb/3mhzi1nz-wandb-metadata.json
-2022-12-14 23:05:13,357 INFO    Thread-24 :5676 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpkm9ugn1dwandb/2v25ypom-diff.patch
-2022-12-14 23:05:27,734 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:05:27,734 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:05:42,101 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/config.yaml
-2022-12-14 23:05:42,969 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:05:42,970 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:05:58,230 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:05:58,231 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:06:11,094 DEBUG   SystemMonitor:5676 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:06:11,097 DEBUG   SenderThread:5676 [sender.py:send():303] send: telemetry
-2022-12-14 23:06:11,097 DEBUG   SenderThread:5676 [sender.py:send():303] send: stats
-2022-12-14 23:06:11,246 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/output.log
-2022-12-14 23:06:13,256 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/output.log
-2022-12-14 23:06:13,256 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/config.yaml
-2022-12-14 23:06:13,500 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:06:13,500 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:06:28,763 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:06:28,764 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:06:37,388 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/output.log
-2022-12-14 23:06:41,099 DEBUG   SenderThread:5676 [sender.py:send():303] send: stats
-2022-12-14 23:06:44,171 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:06:44,176 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:06:59,421 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:06:59,422 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:07:11,104 DEBUG   SenderThread:5676 [sender.py:send():303] send: stats
-2022-12-14 23:07:14,679 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:07:14,680 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:07:26,648 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/output.log
-2022-12-14 23:07:29,945 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:07:29,946 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:07:39,715 INFO    Thread-19 :5676 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/files/output.log
-2022-12-14 23:07:41,113 DEBUG   SenderThread:5676 [sender.py:send():303] send: stats
-2022-12-14 23:07:45,197 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:07:45,197 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:08:00,423 DEBUG   HandlerThread:5676 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:08:00,424 DEBUG   SenderThread:5676 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:08:11,123 DEBUG   SenderThread:5676 [sender.py:send():303] send: stats
diff --git a/wandb/run-20221214_230510-1t2jf2ki/logs/debug.log b/wandb/run-20221214_230510-1t2jf2ki/logs/debug.log
deleted file mode 100644
index a447ab7..0000000
--- a/wandb/run-20221214_230510-1t2jf2ki/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:05:10,349 INFO    MainThread:5666 [wandb_setup.py:_flush():68] Configure stats pid to 5666
-2022-12-14 23:05:10,349 INFO    MainThread:5666 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:05:10,349 INFO    MainThread:5666 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:05:10,349 INFO    MainThread:5666 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:05:10,349 INFO    MainThread:5666 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:05:10,350 INFO    MainThread:5666 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/logs/debug.log
-2022-12-14 23:05:10,350 INFO    MainThread:5666 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230510-1t2jf2ki/logs/debug-internal.log
-2022-12-14 23:05:10,350 INFO    MainThread:5666 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:05:10,350 INFO    MainThread:5666 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:05:10,351 INFO    MainThread:5666 [wandb_init.py:init():569] starting backend
-2022-12-14 23:05:10,351 INFO    MainThread:5666 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:05:10,378 INFO    MainThread:5666 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:05:10,383 INFO    MainThread:5666 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:05:10,390 INFO    MainThread:5666 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:05:10,402 INFO    MainThread:5666 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:05:10,925 INFO    MainThread:5666 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:05:11,042 INFO    MainThread:5666 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:05:11,042 INFO    MainThread:5666 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:05:12,366 INFO    MainThread:5666 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:05:12,366 INFO    MainThread:5666 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:05:12,367 INFO    MainThread:5666 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:05:12,367 INFO    MainThread:5666 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:05:12,368 INFO    MainThread:5666 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_230510-1t2jf2ki/run-1t2jf2ki.wandb b/wandb/run-20221214_230510-1t2jf2ki/run-1t2jf2ki.wandb
deleted file mode 100644
index 78090a8..0000000
Binary files a/wandb/run-20221214_230510-1t2jf2ki/run-1t2jf2ki.wandb and /dev/null differ
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_230818-2ud2n34b/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index d6c46f1..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,469 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for i in range(n_step):
-
-            # render gym env
-            if render:
-                self.env.render(mode='human')
-                
-            # action logic
-            action, log_probability, _ = self.step(obs)
-                    
-            # STEP 3: collecting set of trajectories D_k by running action 
-            # that was sampled from policy in environment
-            __obs, reward, done, _ = self.env.step(action)
-            value = self.get_value(__obs)
-            
-            # tracking of values
-            trajectory_obs.append(obs)
-            trajectory_actions.append(action)
-            trajectory_action_probs.append(log_probability)
-            trajectory_rewards.append(reward)
-            trajectory_values.append(value.detach())
-                
-            obs = __obs
-            self.num_steps += 1
-
-            # break out of loop if episode is terminated
-            if done:
-                # calculate stats and reset
-                self.ep_returns.append(sum(rewards))
-                # STEP 5: compute advantage estimates A_t at timestep num_steps
-                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                trajectory_advantages.append(advantage)
-                obs = self.env.reset()
-                break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/conda-environment.yaml b/wandb/run-20221214_230818-2ud2n34b/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/config.yaml b/wandb/run-20221214_230818-2ud2n34b/files/config.yaml
deleted file mode 100644
index 029187c..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671055698.553029
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/diff.patch b/wandb/run-20221214_230818-2ud2n34b/files/diff.patch
deleted file mode 100644
index 5484aff..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/diff.patch
+++ /dev/null
@@ -1,556 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..d6c46f1 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,76 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for i in range(n_step):
- 
-             # render gym env
-             if render:
-                 self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
--                # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
--                # STEP 3: collecting set of trajectories D_k by running action 
--                # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
--
--                # tracking of values
--                trajectory_actions.append(action)
--                trajectory_action_probs.append(log_probability)
--                trajectory_rewards.append(reward)
-                 
--                # break out of loop if episode is terminated
--                if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
--                    break
-+            # action logic
-+            action, log_probability, _ = self.step(obs)
-+                    
-+            # STEP 3: collecting set of trajectories D_k by running action 
-+            # that was sampled from policy in environment
-+            __obs, reward, done, _ = self.env.step(action)
-+            value = self.get_value(__obs)
-             
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-+            # tracking of values
-+            trajectory_obs.append(obs)
-+            trajectory_actions.append(action)
-+            trajectory_action_probs.append(log_probability)
-+            trajectory_rewards.append(reward)
-+            trajectory_values.append(value.detach())
-+                
-+            obs = __obs
-+            self.num_steps += 1
-+
-+            # break out of loop if episode is terminated
-+            if done:
-+                # calculate stats and reset
-+                self.ep_returns.append(sum(rewards))
-+                # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                trajectory_advantages.append(advantage)
-+                obs = self.env.reset()
-+                break
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +269,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
--            
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +337,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +379,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +430,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +451,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/output.log b/wandb/run-20221214_230818-2ud2n34b/files/output.log
deleted file mode 100644
index 4e9efca..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/output.log
+++ /dev/null
@@ -1,6 +0,0 @@
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
-DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/5288891/envelope/ HTTP/1.1" 200 2
-INFO:root:Collecting batch trajectories...
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
-  return _methods._mean(a, axis=axis, dtype=dtype,
-/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/requirements.txt b/wandb/run-20221214_230818-2ud2n34b/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/wandb-metadata.json b/wandb/run-20221214_230818-2ud2n34b/files/wandb-metadata.json
deleted file mode 100644
index cc637e6..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:08:19.300788",
-    "startedAt": "2022-12-14T22:08:18.509433",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_230818-2ud2n34b/files/wandb-summary.json b/wandb/run-20221214_230818-2ud2n34b/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_230818-2ud2n34b/logs/debug-internal.log b/wandb/run-20221214_230818-2ud2n34b/logs/debug-internal.log
deleted file mode 100644
index d182b32..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/logs/debug-internal.log
+++ /dev/null
@@ -1,106 +0,0 @@
-2022-12-14 23:08:18,568 INFO    StreamThr :5842 [internal.py:wandb_internal():87] W&B internal server running at pid: 5842, started at: 2022-12-14 23:08:18.566395
-2022-12-14 23:08:18,572 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:08:18,574 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: status
-2022-12-14 23:08:18,578 INFO    WriterThread:5842 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/run-2ud2n34b.wandb
-2022-12-14 23:08:18,578 DEBUG   SenderThread:5842 [sender.py:send():303] send: header
-2022-12-14 23:08:18,578 DEBUG   SenderThread:5842 [sender.py:send():303] send: run
-2022-12-14 23:08:19,126 INFO    SenderThread:5842 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files
-2022-12-14 23:08:19,126 INFO    SenderThread:5842 [sender.py:_start_run_threads():928] run started: 2ud2n34b with start time 1671055698.553029
-2022-12-14 23:08:19,126 DEBUG   SenderThread:5842 [sender.py:send():303] send: summary
-2022-12-14 23:08:19,127 INFO    SenderThread:5842 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:08:19,129 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:08:19,130 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:08:19,293 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:08:19,299 DEBUG   HandlerThread:5842 [system_info.py:__init__():31] System info init
-2022-12-14 23:08:19,299 DEBUG   HandlerThread:5842 [system_info.py:__init__():46] System info init done
-2022-12-14 23:08:19,299 INFO    HandlerThread:5842 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:08:19,300 INFO    SystemMonitor:5842 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:08:19,300 INFO    HandlerThread:5842 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:08:19,300 DEBUG   HandlerThread:5842 [system_info.py:probe():195] Probing system
-2022-12-14 23:08:19,301 INFO    SystemMonitor:5842 [interfaces.py:start():168] Started cpu
-2022-12-14 23:08:19,302 INFO    SystemMonitor:5842 [interfaces.py:start():168] Started disk
-2022-12-14 23:08:19,302 INFO    SystemMonitor:5842 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:08:19,304 INFO    SystemMonitor:5842 [interfaces.py:start():168] Started memory
-2022-12-14 23:08:19,307 INFO    SystemMonitor:5842 [interfaces.py:start():168] Started network
-2022-12-14 23:08:19,312 DEBUG   HandlerThread:5842 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:08:19,328 DEBUG   HandlerThread:5842 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:08:19,328 DEBUG   HandlerThread:5842 [system_info.py:probe():241] Probing system done
-2022-12-14 23:08:19,329 DEBUG   HandlerThread:5842 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:08:19.300788', 'startedAt': '2022-12-14T22:08:18.509433', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:08:19,329 INFO    HandlerThread:5842 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:08:19,329 INFO    HandlerThread:5842 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:08:19,329 DEBUG   HandlerThread:5842 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:08:19,330 DEBUG   HandlerThread:5842 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:08:19,330 DEBUG   HandlerThread:5842 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:08:20,135 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/conda-environment.yaml
-2022-12-14 23:08:20,135 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/wandb-summary.json
-2022-12-14 23:08:20,135 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/requirements.txt
-2022-12-14 23:08:20,417 DEBUG   HandlerThread:5842 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:08:20,417 DEBUG   HandlerThread:5842 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:08:20,424 DEBUG   HandlerThread:5842 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:08:20,424 DEBUG   HandlerThread:5842 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:08:20,485 DEBUG   HandlerThread:5842 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:08:20,486 INFO    HandlerThread:5842 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:08:20,574 DEBUG   SenderThread:5842 [sender.py:send():303] send: files
-2022-12-14 23:08:20,574 INFO    SenderThread:5842 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:08:20,575 INFO    SenderThread:5842 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:08:20,575 INFO    SenderThread:5842 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:08:20,581 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:08:20,581 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:08:20,907 DEBUG   SenderThread:5842 [sender.py:send():303] send: telemetry
-2022-12-14 23:08:20,927 INFO    Thread-23 :5842 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc3mk0rzdwandb/22899iss-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/conda-environment.yaml
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/diff.patch
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/wandb-metadata.json
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/code
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/code/PPO
-2022-12-14 23:08:21,137 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/code/PPO/ppo_torch
-2022-12-14 23:08:21,465 INFO    Thread-24 :5842 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc3mk0rzdwandb/3hz3w98g-diff.patch
-2022-12-14 23:08:21,489 INFO    Thread-22 :5842 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc3mk0rzdwandb/3p31dc3g-wandb-metadata.json
-2022-12-14 23:08:35,920 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:08:35,920 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:08:50,280 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/config.yaml
-2022-12-14 23:08:51,166 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:08:51,167 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:09:06,435 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:09:06,436 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:09:19,311 DEBUG   SystemMonitor:5842 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:09:19,314 DEBUG   SenderThread:5842 [sender.py:send():303] send: telemetry
-2022-12-14 23:09:19,315 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:09:19,444 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/output.log
-2022-12-14 23:09:21,458 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/output.log
-2022-12-14 23:09:21,459 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/config.yaml
-2022-12-14 23:09:21,698 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:09:21,698 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:09:36,937 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:09:36,938 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:09:49,318 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:09:52,218 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:09:52,219 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:10:07,496 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:10:07,496 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:10:19,331 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:10:22,752 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:10:22,752 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:10:37,991 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:10:37,992 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:10:39,896 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/output.log
-2022-12-14 23:10:49,332 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:10:53,232 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:10:53,232 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:11:08,474 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:11:08,475 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:11:19,340 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:11:23,772 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:11:23,773 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:11:39,015 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:11:39,016 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:11:49,339 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:11:52,289 INFO    Thread-19 :5842 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/files/output.log
-2022-12-14 23:11:54,248 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:11:54,249 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:12:09,495 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:12:09,496 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:12:19,342 DEBUG   SenderThread:5842 [sender.py:send():303] send: stats
-2022-12-14 23:12:24,832 DEBUG   HandlerThread:5842 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:12:24,832 DEBUG   SenderThread:5842 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_230818-2ud2n34b/logs/debug.log b/wandb/run-20221214_230818-2ud2n34b/logs/debug.log
deleted file mode 100644
index 49944a5..0000000
--- a/wandb/run-20221214_230818-2ud2n34b/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:08:18,514 INFO    MainThread:5823 [wandb_setup.py:_flush():68] Configure stats pid to 5823
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/logs/debug.log
-2022-12-14 23:08:18,515 INFO    MainThread:5823 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_230818-2ud2n34b/logs/debug-internal.log
-2022-12-14 23:08:18,516 INFO    MainThread:5823 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:08:18,516 INFO    MainThread:5823 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:08:18,516 INFO    MainThread:5823 [wandb_init.py:init():569] starting backend
-2022-12-14 23:08:18,516 INFO    MainThread:5823 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:08:18,546 INFO    MainThread:5823 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:08:18,552 INFO    MainThread:5823 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:08:18,559 INFO    MainThread:5823 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:08:18,571 INFO    MainThread:5823 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:08:19,128 INFO    MainThread:5823 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:08:19,258 INFO    MainThread:5823 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:08:19,258 INFO    MainThread:5823 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:08:20,580 INFO    MainThread:5823 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:08:20,581 INFO    MainThread:5823 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:08:20,581 INFO    MainThread:5823 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:08:20,582 INFO    MainThread:5823 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:08:20,582 INFO    MainThread:5823 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_230818-2ud2n34b/run-2ud2n34b.wandb b/wandb/run-20221214_230818-2ud2n34b/run-2ud2n34b.wandb
deleted file mode 100644
index 84f45be..0000000
Binary files a/wandb/run-20221214_230818-2ud2n34b/run-2ud2n34b.wandb and /dev/null differ
diff --git a/wandb/run-20221214_231229-l0d2844l/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231229-l0d2844l/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 699a484..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,469 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            # render gym env
-            if render:
-                self.env.render(mode='human')
-                
-            # action logic
-            action, log_probability, _ = self.step(obs)
-                    
-            # STEP 3: collecting set of trajectories D_k by running action 
-            # that was sampled from policy in environment
-            __obs, reward, done, _ = self.env.step(action)
-            value = self.get_value(__obs)
-
-            # tracking of values
-            trajectory_obs.append(obs)
-            trajectory_actions.append(action)
-            trajectory_action_probs.append(log_probability)
-            trajectory_rewards.append(reward)
-            trajectory_values.append(value.detach())
-                
-            obs = __obs
-            self.num_steps += 1
-
-            # break out of loop if episode is terminated
-            if done:
-                # calculate stats and reset
-                self.ep_returns.append(sum(rewards))
-                # STEP 5: compute advantage estimates A_t at timestep num_steps
-                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                trajectory_advantages.append(advantage)
-                obs = self.env.reset()
-                break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231229-l0d2844l/files/conda-environment.yaml b/wandb/run-20221214_231229-l0d2844l/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231229-l0d2844l/files/config.yaml b/wandb/run-20221214_231229-l0d2844l/files/config.yaml
deleted file mode 100644
index 92562b6..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/config.yaml
+++ /dev/null
@@ -1,58 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671055949.438856
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231229-l0d2844l/files/diff.patch b/wandb/run-20221214_231229-l0d2844l/files/diff.patch
deleted file mode 100644
index 23dee22..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/diff.patch
+++ /dev/null
@@ -1,556 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..699a484 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,76 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for _ in range(n_step):
- 
-             # render gym env
-             if render:
-                 self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
--                # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
-                 
--                # STEP 3: collecting set of trajectories D_k by running action 
--                # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
--
--                # tracking of values
--                trajectory_actions.append(action)
--                trajectory_action_probs.append(log_probability)
--                trajectory_rewards.append(reward)
-+            # action logic
-+            action, log_probability, _ = self.step(obs)
-+                    
-+            # STEP 3: collecting set of trajectories D_k by running action 
-+            # that was sampled from policy in environment
-+            __obs, reward, done, _ = self.env.step(action)
-+            value = self.get_value(__obs)
-+
-+            # tracking of values
-+            trajectory_obs.append(obs)
-+            trajectory_actions.append(action)
-+            trajectory_action_probs.append(log_probability)
-+            trajectory_rewards.append(reward)
-+            trajectory_values.append(value.detach())
-                 
--                # break out of loop if episode is terminated
--                if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
--                    break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-+            obs = __obs
-+            self.num_steps += 1
-+
-+            # break out of loop if episode is terminated
-+            if done:
-+                # calculate stats and reset
-+                self.ep_returns.append(sum(rewards))
-+                # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                trajectory_advantages.append(advantage)
-+                obs = self.env.reset()
-+                break
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +269,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
--            
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +337,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +379,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +430,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +451,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231229-l0d2844l/files/output.log b/wandb/run-20221214_231229-l0d2844l/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_231229-l0d2844l/files/requirements.txt b/wandb/run-20221214_231229-l0d2844l/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231229-l0d2844l/files/wandb-metadata.json b/wandb/run-20221214_231229-l0d2844l/files/wandb-metadata.json
deleted file mode 100644
index f3390bc..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:12:30.150175",
-    "startedAt": "2022-12-14T22:12:29.396147",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231229-l0d2844l/files/wandb-summary.json b/wandb/run-20221214_231229-l0d2844l/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231229-l0d2844l/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231229-l0d2844l/logs/debug-internal.log b/wandb/run-20221214_231229-l0d2844l/logs/debug-internal.log
deleted file mode 100644
index 4365a63..0000000
--- a/wandb/run-20221214_231229-l0d2844l/logs/debug-internal.log
+++ /dev/null
@@ -1,63 +0,0 @@
-2022-12-14 23:12:29,453 INFO    StreamThr :6029 [internal.py:wandb_internal():87] W&B internal server running at pid: 6029, started at: 2022-12-14 23:12:29.453098
-2022-12-14 23:12:29,458 DEBUG   HandlerThread:6029 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:12:29,460 DEBUG   SenderThread:6029 [sender.py:send_request():317] send_request: status
-2022-12-14 23:12:29,464 INFO    WriterThread:6029 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/run-l0d2844l.wandb
-2022-12-14 23:12:29,464 DEBUG   SenderThread:6029 [sender.py:send():303] send: header
-2022-12-14 23:12:29,464 DEBUG   SenderThread:6029 [sender.py:send():303] send: run
-2022-12-14 23:12:29,972 INFO    SenderThread:6029 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files
-2022-12-14 23:12:29,972 INFO    SenderThread:6029 [sender.py:_start_run_threads():928] run started: l0d2844l with start time 1671055949.438856
-2022-12-14 23:12:29,972 DEBUG   SenderThread:6029 [sender.py:send():303] send: summary
-2022-12-14 23:12:29,973 INFO    SenderThread:6029 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:12:29,975 DEBUG   HandlerThread:6029 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:12:29,975 DEBUG   SenderThread:6029 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:12:30,142 DEBUG   HandlerThread:6029 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:12:30,148 DEBUG   HandlerThread:6029 [system_info.py:__init__():31] System info init
-2022-12-14 23:12:30,149 DEBUG   HandlerThread:6029 [system_info.py:__init__():46] System info init done
-2022-12-14 23:12:30,149 INFO    HandlerThread:6029 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:12:30,149 INFO    SystemMonitor:6029 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:12:30,149 INFO    HandlerThread:6029 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:12:30,150 DEBUG   HandlerThread:6029 [system_info.py:probe():195] Probing system
-2022-12-14 23:12:30,150 INFO    SystemMonitor:6029 [interfaces.py:start():168] Started cpu
-2022-12-14 23:12:30,151 INFO    SystemMonitor:6029 [interfaces.py:start():168] Started disk
-2022-12-14 23:12:30,152 INFO    SystemMonitor:6029 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:12:30,155 INFO    SystemMonitor:6029 [interfaces.py:start():168] Started memory
-2022-12-14 23:12:30,158 INFO    SystemMonitor:6029 [interfaces.py:start():168] Started network
-2022-12-14 23:12:30,161 DEBUG   HandlerThread:6029 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:12:30,176 DEBUG   HandlerThread:6029 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:12:30,177 DEBUG   HandlerThread:6029 [system_info.py:probe():241] Probing system done
-2022-12-14 23:12:30,177 DEBUG   HandlerThread:6029 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:12:30.150175', 'startedAt': '2022-12-14T22:12:29.396147', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:12:30,177 INFO    HandlerThread:6029 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:12:30,177 INFO    HandlerThread:6029 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:12:30,177 DEBUG   HandlerThread:6029 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:12:30,178 DEBUG   HandlerThread:6029 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:12:30,178 DEBUG   HandlerThread:6029 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:12:30,981 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/wandb-summary.json
-2022-12-14 23:12:30,981 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/requirements.txt
-2022-12-14 23:12:30,981 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/conda-environment.yaml
-2022-12-14 23:12:31,261 DEBUG   HandlerThread:6029 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:12:31,261 DEBUG   HandlerThread:6029 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:12:31,268 DEBUG   HandlerThread:6029 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:12:31,269 DEBUG   HandlerThread:6029 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:12:31,329 DEBUG   HandlerThread:6029 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:12:31,331 INFO    HandlerThread:6029 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:12:31,418 DEBUG   SenderThread:6029 [sender.py:send():303] send: files
-2022-12-14 23:12:31,418 INFO    SenderThread:6029 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:12:31,418 INFO    SenderThread:6029 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:12:31,419 INFO    SenderThread:6029 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:12:31,426 DEBUG   HandlerThread:6029 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:12:31,430 DEBUG   SenderThread:6029 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:12:31,755 INFO    Thread-23 :6029 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp26jjp522wandb/2cji0e9o-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:12:31,795 DEBUG   SenderThread:6029 [sender.py:send():303] send: telemetry
-2022-12-14 23:12:31,983 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/conda-environment.yaml
-2022-12-14 23:12:31,983 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/output.log
-2022-12-14 23:12:31,983 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:12:31,983 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/diff.patch
-2022-12-14 23:12:31,983 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/wandb-metadata.json
-2022-12-14 23:12:31,984 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/code
-2022-12-14 23:12:31,984 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/code/PPO/ppo_torch
-2022-12-14 23:12:31,984 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/code/PPO
-2022-12-14 23:12:32,190 INFO    Thread-22 :6029 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp26jjp522wandb/5qa98m7b-wandb-metadata.json
-2022-12-14 23:12:32,326 INFO    Thread-24 :6029 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp26jjp522wandb/32q3lthg-diff.patch
-2022-12-14 23:12:33,992 INFO    Thread-19 :6029 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/files/output.log
-2022-12-14 23:12:46,805 DEBUG   HandlerThread:6029 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:12:46,805 DEBUG   SenderThread:6029 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231229-l0d2844l/logs/debug.log b/wandb/run-20221214_231229-l0d2844l/logs/debug.log
deleted file mode 100644
index 6135d0d..0000000
--- a/wandb/run-20221214_231229-l0d2844l/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:12:29,400 INFO    MainThread:6011 [wandb_setup.py:_flush():68] Configure stats pid to 6011
-2022-12-14 23:12:29,400 INFO    MainThread:6011 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:12:29,400 INFO    MainThread:6011 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:12:29,401 INFO    MainThread:6011 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:12:29,401 INFO    MainThread:6011 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:12:29,401 INFO    MainThread:6011 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/logs/debug.log
-2022-12-14 23:12:29,401 INFO    MainThread:6011 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231229-l0d2844l/logs/debug-internal.log
-2022-12-14 23:12:29,402 INFO    MainThread:6011 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:12:29,402 INFO    MainThread:6011 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:12:29,402 INFO    MainThread:6011 [wandb_init.py:init():569] starting backend
-2022-12-14 23:12:29,402 INFO    MainThread:6011 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:12:29,432 INFO    MainThread:6011 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:12:29,438 INFO    MainThread:6011 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:12:29,445 INFO    MainThread:6011 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:12:29,457 INFO    MainThread:6011 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:12:29,973 INFO    MainThread:6011 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:12:30,107 INFO    MainThread:6011 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:12:30,108 INFO    MainThread:6011 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:12:31,425 INFO    MainThread:6011 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:12:31,426 INFO    MainThread:6011 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:12:31,426 INFO    MainThread:6011 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:12:31,426 INFO    MainThread:6011 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:12:31,427 INFO    MainThread:6011 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231229-l0d2844l/run-l0d2844l.wandb b/wandb/run-20221214_231229-l0d2844l/run-l0d2844l.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index fb54fc7..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/conda-environment.yaml b/wandb/run-20221214_231300-u4yh5zn7/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/config.yaml b/wandb/run-20221214_231300-u4yh5zn7/files/config.yaml
deleted file mode 100644
index 2b06c1a..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671055980.181918
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/diff.patch b/wandb/run-20221214_231300-u4yh5zn7/files/diff.patch
deleted file mode 100644
index a2b4168..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..fb54fc7 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/output.log b/wandb/run-20221214_231300-u4yh5zn7/files/output.log
deleted file mode 100644
index 5202b7c..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/output.log
+++ /dev/null
@@ -1,3 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/requirements.txt b/wandb/run-20221214_231300-u4yh5zn7/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/wandb-metadata.json b/wandb/run-20221214_231300-u4yh5zn7/files/wandb-metadata.json
deleted file mode 100644
index 90df2ec..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:13:00.904302",
-    "startedAt": "2022-12-14T22:13:00.139133",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231300-u4yh5zn7/files/wandb-summary.json b/wandb/run-20221214_231300-u4yh5zn7/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231300-u4yh5zn7/logs/debug-internal.log b/wandb/run-20221214_231300-u4yh5zn7/logs/debug-internal.log
deleted file mode 100644
index ff1ff0b..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/logs/debug-internal.log
+++ /dev/null
@@ -1,77 +0,0 @@
-2022-12-14 23:13:00,197 INFO    StreamThr :6108 [internal.py:wandb_internal():87] W&B internal server running at pid: 6108, started at: 2022-12-14 23:13:00.196619
-2022-12-14 23:13:00,201 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:13:00,203 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: status
-2022-12-14 23:13:00,207 INFO    WriterThread:6108 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/run-u4yh5zn7.wandb
-2022-12-14 23:13:00,207 DEBUG   SenderThread:6108 [sender.py:send():303] send: header
-2022-12-14 23:13:00,208 DEBUG   SenderThread:6108 [sender.py:send():303] send: run
-2022-12-14 23:13:00,712 INFO    SenderThread:6108 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files
-2022-12-14 23:13:00,712 INFO    SenderThread:6108 [sender.py:_start_run_threads():928] run started: u4yh5zn7 with start time 1671055980.181918
-2022-12-14 23:13:00,713 DEBUG   SenderThread:6108 [sender.py:send():303] send: summary
-2022-12-14 23:13:00,713 INFO    SenderThread:6108 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:13:00,715 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:13:00,715 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:13:00,896 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:13:00,902 DEBUG   HandlerThread:6108 [system_info.py:__init__():31] System info init
-2022-12-14 23:13:00,902 DEBUG   HandlerThread:6108 [system_info.py:__init__():46] System info init done
-2022-12-14 23:13:00,902 INFO    HandlerThread:6108 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:13:00,903 INFO    HandlerThread:6108 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:13:00,903 INFO    SystemMonitor:6108 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:13:00,903 DEBUG   HandlerThread:6108 [system_info.py:probe():195] Probing system
-2022-12-14 23:13:00,905 INFO    SystemMonitor:6108 [interfaces.py:start():168] Started cpu
-2022-12-14 23:13:00,905 INFO    SystemMonitor:6108 [interfaces.py:start():168] Started disk
-2022-12-14 23:13:00,907 INFO    SystemMonitor:6108 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:13:00,910 INFO    SystemMonitor:6108 [interfaces.py:start():168] Started memory
-2022-12-14 23:13:00,913 INFO    SystemMonitor:6108 [interfaces.py:start():168] Started network
-2022-12-14 23:13:00,915 DEBUG   HandlerThread:6108 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:13:00,931 DEBUG   HandlerThread:6108 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:13:00,931 DEBUG   HandlerThread:6108 [system_info.py:probe():241] Probing system done
-2022-12-14 23:13:00,932 DEBUG   HandlerThread:6108 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:13:00.904302', 'startedAt': '2022-12-14T22:13:00.139133', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:13:00,932 INFO    HandlerThread:6108 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:13:00,932 INFO    HandlerThread:6108 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:13:00,932 DEBUG   HandlerThread:6108 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:13:00,932 DEBUG   HandlerThread:6108 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:13:00,933 DEBUG   HandlerThread:6108 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:13:01,721 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/wandb-summary.json
-2022-12-14 23:13:01,722 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/conda-environment.yaml
-2022-12-14 23:13:01,722 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/requirements.txt
-2022-12-14 23:13:02,022 DEBUG   HandlerThread:6108 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:13:02,022 DEBUG   HandlerThread:6108 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:13:02,029 DEBUG   HandlerThread:6108 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:13:02,029 DEBUG   HandlerThread:6108 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:13:02,091 DEBUG   HandlerThread:6108 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:13:02,092 INFO    HandlerThread:6108 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:13:02,180 DEBUG   SenderThread:6108 [sender.py:send():303] send: files
-2022-12-14 23:13:02,180 INFO    SenderThread:6108 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:13:02,180 INFO    SenderThread:6108 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:13:02,181 INFO    SenderThread:6108 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:13:02,191 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:13:02,193 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:13:02,492 DEBUG   SenderThread:6108 [sender.py:send():303] send: telemetry
-2022-12-14 23:13:02,515 INFO    Thread-23 :6108 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc6eiwk18wandb/mewurs0y-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:13:02,724 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/conda-environment.yaml
-2022-12-14 23:13:02,724 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:13:02,724 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/output.log
-2022-12-14 23:13:02,724 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/diff.patch
-2022-12-14 23:13:02,725 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/wandb-metadata.json
-2022-12-14 23:13:02,725 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO/ppo_torch
-2022-12-14 23:13:02,725 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/code
-2022-12-14 23:13:02,725 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/code/PPO
-2022-12-14 23:13:03,274 INFO    Thread-22 :6108 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc6eiwk18wandb/177nltaz-wandb-metadata.json
-2022-12-14 23:13:03,275 INFO    Thread-24 :6108 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpc6eiwk18wandb/1f6tid1o-diff.patch
-2022-12-14 23:13:04,733 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/output.log
-2022-12-14 23:13:17,539 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:13:17,540 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:13:31,893 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/config.yaml
-2022-12-14 23:13:32,786 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:13:32,786 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:13:48,076 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:13:48,077 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:14:00,917 DEBUG   SystemMonitor:6108 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:14:00,923 DEBUG   SenderThread:6108 [sender.py:send():303] send: telemetry
-2022-12-14 23:14:00,924 DEBUG   SenderThread:6108 [sender.py:send():303] send: stats
-2022-12-14 23:14:03,065 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/config.yaml
-2022-12-14 23:14:03,066 INFO    Thread-19 :6108 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/files/output.log
-2022-12-14 23:14:03,344 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:14:03,345 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:14:18,618 DEBUG   HandlerThread:6108 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:14:18,619 DEBUG   SenderThread:6108 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231300-u4yh5zn7/logs/debug.log b/wandb/run-20221214_231300-u4yh5zn7/logs/debug.log
deleted file mode 100644
index a1d2916..0000000
--- a/wandb/run-20221214_231300-u4yh5zn7/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:13:00,143 INFO    MainThread:6098 [wandb_setup.py:_flush():68] Configure stats pid to 6098
-2022-12-14 23:13:00,143 INFO    MainThread:6098 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:13:00,143 INFO    MainThread:6098 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:13:00,143 INFO    MainThread:6098 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:13:00,144 INFO    MainThread:6098 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:13:00,144 INFO    MainThread:6098 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/logs/debug.log
-2022-12-14 23:13:00,144 INFO    MainThread:6098 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231300-u4yh5zn7/logs/debug-internal.log
-2022-12-14 23:13:00,144 INFO    MainThread:6098 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:13:00,145 INFO    MainThread:6098 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:13:00,145 INFO    MainThread:6098 [wandb_init.py:init():569] starting backend
-2022-12-14 23:13:00,145 INFO    MainThread:6098 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:13:00,175 INFO    MainThread:6098 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:13:00,181 INFO    MainThread:6098 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:13:00,188 INFO    MainThread:6098 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:13:00,200 INFO    MainThread:6098 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:13:00,713 INFO    MainThread:6098 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:13:00,864 INFO    MainThread:6098 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:13:00,864 INFO    MainThread:6098 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:13:02,187 INFO    MainThread:6098 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:13:02,188 INFO    MainThread:6098 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:13:02,188 INFO    MainThread:6098 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:13:02,188 INFO    MainThread:6098 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:13:02,189 INFO    MainThread:6098 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231300-u4yh5zn7/run-u4yh5zn7.wandb b/wandb/run-20221214_231300-u4yh5zn7/run-u4yh5zn7.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231433-t7ec8drr/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 1901085..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/conda-environment.yaml b/wandb/run-20221214_231433-t7ec8drr/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/config.yaml b/wandb/run-20221214_231433-t7ec8drr/files/config.yaml
deleted file mode 100644
index 7e79ec5..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/config.yaml
+++ /dev/null
@@ -1,61 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056073.522508
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/diff.patch b/wandb/run-20221214_231433-t7ec8drr/files/diff.patch
deleted file mode 100644
index 7abe44b..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..1901085 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/output.log b/wandb/run-20221214_231433-t7ec8drr/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/requirements.txt b/wandb/run-20221214_231433-t7ec8drr/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/wandb-metadata.json b/wandb/run-20221214_231433-t7ec8drr/files/wandb-metadata.json
deleted file mode 100644
index 0b6a022..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:14:34.260250",
-    "startedAt": "2022-12-14T22:14:33.491113",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231433-t7ec8drr/files/wandb-summary.json b/wandb/run-20221214_231433-t7ec8drr/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231433-t7ec8drr/logs/debug-internal.log b/wandb/run-20221214_231433-t7ec8drr/logs/debug-internal.log
deleted file mode 100644
index e2838eb..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/logs/debug-internal.log
+++ /dev/null
@@ -1,66 +0,0 @@
-2022-12-14 23:14:33,536 INFO    StreamThr :6203 [internal.py:wandb_internal():87] W&B internal server running at pid: 6203, started at: 2022-12-14 23:14:33.535254
-2022-12-14 23:14:33,539 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:14:33,542 DEBUG   SenderThread:6203 [sender.py:send_request():317] send_request: status
-2022-12-14 23:14:33,545 DEBUG   SenderThread:6203 [sender.py:send():303] send: header
-2022-12-14 23:14:33,546 DEBUG   SenderThread:6203 [sender.py:send():303] send: run
-2022-12-14 23:14:33,551 INFO    WriterThread:6203 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/run-t7ec8drr.wandb
-2022-12-14 23:14:34,072 INFO    SenderThread:6203 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files
-2022-12-14 23:14:34,073 INFO    SenderThread:6203 [sender.py:_start_run_threads():928] run started: t7ec8drr with start time 1671056073.522508
-2022-12-14 23:14:34,073 DEBUG   SenderThread:6203 [sender.py:send():303] send: summary
-2022-12-14 23:14:34,073 INFO    SenderThread:6203 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:14:34,076 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:14:34,076 DEBUG   SenderThread:6203 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:14:34,251 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:14:34,258 DEBUG   HandlerThread:6203 [system_info.py:__init__():31] System info init
-2022-12-14 23:14:34,258 DEBUG   HandlerThread:6203 [system_info.py:__init__():46] System info init done
-2022-12-14 23:14:34,258 INFO    HandlerThread:6203 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:14:34,258 INFO    SystemMonitor:6203 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:14:34,259 INFO    HandlerThread:6203 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:14:34,259 INFO    SystemMonitor:6203 [interfaces.py:start():168] Started cpu
-2022-12-14 23:14:34,260 DEBUG   HandlerThread:6203 [system_info.py:probe():195] Probing system
-2022-12-14 23:14:34,260 INFO    SystemMonitor:6203 [interfaces.py:start():168] Started disk
-2022-12-14 23:14:34,260 INFO    SystemMonitor:6203 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:14:34,266 INFO    SystemMonitor:6203 [interfaces.py:start():168] Started memory
-2022-12-14 23:14:34,269 INFO    SystemMonitor:6203 [interfaces.py:start():168] Started network
-2022-12-14 23:14:34,270 DEBUG   HandlerThread:6203 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:14:34,289 DEBUG   HandlerThread:6203 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:14:34,290 DEBUG   HandlerThread:6203 [system_info.py:probe():241] Probing system done
-2022-12-14 23:14:34,290 DEBUG   HandlerThread:6203 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:14:34.260250', 'startedAt': '2022-12-14T22:14:33.491113', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:14:34,290 INFO    HandlerThread:6203 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:14:34,290 INFO    HandlerThread:6203 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:14:34,290 DEBUG   HandlerThread:6203 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:14:34,291 DEBUG   HandlerThread:6203 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:14:34,291 DEBUG   HandlerThread:6203 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:14:35,081 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/requirements.txt
-2022-12-14 23:14:35,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/conda-environment.yaml
-2022-12-14 23:14:35,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/wandb-summary.json
-2022-12-14 23:14:35,392 DEBUG   HandlerThread:6203 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:14:35,392 DEBUG   HandlerThread:6203 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:14:35,399 DEBUG   HandlerThread:6203 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:14:35,399 DEBUG   HandlerThread:6203 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:14:35,457 DEBUG   HandlerThread:6203 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:14:35,458 INFO    HandlerThread:6203 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:14:35,546 DEBUG   SenderThread:6203 [sender.py:send():303] send: files
-2022-12-14 23:14:35,546 INFO    SenderThread:6203 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:14:35,546 INFO    SenderThread:6203 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:14:35,546 INFO    SenderThread:6203 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:14:35,554 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:14:35,554 DEBUG   SenderThread:6203 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:14:35,928 DEBUG   SenderThread:6203 [sender.py:send():303] send: telemetry
-2022-12-14 23:14:35,946 INFO    Thread-23 :6203 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpbrks6d7ywandb/2p9ibt8o-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:14:36,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/conda-environment.yaml
-2022-12-14 23:14:36,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/output.log
-2022-12-14 23:14:36,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/wandb-metadata.json
-2022-12-14 23:14:36,082 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:14:36,083 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/diff.patch
-2022-12-14 23:14:36,083 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/code
-2022-12-14 23:14:36,083 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/code/PPO
-2022-12-14 23:14:36,083 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/code/PPO/ppo_torch
-2022-12-14 23:14:36,385 INFO    Thread-22 :6203 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpbrks6d7ywandb/bjjxms6f-wandb-metadata.json
-2022-12-14 23:14:36,441 INFO    Thread-24 :6203 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpbrks6d7ywandb/25yneuv9-diff.patch
-2022-12-14 23:14:38,096 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/output.log
-2022-12-14 23:14:50,938 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:14:50,939 DEBUG   SenderThread:6203 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:15:05,222 INFO    Thread-19 :6203 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/files/config.yaml
-2022-12-14 23:15:06,189 DEBUG   HandlerThread:6203 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:15:06,190 DEBUG   SenderThread:6203 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231433-t7ec8drr/logs/debug.log b/wandb/run-20221214_231433-t7ec8drr/logs/debug.log
deleted file mode 100644
index c4c2c35..0000000
--- a/wandb/run-20221214_231433-t7ec8drr/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:14:33,493 INFO    MainThread:6191 [wandb_setup.py:_flush():68] Configure stats pid to 6191
-2022-12-14 23:14:33,493 INFO    MainThread:6191 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:14:33,493 INFO    MainThread:6191 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:14:33,493 INFO    MainThread:6191 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:14:33,493 INFO    MainThread:6191 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/logs/debug.log
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231433-t7ec8drr/logs/debug-internal.log
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:init():569] starting backend
-2022-12-14 23:14:33,494 INFO    MainThread:6191 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:14:33,516 INFO    MainThread:6191 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:14:33,522 INFO    MainThread:6191 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:14:33,528 INFO    MainThread:6191 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:14:33,539 INFO    MainThread:6191 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:14:34,074 INFO    MainThread:6191 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:14:34,217 INFO    MainThread:6191 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:14:34,217 INFO    MainThread:6191 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:14:35,553 INFO    MainThread:6191 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:14:35,553 INFO    MainThread:6191 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:14:35,554 INFO    MainThread:6191 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:14:35,554 INFO    MainThread:6191 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:14:35,555 INFO    MainThread:6191 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231433-t7ec8drr/run-t7ec8drr.wandb b/wandb/run-20221214_231433-t7ec8drr/run-t7ec8drr.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231510-1tdji7gs/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 1901085..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/conda-environment.yaml b/wandb/run-20221214_231510-1tdji7gs/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/config.yaml b/wandb/run-20221214_231510-1tdji7gs/files/config.yaml
deleted file mode 100644
index 74a0870..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/config.yaml
+++ /dev/null
@@ -1,61 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056110.248621
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/diff.patch b/wandb/run-20221214_231510-1tdji7gs/files/diff.patch
deleted file mode 100644
index 7abe44b..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..1901085 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/output.log b/wandb/run-20221214_231510-1tdji7gs/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/requirements.txt b/wandb/run-20221214_231510-1tdji7gs/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/wandb-metadata.json b/wandb/run-20221214_231510-1tdji7gs/files/wandb-metadata.json
deleted file mode 100644
index 4851127..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:15:10.975468",
-    "startedAt": "2022-12-14T22:15:10.205970",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231510-1tdji7gs/files/wandb-summary.json b/wandb/run-20221214_231510-1tdji7gs/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231510-1tdji7gs/logs/debug-internal.log b/wandb/run-20221214_231510-1tdji7gs/logs/debug-internal.log
deleted file mode 100644
index 6f58839..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/logs/debug-internal.log
+++ /dev/null
@@ -1,66 +0,0 @@
-2022-12-14 23:15:10,263 INFO    StreamThr :6280 [internal.py:wandb_internal():87] W&B internal server running at pid: 6280, started at: 2022-12-14 23:15:10.261517
-2022-12-14 23:15:10,267 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:15:10,269 DEBUG   SenderThread:6280 [sender.py:send_request():317] send_request: status
-2022-12-14 23:15:10,273 DEBUG   SenderThread:6280 [sender.py:send():303] send: header
-2022-12-14 23:15:10,274 DEBUG   SenderThread:6280 [sender.py:send():303] send: run
-2022-12-14 23:15:10,279 INFO    WriterThread:6280 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/run-1tdji7gs.wandb
-2022-12-14 23:15:10,800 INFO    SenderThread:6280 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files
-2022-12-14 23:15:10,801 INFO    SenderThread:6280 [sender.py:_start_run_threads():928] run started: 1tdji7gs with start time 1671056110.248621
-2022-12-14 23:15:10,801 DEBUG   SenderThread:6280 [sender.py:send():303] send: summary
-2022-12-14 23:15:10,801 INFO    SenderThread:6280 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:15:10,803 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:15:10,804 DEBUG   SenderThread:6280 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:15:10,968 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:15:10,974 DEBUG   HandlerThread:6280 [system_info.py:__init__():31] System info init
-2022-12-14 23:15:10,974 DEBUG   HandlerThread:6280 [system_info.py:__init__():46] System info init done
-2022-12-14 23:15:10,974 INFO    HandlerThread:6280 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:15:10,975 INFO    HandlerThread:6280 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:15:10,975 DEBUG   HandlerThread:6280 [system_info.py:probe():195] Probing system
-2022-12-14 23:15:10,975 INFO    SystemMonitor:6280 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:15:10,976 INFO    SystemMonitor:6280 [interfaces.py:start():168] Started cpu
-2022-12-14 23:15:10,979 INFO    SystemMonitor:6280 [interfaces.py:start():168] Started disk
-2022-12-14 23:15:10,981 INFO    SystemMonitor:6280 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:15:10,985 INFO    SystemMonitor:6280 [interfaces.py:start():168] Started memory
-2022-12-14 23:15:10,987 DEBUG   HandlerThread:6280 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:15:10,989 INFO    SystemMonitor:6280 [interfaces.py:start():168] Started network
-2022-12-14 23:15:11,003 DEBUG   HandlerThread:6280 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:15:11,004 DEBUG   HandlerThread:6280 [system_info.py:probe():241] Probing system done
-2022-12-14 23:15:11,004 DEBUG   HandlerThread:6280 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:15:10.975468', 'startedAt': '2022-12-14T22:15:10.205970', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:15:11,004 INFO    HandlerThread:6280 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:15:11,004 INFO    HandlerThread:6280 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:15:11,004 DEBUG   HandlerThread:6280 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:15:11,005 DEBUG   HandlerThread:6280 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:15:11,005 DEBUG   HandlerThread:6280 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:15:11,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/requirements.txt
-2022-12-14 23:15:11,810 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/conda-environment.yaml
-2022-12-14 23:15:11,810 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/wandb-summary.json
-2022-12-14 23:15:12,071 DEBUG   HandlerThread:6280 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:15:12,071 DEBUG   HandlerThread:6280 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:15:12,078 DEBUG   HandlerThread:6280 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:15:12,078 DEBUG   HandlerThread:6280 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:15:12,138 DEBUG   HandlerThread:6280 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:15:12,140 INFO    HandlerThread:6280 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:15:12,230 DEBUG   SenderThread:6280 [sender.py:send():303] send: files
-2022-12-14 23:15:12,230 INFO    SenderThread:6280 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:15:12,230 INFO    SenderThread:6280 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:15:12,231 INFO    SenderThread:6280 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:15:12,240 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:15:12,242 DEBUG   SenderThread:6280 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:15:12,729 DEBUG   SenderThread:6280 [sender.py:send():303] send: telemetry
-2022-12-14 23:15:12,743 INFO    Thread-23 :6280 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpllruy8n7wandb/2uiafak4-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:15:12,808 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/conda-environment.yaml
-2022-12-14 23:15:12,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/output.log
-2022-12-14 23:15:12,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/wandb-metadata.json
-2022-12-14 23:15:12,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/diff.patch
-2022-12-14 23:15:12,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:15:12,809 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/code
-2022-12-14 23:15:12,810 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/code/PPO
-2022-12-14 23:15:12,810 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/code/PPO/ppo_torch
-2022-12-14 23:15:13,015 INFO    Thread-22 :6280 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpllruy8n7wandb/1lee2ljm-wandb-metadata.json
-2022-12-14 23:15:13,202 INFO    Thread-24 :6280 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpllruy8n7wandb/2864z9kh-diff.patch
-2022-12-14 23:15:14,821 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/output.log
-2022-12-14 23:15:27,735 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:15:27,736 DEBUG   SenderThread:6280 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:15:41,952 INFO    Thread-19 :6280 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/files/config.yaml
-2022-12-14 23:15:42,970 DEBUG   HandlerThread:6280 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:15:42,970 DEBUG   SenderThread:6280 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231510-1tdji7gs/logs/debug.log b/wandb/run-20221214_231510-1tdji7gs/logs/debug.log
deleted file mode 100644
index d828575..0000000
--- a/wandb/run-20221214_231510-1tdji7gs/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_setup.py:_flush():68] Configure stats pid to 6265
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/logs/debug.log
-2022-12-14 23:15:10,211 INFO    MainThread:6265 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231510-1tdji7gs/logs/debug-internal.log
-2022-12-14 23:15:10,212 INFO    MainThread:6265 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:15:10,212 INFO    MainThread:6265 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:15:10,212 INFO    MainThread:6265 [wandb_init.py:init():569] starting backend
-2022-12-14 23:15:10,213 INFO    MainThread:6265 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:15:10,242 INFO    MainThread:6265 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:15:10,248 INFO    MainThread:6265 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:15:10,255 INFO    MainThread:6265 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:15:10,267 INFO    MainThread:6265 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:15:10,802 INFO    MainThread:6265 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:15:10,934 INFO    MainThread:6265 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:15:10,934 INFO    MainThread:6265 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:15:12,237 INFO    MainThread:6265 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:15:12,238 INFO    MainThread:6265 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:15:12,238 INFO    MainThread:6265 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:15:12,238 INFO    MainThread:6265 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:15:12,239 INFO    MainThread:6265 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231510-1tdji7gs/run-1tdji7gs.wandb b/wandb/run-20221214_231510-1tdji7gs/run-1tdji7gs.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 1901085..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/conda-environment.yaml b/wandb/run-20221214_231546-2bwgl6rf/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/config.yaml b/wandb/run-20221214_231546-2bwgl6rf/files/config.yaml
deleted file mode 100644
index a86d8e7..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056146.618165
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/diff.patch b/wandb/run-20221214_231546-2bwgl6rf/files/diff.patch
deleted file mode 100644
index 7abe44b..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..1901085 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(trajectory_rewards, trajectory_values)
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/output.log b/wandb/run-20221214_231546-2bwgl6rf/files/output.log
deleted file mode 100644
index 5202b7c..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/output.log
+++ /dev/null
@@ -1,3 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/requirements.txt b/wandb/run-20221214_231546-2bwgl6rf/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/wandb-metadata.json b/wandb/run-20221214_231546-2bwgl6rf/files/wandb-metadata.json
deleted file mode 100644
index f9eb262..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:15:47.352529",
-    "startedAt": "2022-12-14T22:15:46.582329",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231546-2bwgl6rf/files/wandb-summary.json b/wandb/run-20221214_231546-2bwgl6rf/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231546-2bwgl6rf/logs/debug-internal.log b/wandb/run-20221214_231546-2bwgl6rf/logs/debug-internal.log
deleted file mode 100644
index 68c9a83..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/logs/debug-internal.log
+++ /dev/null
@@ -1,77 +0,0 @@
-2022-12-14 23:15:46,632 INFO    StreamThr :6356 [internal.py:wandb_internal():87] W&B internal server running at pid: 6356, started at: 2022-12-14 23:15:46.630568
-2022-12-14 23:15:46,635 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:15:46,638 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: status
-2022-12-14 23:15:46,642 DEBUG   SenderThread:6356 [sender.py:send():303] send: header
-2022-12-14 23:15:46,642 INFO    WriterThread:6356 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/run-2bwgl6rf.wandb
-2022-12-14 23:15:46,643 DEBUG   SenderThread:6356 [sender.py:send():303] send: run
-2022-12-14 23:15:47,151 INFO    SenderThread:6356 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files
-2022-12-14 23:15:47,152 INFO    SenderThread:6356 [sender.py:_start_run_threads():928] run started: 2bwgl6rf with start time 1671056146.618165
-2022-12-14 23:15:47,152 DEBUG   SenderThread:6356 [sender.py:send():303] send: summary
-2022-12-14 23:15:47,152 INFO    SenderThread:6356 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:15:47,154 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:15:47,154 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:15:47,345 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:15:47,350 DEBUG   HandlerThread:6356 [system_info.py:__init__():31] System info init
-2022-12-14 23:15:47,351 DEBUG   HandlerThread:6356 [system_info.py:__init__():46] System info init done
-2022-12-14 23:15:47,351 INFO    HandlerThread:6356 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:15:47,351 INFO    SystemMonitor:6356 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:15:47,352 INFO    HandlerThread:6356 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:15:47,352 DEBUG   HandlerThread:6356 [system_info.py:probe():195] Probing system
-2022-12-14 23:15:47,353 INFO    SystemMonitor:6356 [interfaces.py:start():168] Started cpu
-2022-12-14 23:15:47,354 INFO    SystemMonitor:6356 [interfaces.py:start():168] Started disk
-2022-12-14 23:15:47,354 INFO    SystemMonitor:6356 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:15:47,358 INFO    SystemMonitor:6356 [interfaces.py:start():168] Started memory
-2022-12-14 23:15:47,360 INFO    SystemMonitor:6356 [interfaces.py:start():168] Started network
-2022-12-14 23:15:47,362 DEBUG   HandlerThread:6356 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:15:47,377 DEBUG   HandlerThread:6356 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:15:47,377 DEBUG   HandlerThread:6356 [system_info.py:probe():241] Probing system done
-2022-12-14 23:15:47,377 DEBUG   HandlerThread:6356 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:15:47.352529', 'startedAt': '2022-12-14T22:15:46.582329', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:15:47,377 INFO    HandlerThread:6356 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:15:47,378 INFO    HandlerThread:6356 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:15:47,378 DEBUG   HandlerThread:6356 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:15:47,378 DEBUG   HandlerThread:6356 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:15:47,378 DEBUG   HandlerThread:6356 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:15:48,161 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/requirements.txt
-2022-12-14 23:15:48,161 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/wandb-summary.json
-2022-12-14 23:15:48,161 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/conda-environment.yaml
-2022-12-14 23:15:48,461 DEBUG   HandlerThread:6356 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:15:48,462 DEBUG   HandlerThread:6356 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:15:48,469 DEBUG   HandlerThread:6356 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:15:48,469 DEBUG   HandlerThread:6356 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:15:48,531 DEBUG   HandlerThread:6356 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:15:48,532 INFO    HandlerThread:6356 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:15:48,620 DEBUG   SenderThread:6356 [sender.py:send():303] send: files
-2022-12-14 23:15:48,620 INFO    SenderThread:6356 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:15:48,621 INFO    SenderThread:6356 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:15:48,621 INFO    SenderThread:6356 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:15:48,627 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:15:48,627 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:15:48,914 DEBUG   SenderThread:6356 [sender.py:send():303] send: telemetry
-2022-12-14 23:15:48,978 INFO    Thread-23 :6356 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfn25abwkwandb/2ic4u327-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:15:49,162 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/conda-environment.yaml
-2022-12-14 23:15:49,162 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/output.log
-2022-12-14 23:15:49,162 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:15:49,163 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/diff.patch
-2022-12-14 23:15:49,163 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/wandb-metadata.json
-2022-12-14 23:15:49,163 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO/ppo_torch
-2022-12-14 23:15:49,163 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/code/PPO
-2022-12-14 23:15:49,163 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/code
-2022-12-14 23:15:49,314 INFO    Thread-22 :6356 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfn25abwkwandb/qryodz45-wandb-metadata.json
-2022-12-14 23:15:49,474 INFO    Thread-24 :6356 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfn25abwkwandb/1i8e7ro6-diff.patch
-2022-12-14 23:15:51,176 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/output.log
-2022-12-14 23:16:03,930 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:16:03,931 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:16:18,332 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/config.yaml
-2022-12-14 23:16:19,179 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:16:19,179 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:16:34,460 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:16:34,461 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:16:47,364 DEBUG   SystemMonitor:6356 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:16:47,368 DEBUG   SenderThread:6356 [sender.py:send():303] send: telemetry
-2022-12-14 23:16:47,369 DEBUG   SenderThread:6356 [sender.py:send():303] send: stats
-2022-12-14 23:16:49,497 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/output.log
-2022-12-14 23:16:49,498 INFO    Thread-19 :6356 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/files/config.yaml
-2022-12-14 23:16:49,708 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:16:49,708 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:17:04,975 DEBUG   HandlerThread:6356 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:17:04,975 DEBUG   SenderThread:6356 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231546-2bwgl6rf/logs/debug.log b/wandb/run-20221214_231546-2bwgl6rf/logs/debug.log
deleted file mode 100644
index d7e7281..0000000
--- a/wandb/run-20221214_231546-2bwgl6rf/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:15:46,585 INFO    MainThread:6346 [wandb_setup.py:_flush():68] Configure stats pid to 6346
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/logs/debug.log
-2022-12-14 23:15:46,586 INFO    MainThread:6346 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231546-2bwgl6rf/logs/debug-internal.log
-2022-12-14 23:15:46,587 INFO    MainThread:6346 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:15:46,587 INFO    MainThread:6346 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:15:46,587 INFO    MainThread:6346 [wandb_init.py:init():569] starting backend
-2022-12-14 23:15:46,587 INFO    MainThread:6346 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:15:46,612 INFO    MainThread:6346 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:15:46,617 INFO    MainThread:6346 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:15:46,624 INFO    MainThread:6346 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:15:46,636 INFO    MainThread:6346 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:15:47,153 INFO    MainThread:6346 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:15:47,313 INFO    MainThread:6346 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:15:47,313 INFO    MainThread:6346 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:15:48,627 INFO    MainThread:6346 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:15:48,627 INFO    MainThread:6346 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:15:48,627 INFO    MainThread:6346 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:15:48,627 INFO    MainThread:6346 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:15:48,628 INFO    MainThread:6346 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231546-2bwgl6rf/run-2bwgl6rf.wandb b/wandb/run-20221214_231546-2bwgl6rf/run-2bwgl6rf.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231708-32q98p5j/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231708-32q98p5j/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index af6a9be..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards, dtype=torch.float), np.array(trajectory_values, dtype=torch.float))
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231708-32q98p5j/files/conda-environment.yaml b/wandb/run-20221214_231708-32q98p5j/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231708-32q98p5j/files/config.yaml b/wandb/run-20221214_231708-32q98p5j/files/config.yaml
deleted file mode 100644
index cd6de37..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/config.yaml
+++ /dev/null
@@ -1,58 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056228.651404
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231708-32q98p5j/files/diff.patch b/wandb/run-20221214_231708-32q98p5j/files/diff.patch
deleted file mode 100644
index d5688f1..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..af6a9be 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards, dtype=torch.float), np.array(trajectory_values, dtype=torch.float))
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231708-32q98p5j/files/output.log b/wandb/run-20221214_231708-32q98p5j/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_231708-32q98p5j/files/requirements.txt b/wandb/run-20221214_231708-32q98p5j/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231708-32q98p5j/files/wandb-metadata.json b/wandb/run-20221214_231708-32q98p5j/files/wandb-metadata.json
deleted file mode 100644
index 8a7918e..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:17:09.368795",
-    "startedAt": "2022-12-14T22:17:08.609734",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231708-32q98p5j/files/wandb-summary.json b/wandb/run-20221214_231708-32q98p5j/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231708-32q98p5j/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231708-32q98p5j/logs/debug-internal.log b/wandb/run-20221214_231708-32q98p5j/logs/debug-internal.log
deleted file mode 100644
index d924ed5..0000000
--- a/wandb/run-20221214_231708-32q98p5j/logs/debug-internal.log
+++ /dev/null
@@ -1,63 +0,0 @@
-2022-12-14 23:17:08,666 INFO    StreamThr :6471 [internal.py:wandb_internal():87] W&B internal server running at pid: 6471, started at: 2022-12-14 23:17:08.665371
-2022-12-14 23:17:08,670 DEBUG   HandlerThread:6471 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:17:08,673 DEBUG   SenderThread:6471 [sender.py:send_request():317] send_request: status
-2022-12-14 23:17:08,676 INFO    WriterThread:6471 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/run-32q98p5j.wandb
-2022-12-14 23:17:08,677 DEBUG   SenderThread:6471 [sender.py:send():303] send: header
-2022-12-14 23:17:08,677 DEBUG   SenderThread:6471 [sender.py:send():303] send: run
-2022-12-14 23:17:09,190 INFO    SenderThread:6471 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files
-2022-12-14 23:17:09,190 INFO    SenderThread:6471 [sender.py:_start_run_threads():928] run started: 32q98p5j with start time 1671056228.651404
-2022-12-14 23:17:09,190 DEBUG   SenderThread:6471 [sender.py:send():303] send: summary
-2022-12-14 23:17:09,191 INFO    SenderThread:6471 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:17:09,194 DEBUG   HandlerThread:6471 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:17:09,194 DEBUG   SenderThread:6471 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:17:09,358 DEBUG   HandlerThread:6471 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:17:09,366 DEBUG   HandlerThread:6471 [system_info.py:__init__():31] System info init
-2022-12-14 23:17:09,367 DEBUG   HandlerThread:6471 [system_info.py:__init__():46] System info init done
-2022-12-14 23:17:09,367 INFO    HandlerThread:6471 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:17:09,368 INFO    HandlerThread:6471 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:17:09,368 DEBUG   HandlerThread:6471 [system_info.py:probe():195] Probing system
-2022-12-14 23:17:09,371 INFO    SystemMonitor:6471 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:17:09,374 INFO    SystemMonitor:6471 [interfaces.py:start():168] Started cpu
-2022-12-14 23:17:09,376 DEBUG   HandlerThread:6471 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:17:09,378 INFO    SystemMonitor:6471 [interfaces.py:start():168] Started disk
-2022-12-14 23:17:09,378 INFO    SystemMonitor:6471 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:17:09,387 INFO    SystemMonitor:6471 [interfaces.py:start():168] Started memory
-2022-12-14 23:17:09,391 INFO    SystemMonitor:6471 [interfaces.py:start():168] Started network
-2022-12-14 23:17:09,401 DEBUG   HandlerThread:6471 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:17:09,401 DEBUG   HandlerThread:6471 [system_info.py:probe():241] Probing system done
-2022-12-14 23:17:09,402 DEBUG   HandlerThread:6471 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:17:09.368795', 'startedAt': '2022-12-14T22:17:08.609734', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:17:09,402 INFO    HandlerThread:6471 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:17:09,402 INFO    HandlerThread:6471 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:17:09,402 DEBUG   HandlerThread:6471 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:17:09,403 DEBUG   HandlerThread:6471 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:17:09,403 DEBUG   HandlerThread:6471 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:17:10,196 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/requirements.txt
-2022-12-14 23:17:10,197 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/conda-environment.yaml
-2022-12-14 23:17:10,197 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/wandb-summary.json
-2022-12-14 23:17:10,477 DEBUG   HandlerThread:6471 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:17:10,477 DEBUG   HandlerThread:6471 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:17:10,485 DEBUG   HandlerThread:6471 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:17:10,485 DEBUG   HandlerThread:6471 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:17:10,548 DEBUG   HandlerThread:6471 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:17:10,549 INFO    HandlerThread:6471 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:17:10,637 DEBUG   SenderThread:6471 [sender.py:send():303] send: files
-2022-12-14 23:17:10,637 INFO    SenderThread:6471 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:17:10,638 INFO    SenderThread:6471 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:17:10,638 INFO    SenderThread:6471 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:17:10,652 DEBUG   HandlerThread:6471 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:17:10,653 DEBUG   SenderThread:6471 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:17:11,004 INFO    Thread-23 :6471 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp2z9ppw_pwandb/d5jm56vs-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:17:11,011 DEBUG   SenderThread:6471 [sender.py:send():303] send: telemetry
-2022-12-14 23:17:11,194 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/conda-environment.yaml
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/output.log
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/wandb-metadata.json
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/diff.patch
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/code/PPO/ppo_torch
-2022-12-14 23:17:11,195 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/code
-2022-12-14 23:17:11,196 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/code/PPO
-2022-12-14 23:17:11,490 INFO    Thread-22 :6471 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp2z9ppw_pwandb/2yhug597-wandb-metadata.json
-2022-12-14 23:17:11,635 INFO    Thread-24 :6471 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp2z9ppw_pwandb/sv4diuzg-diff.patch
-2022-12-14 23:17:13,207 INFO    Thread-19 :6471 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/files/output.log
-2022-12-14 23:17:26,021 DEBUG   HandlerThread:6471 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:17:26,022 DEBUG   SenderThread:6471 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231708-32q98p5j/logs/debug.log b/wandb/run-20221214_231708-32q98p5j/logs/debug.log
deleted file mode 100644
index 03dd02a..0000000
--- a/wandb/run-20221214_231708-32q98p5j/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:17:08,614 INFO    MainThread:6455 [wandb_setup.py:_flush():68] Configure stats pid to 6455
-2022-12-14 23:17:08,614 INFO    MainThread:6455 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:17:08,614 INFO    MainThread:6455 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:17:08,614 INFO    MainThread:6455 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:17:08,614 INFO    MainThread:6455 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:17:08,615 INFO    MainThread:6455 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/logs/debug.log
-2022-12-14 23:17:08,615 INFO    MainThread:6455 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231708-32q98p5j/logs/debug-internal.log
-2022-12-14 23:17:08,615 INFO    MainThread:6455 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:17:08,616 INFO    MainThread:6455 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:17:08,616 INFO    MainThread:6455 [wandb_init.py:init():569] starting backend
-2022-12-14 23:17:08,616 INFO    MainThread:6455 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:17:08,645 INFO    MainThread:6455 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:17:08,651 INFO    MainThread:6455 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:17:08,658 INFO    MainThread:6455 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:17:08,670 INFO    MainThread:6455 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:17:09,192 INFO    MainThread:6455 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:17:09,323 INFO    MainThread:6455 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:17:09,324 INFO    MainThread:6455 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:17:10,645 INFO    MainThread:6455 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:17:10,645 INFO    MainThread:6455 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:17:10,645 INFO    MainThread:6455 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:17:10,646 INFO    MainThread:6455 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:17:10,646 INFO    MainThread:6455 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231708-32q98p5j/run-32q98p5j.wandb b/wandb/run-20221214_231708-32q98p5j/run-32q98p5j.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231733-h9z9voos/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231733-h9z9voos/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 2caebd6..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-                    trajectory_advantages.append(advantage)
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231733-h9z9voos/files/conda-environment.yaml b/wandb/run-20221214_231733-h9z9voos/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231733-h9z9voos/files/config.yaml b/wandb/run-20221214_231733-h9z9voos/files/config.yaml
deleted file mode 100644
index f35b2a1..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/config.yaml
+++ /dev/null
@@ -1,61 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056254.040938
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231733-h9z9voos/files/diff.patch b/wandb/run-20221214_231733-h9z9voos/files/diff.patch
deleted file mode 100644
index 43a23a1..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..2caebd6 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-+                    trajectory_advantages.append(advantage)
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231733-h9z9voos/files/output.log b/wandb/run-20221214_231733-h9z9voos/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_231733-h9z9voos/files/requirements.txt b/wandb/run-20221214_231733-h9z9voos/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231733-h9z9voos/files/wandb-metadata.json b/wandb/run-20221214_231733-h9z9voos/files/wandb-metadata.json
deleted file mode 100644
index ad64285..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:17:34.772069",
-    "startedAt": "2022-12-14T22:17:33.997211",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231733-h9z9voos/files/wandb-summary.json b/wandb/run-20221214_231733-h9z9voos/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_231733-h9z9voos/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_231733-h9z9voos/logs/debug-internal.log b/wandb/run-20221214_231733-h9z9voos/logs/debug-internal.log
deleted file mode 100644
index 0ef9fc8..0000000
--- a/wandb/run-20221214_231733-h9z9voos/logs/debug-internal.log
+++ /dev/null
@@ -1,68 +0,0 @@
-2022-12-14 23:17:34,056 INFO    StreamThr :6546 [internal.py:wandb_internal():87] W&B internal server running at pid: 6546, started at: 2022-12-14 23:17:34.054824
-2022-12-14 23:17:34,059 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:17:34,061 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: status
-2022-12-14 23:17:34,066 INFO    WriterThread:6546 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/run-h9z9voos.wandb
-2022-12-14 23:17:34,066 DEBUG   SenderThread:6546 [sender.py:send():303] send: header
-2022-12-14 23:17:34,066 DEBUG   SenderThread:6546 [sender.py:send():303] send: run
-2022-12-14 23:17:34,591 INFO    SenderThread:6546 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files
-2022-12-14 23:17:34,591 INFO    SenderThread:6546 [sender.py:_start_run_threads():928] run started: h9z9voos with start time 1671056254.040938
-2022-12-14 23:17:34,591 DEBUG   SenderThread:6546 [sender.py:send():303] send: summary
-2022-12-14 23:17:34,592 INFO    SenderThread:6546 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:17:34,595 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:17:34,595 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:17:34,763 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:17:34,770 DEBUG   HandlerThread:6546 [system_info.py:__init__():31] System info init
-2022-12-14 23:17:34,770 DEBUG   HandlerThread:6546 [system_info.py:__init__():46] System info init done
-2022-12-14 23:17:34,770 INFO    HandlerThread:6546 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:17:34,771 INFO    HandlerThread:6546 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:17:34,771 INFO    SystemMonitor:6546 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:17:34,771 DEBUG   HandlerThread:6546 [system_info.py:probe():195] Probing system
-2022-12-14 23:17:34,773 INFO    SystemMonitor:6546 [interfaces.py:start():168] Started cpu
-2022-12-14 23:17:34,775 INFO    SystemMonitor:6546 [interfaces.py:start():168] Started disk
-2022-12-14 23:17:34,777 INFO    SystemMonitor:6546 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:17:34,781 INFO    SystemMonitor:6546 [interfaces.py:start():168] Started memory
-2022-12-14 23:17:34,782 INFO    SystemMonitor:6546 [interfaces.py:start():168] Started network
-2022-12-14 23:17:34,784 DEBUG   HandlerThread:6546 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:17:34,799 DEBUG   HandlerThread:6546 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:17:34,799 DEBUG   HandlerThread:6546 [system_info.py:probe():241] Probing system done
-2022-12-14 23:17:34,799 DEBUG   HandlerThread:6546 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:17:34.772069', 'startedAt': '2022-12-14T22:17:33.997211', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:17:34,799 INFO    HandlerThread:6546 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:17:34,800 INFO    HandlerThread:6546 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:17:34,800 DEBUG   HandlerThread:6546 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:17:34,800 DEBUG   HandlerThread:6546 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:17:34,800 DEBUG   HandlerThread:6546 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:17:35,600 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/wandb-summary.json
-2022-12-14 23:17:35,600 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/requirements.txt
-2022-12-14 23:17:35,600 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/conda-environment.yaml
-2022-12-14 23:17:35,891 DEBUG   HandlerThread:6546 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:17:35,891 DEBUG   HandlerThread:6546 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:17:35,899 DEBUG   HandlerThread:6546 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:17:35,899 DEBUG   HandlerThread:6546 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:17:35,965 DEBUG   HandlerThread:6546 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:17:35,967 INFO    HandlerThread:6546 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:17:36,058 DEBUG   SenderThread:6546 [sender.py:send():303] send: files
-2022-12-14 23:17:36,058 INFO    SenderThread:6546 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:17:36,059 INFO    SenderThread:6546 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:17:36,059 INFO    SenderThread:6546 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:17:36,070 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:17:36,071 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:17:36,419 DEBUG   SenderThread:6546 [sender.py:send():303] send: telemetry
-2022-12-14 23:17:36,602 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/conda-environment.yaml
-2022-12-14 23:17:36,602 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/diff.patch
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/output.log
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/wandb-metadata.json
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/code/PPO/ppo_torch
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/code
-2022-12-14 23:17:36,603 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/code/PPO
-2022-12-14 23:17:36,613 INFO    Thread-23 :6546 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp9f7l_9jzwandb/2sf6sq2x-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:17:36,856 INFO    Thread-22 :6546 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp9f7l_9jzwandb/uabc7wz2-wandb-metadata.json
-2022-12-14 23:17:36,941 INFO    Thread-24 :6546 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp9f7l_9jzwandb/3qnd0m9a-diff.patch
-2022-12-14 23:17:38,614 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/output.log
-2022-12-14 23:17:51,432 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:17:51,433 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:18:05,773 INFO    Thread-19 :6546 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/files/config.yaml
-2022-12-14 23:18:06,696 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:18:06,696 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:18:21,965 DEBUG   HandlerThread:6546 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:18:21,966 DEBUG   SenderThread:6546 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231733-h9z9voos/logs/debug.log b/wandb/run-20221214_231733-h9z9voos/logs/debug.log
deleted file mode 100644
index ef6311d..0000000
--- a/wandb/run-20221214_231733-h9z9voos/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:17:34,002 INFO    MainThread:6532 [wandb_setup.py:_flush():68] Configure stats pid to 6532
-2022-12-14 23:17:34,002 INFO    MainThread:6532 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:17:34,002 INFO    MainThread:6532 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:17:34,002 INFO    MainThread:6532 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:17:34,003 INFO    MainThread:6532 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:17:34,003 INFO    MainThread:6532 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/logs/debug.log
-2022-12-14 23:17:34,003 INFO    MainThread:6532 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231733-h9z9voos/logs/debug-internal.log
-2022-12-14 23:17:34,004 INFO    MainThread:6532 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:17:34,004 INFO    MainThread:6532 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:17:34,004 INFO    MainThread:6532 [wandb_init.py:init():569] starting backend
-2022-12-14 23:17:34,004 INFO    MainThread:6532 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:17:34,034 INFO    MainThread:6532 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:17:34,040 INFO    MainThread:6532 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:17:34,047 INFO    MainThread:6532 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:17:34,059 INFO    MainThread:6532 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:17:34,593 INFO    MainThread:6532 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:17:34,734 INFO    MainThread:6532 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:17:34,734 INFO    MainThread:6532 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:17:36,065 INFO    MainThread:6532 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:17:36,065 INFO    MainThread:6532 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:17:36,066 INFO    MainThread:6532 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:17:36,066 INFO    MainThread:6532 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:17:36,067 INFO    MainThread:6532 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231733-h9z9voos/run-h9z9voos.wandb b/wandb/run-20221214_231733-h9z9voos/run-h9z9voos.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_231828-33403gym/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_231828-33403gym/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index ad9ffce..0000000
--- a/wandb/run-20221214_231828-33403gym/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,470 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-                    trajectory_advantages = advantage
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_231828-33403gym/files/conda-environment.yaml b/wandb/run-20221214_231828-33403gym/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_231828-33403gym/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_231828-33403gym/files/config.yaml b/wandb/run-20221214_231828-33403gym/files/config.yaml
deleted file mode 100644
index d2fa5aa..0000000
--- a/wandb/run-20221214_231828-33403gym/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056308.067578
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_231828-33403gym/files/diff.patch b/wandb/run-20221214_231828-33403gym/files/diff.patch
deleted file mode 100644
index d08c145..0000000
--- a/wandb/run-20221214_231828-33403gym/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..ad9ffce 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, min=(1.0 - clip_eps), max=(1.0 + clip_eps)) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-+                    trajectory_advantages = advantage
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,44 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +338,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +380,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +431,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +452,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_231828-33403gym/files/output.log b/wandb/run-20221214_231828-33403gym/files/output.log
deleted file mode 100644
index c8dde89..0000000
--- a/wandb/run-20221214_231828-33403gym/files/output.log
+++ /dev/null
@@ -1,9 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
-DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/5288891/envelope/ HTTP/1.1" 200 2
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 43.40618133544922
-INFO:root:Time step: 2000
diff --git a/wandb/run-20221214_231828-33403gym/files/requirements.txt b/wandb/run-20221214_231828-33403gym/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_231828-33403gym/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_231828-33403gym/files/wandb-metadata.json b/wandb/run-20221214_231828-33403gym/files/wandb-metadata.json
deleted file mode 100644
index 897cb8c..0000000
--- a/wandb/run-20221214_231828-33403gym/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:18:28.860503",
-    "startedAt": "2022-12-14T22:18:28.026361",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_231828-33403gym/files/wandb-summary.json b/wandb/run-20221214_231828-33403gym/files/wandb-summary.json
deleted file mode 100644
index e473b88..0000000
--- a/wandb/run-20221214_231828-33403gym/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"time/step": 2000, "loss/policy loss": 0.0, "loss/value loss": 43.40618133544922, "reward/mean return": -6506.470434332803, "_timestamp": 1671056489.4212139, "_runtime": 181.35363578796387, "_step": 0}
\ No newline at end of file
diff --git a/wandb/run-20221214_231828-33403gym/logs/debug-internal.log b/wandb/run-20221214_231828-33403gym/logs/debug-internal.log
deleted file mode 100644
index d3a17b2..0000000
--- a/wandb/run-20221214_231828-33403gym/logs/debug-internal.log
+++ /dev/null
@@ -1,102 +0,0 @@
-2022-12-14 23:18:28,082 INFO    StreamThr :6636 [internal.py:wandb_internal():87] W&B internal server running at pid: 6636, started at: 2022-12-14 23:18:28.080469
-2022-12-14 23:18:28,086 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:18:28,088 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: status
-2022-12-14 23:18:28,092 DEBUG   SenderThread:6636 [sender.py:send():303] send: header
-2022-12-14 23:18:28,092 DEBUG   SenderThread:6636 [sender.py:send():303] send: run
-2022-12-14 23:18:28,097 INFO    WriterThread:6636 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/run-33403gym.wandb
-2022-12-14 23:18:28,632 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:18:28,635 INFO    SenderThread:6636 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files
-2022-12-14 23:18:28,635 INFO    SenderThread:6636 [sender.py:_start_run_threads():928] run started: 33403gym with start time 1671056308.067578
-2022-12-14 23:18:28,635 DEBUG   SenderThread:6636 [sender.py:send():303] send: summary
-2022-12-14 23:18:28,635 INFO    SenderThread:6636 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:18:28,636 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:18:28,853 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:18:28,859 DEBUG   HandlerThread:6636 [system_info.py:__init__():31] System info init
-2022-12-14 23:18:28,859 DEBUG   HandlerThread:6636 [system_info.py:__init__():46] System info init done
-2022-12-14 23:18:28,859 INFO    HandlerThread:6636 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:18:28,859 INFO    SystemMonitor:6636 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:18:28,860 INFO    HandlerThread:6636 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:18:28,860 DEBUG   HandlerThread:6636 [system_info.py:probe():195] Probing system
-2022-12-14 23:18:28,861 INFO    SystemMonitor:6636 [interfaces.py:start():168] Started cpu
-2022-12-14 23:18:28,861 INFO    SystemMonitor:6636 [interfaces.py:start():168] Started disk
-2022-12-14 23:18:28,862 INFO    SystemMonitor:6636 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:18:28,864 INFO    SystemMonitor:6636 [interfaces.py:start():168] Started memory
-2022-12-14 23:18:28,870 INFO    SystemMonitor:6636 [interfaces.py:start():168] Started network
-2022-12-14 23:18:28,871 DEBUG   HandlerThread:6636 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:18:28,887 DEBUG   HandlerThread:6636 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:18:28,887 DEBUG   HandlerThread:6636 [system_info.py:probe():241] Probing system done
-2022-12-14 23:18:28,888 DEBUG   HandlerThread:6636 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:18:28.860503', 'startedAt': '2022-12-14T22:18:28.026361', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:18:28,888 INFO    HandlerThread:6636 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:18:28,888 INFO    HandlerThread:6636 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:18:28,888 DEBUG   HandlerThread:6636 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:18:28,888 DEBUG   HandlerThread:6636 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:18:28,889 DEBUG   HandlerThread:6636 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:18:29,639 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/requirements.txt
-2022-12-14 23:18:29,640 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/conda-environment.yaml
-2022-12-14 23:18:29,640 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/wandb-summary.json
-2022-12-14 23:18:29,989 DEBUG   HandlerThread:6636 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:18:29,989 DEBUG   HandlerThread:6636 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:18:29,997 DEBUG   HandlerThread:6636 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:18:29,997 DEBUG   HandlerThread:6636 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:18:30,063 DEBUG   HandlerThread:6636 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:18:30,064 INFO    HandlerThread:6636 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:18:30,159 DEBUG   SenderThread:6636 [sender.py:send():303] send: files
-2022-12-14 23:18:30,159 INFO    SenderThread:6636 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:18:30,160 INFO    SenderThread:6636 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:18:30,161 INFO    SenderThread:6636 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:18:30,167 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:18:30,167 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:18:30,519 DEBUG   SenderThread:6636 [sender.py:send():303] send: telemetry
-2022-12-14 23:18:30,530 INFO    Thread-23 :6636 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfr_9bg6gwandb/27wkrq5h-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:18:30,641 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/conda-environment.yaml
-2022-12-14 23:18:30,642 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:18:30,642 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/output.log
-2022-12-14 23:18:30,642 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/diff.patch
-2022-12-14 23:18:30,642 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/wandb-metadata.json
-2022-12-14 23:18:30,642 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/code
-2022-12-14 23:18:30,643 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/code/PPO/ppo_torch
-2022-12-14 23:18:30,643 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/code/PPO
-2022-12-14 23:18:31,021 INFO    Thread-24 :6636 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfr_9bg6gwandb/3hagj8ji-diff.patch
-2022-12-14 23:18:31,046 INFO    Thread-22 :6636 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpfr_9bg6gwandb/1h3c75vp-wandb-metadata.json
-2022-12-14 23:18:32,650 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/output.log
-2022-12-14 23:18:45,532 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:18:45,533 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:18:59,792 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/config.yaml
-2022-12-14 23:19:00,766 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:19:00,766 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:19:16,007 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:19:16,008 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:19:28,874 DEBUG   SystemMonitor:6636 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:19:28,879 DEBUG   SenderThread:6636 [sender.py:send():303] send: telemetry
-2022-12-14 23:19:28,879 DEBUG   SenderThread:6636 [sender.py:send():303] send: stats
-2022-12-14 23:19:30,949 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/output.log
-2022-12-14 23:19:30,950 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/config.yaml
-2022-12-14 23:19:33,480 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:19:33,480 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:19:48,727 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:19:48,728 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:19:58,884 DEBUG   SenderThread:6636 [sender.py:send():303] send: stats
-2022-12-14 23:19:59,091 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/output.log
-2022-12-14 23:20:03,999 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:20:03,999 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:20:19,271 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:20:19,271 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:20:28,893 DEBUG   SenderThread:6636 [sender.py:send():303] send: stats
-2022-12-14 23:20:34,533 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:20:34,533 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:20:49,779 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:20:49,779 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:20:58,894 DEBUG   SenderThread:6636 [sender.py:send():303] send: stats
-2022-12-14 23:21:05,054 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:21:05,055 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:21:20,323 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:21:20,323 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:21:28,896 DEBUG   SenderThread:6636 [sender.py:send():303] send: stats
-2022-12-14 23:21:29,424 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:21:29,425 DEBUG   SenderThread:6636 [sender.py:send():303] send: history
-2022-12-14 23:21:29,425 DEBUG   SenderThread:6636 [sender.py:send():303] send: summary
-2022-12-14 23:21:29,426 INFO    SenderThread:6636 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:21:29,558 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/wandb-summary.json
-2022-12-14 23:21:31,570 INFO    Thread-19 :6636 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/files/output.log
-2022-12-14 23:21:35,568 DEBUG   HandlerThread:6636 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:21:35,569 DEBUG   SenderThread:6636 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_231828-33403gym/logs/debug.log b/wandb/run-20221214_231828-33403gym/logs/debug.log
deleted file mode 100644
index 3031bac..0000000
--- a/wandb/run-20221214_231828-33403gym/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:18:28,030 INFO    MainThread:6623 [wandb_setup.py:_flush():68] Configure stats pid to 6623
-2022-12-14 23:18:28,030 INFO    MainThread:6623 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/logs/debug.log
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_231828-33403gym/logs/debug-internal.log
-2022-12-14 23:18:28,031 INFO    MainThread:6623 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:18:28,032 INFO    MainThread:6623 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:18:28,032 INFO    MainThread:6623 [wandb_init.py:init():569] starting backend
-2022-12-14 23:18:28,032 INFO    MainThread:6623 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:18:28,061 INFO    MainThread:6623 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:18:28,067 INFO    MainThread:6623 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:18:28,074 INFO    MainThread:6623 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:18:28,086 INFO    MainThread:6623 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:18:28,631 INFO    MainThread:6623 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:18:28,823 INFO    MainThread:6623 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:18:28,823 INFO    MainThread:6623 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:18:30,166 INFO    MainThread:6623 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:18:30,167 INFO    MainThread:6623 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:18:30,167 INFO    MainThread:6623 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:18:30,167 INFO    MainThread:6623 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:18:30,168 INFO    MainThread:6623 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_231828-33403gym/run-33403gym.wandb b/wandb/run-20221214_231828-33403gym/run-33403gym.wandb
deleted file mode 100644
index 7acfff1..0000000
Binary files a/wandb/run-20221214_231828-33403gym/run-33403gym.wandb and /dev/null differ
diff --git a/wandb/run-20221214_232150-120lppgd/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_232150-120lppgd/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index bcbafc4..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,471 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-                    trajectory_advantages = advantage
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-        best_mean_reward = 0
-        
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(self.ep_returns) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_232150-120lppgd/files/conda-environment.yaml b/wandb/run-20221214_232150-120lppgd/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_232150-120lppgd/files/config.yaml b/wandb/run-20221214_232150-120lppgd/files/config.yaml
deleted file mode 100644
index f69a5f9..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056510.363207
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_232150-120lppgd/files/diff.patch b/wandb/run-20221214_232150-120lppgd/files/diff.patch
deleted file mode 100644
index ee7d7fd..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/diff.patch
+++ /dev/null
@@ -1,551 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..bcbafc4 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
-+        for _ in range(n_step):
- 
--            # render gym env
--            if render:
--                self.env.render(mode='human')
--
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-+                    trajectory_advantages = advantage
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,45 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
-+        best_mean_reward = 0
-         
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
--            
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(self.ep_returns) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +339,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +381,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +432,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +453,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_232150-120lppgd/files/output.log b/wandb/run-20221214_232150-120lppgd/files/output.log
deleted file mode 100644
index fccd9d8..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/output.log
+++ /dev/null
@@ -1,10 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
-DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/5288891/envelope/ HTTP/1.1" 200 2
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 43.40618133544922
-INFO:root:Time step: 2000
-INFO:root:###########################################
diff --git a/wandb/run-20221214_232150-120lppgd/files/requirements.txt b/wandb/run-20221214_232150-120lppgd/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_232150-120lppgd/files/wandb-metadata.json b/wandb/run-20221214_232150-120lppgd/files/wandb-metadata.json
deleted file mode 100644
index c1e3028..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:21:51.064175",
-    "startedAt": "2022-12-14T22:21:50.326419",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_232150-120lppgd/files/wandb-summary.json b/wandb/run-20221214_232150-120lppgd/files/wandb-summary.json
deleted file mode 100644
index e606e1a..0000000
--- a/wandb/run-20221214_232150-120lppgd/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"time/step": 2000, "loss/policy loss": 0.0, "loss/value loss": 43.40618133544922, "reward/mean return": -6506.470434332803, "_timestamp": 1671056579.346784, "_runtime": 68.98357701301575, "_step": 0}
\ No newline at end of file
diff --git a/wandb/run-20221214_232150-120lppgd/logs/debug-internal.log b/wandb/run-20221214_232150-120lppgd/logs/debug-internal.log
deleted file mode 100644
index 2052bee..0000000
--- a/wandb/run-20221214_232150-120lppgd/logs/debug-internal.log
+++ /dev/null
@@ -1,81 +0,0 @@
-2022-12-14 23:21:50,377 INFO    StreamThr :6814 [internal.py:wandb_internal():87] W&B internal server running at pid: 6814, started at: 2022-12-14 23:21:50.376278
-2022-12-14 23:21:50,381 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:21:50,383 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: status
-2022-12-14 23:21:50,386 INFO    WriterThread:6814 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/run-120lppgd.wandb
-2022-12-14 23:21:50,387 DEBUG   SenderThread:6814 [sender.py:send():303] send: header
-2022-12-14 23:21:50,387 DEBUG   SenderThread:6814 [sender.py:send():303] send: run
-2022-12-14 23:21:50,903 INFO    SenderThread:6814 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files
-2022-12-14 23:21:50,903 INFO    SenderThread:6814 [sender.py:_start_run_threads():928] run started: 120lppgd with start time 1671056510.363207
-2022-12-14 23:21:50,903 DEBUG   SenderThread:6814 [sender.py:send():303] send: summary
-2022-12-14 23:21:50,904 INFO    SenderThread:6814 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:21:50,905 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:21:50,906 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:21:51,056 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:21:51,062 DEBUG   HandlerThread:6814 [system_info.py:__init__():31] System info init
-2022-12-14 23:21:51,062 DEBUG   HandlerThread:6814 [system_info.py:__init__():46] System info init done
-2022-12-14 23:21:51,063 INFO    HandlerThread:6814 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:21:51,063 INFO    SystemMonitor:6814 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:21:51,063 INFO    HandlerThread:6814 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:21:51,064 DEBUG   HandlerThread:6814 [system_info.py:probe():195] Probing system
-2022-12-14 23:21:51,065 INFO    SystemMonitor:6814 [interfaces.py:start():168] Started cpu
-2022-12-14 23:21:51,065 INFO    SystemMonitor:6814 [interfaces.py:start():168] Started disk
-2022-12-14 23:21:51,066 INFO    SystemMonitor:6814 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:21:51,067 INFO    SystemMonitor:6814 [interfaces.py:start():168] Started memory
-2022-12-14 23:21:51,072 INFO    SystemMonitor:6814 [interfaces.py:start():168] Started network
-2022-12-14 23:21:51,073 DEBUG   HandlerThread:6814 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:21:51,089 DEBUG   HandlerThread:6814 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:21:51,089 DEBUG   HandlerThread:6814 [system_info.py:probe():241] Probing system done
-2022-12-14 23:21:51,089 DEBUG   HandlerThread:6814 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:21:51.064175', 'startedAt': '2022-12-14T22:21:50.326419', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:21:51,089 INFO    HandlerThread:6814 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:21:51,089 INFO    HandlerThread:6814 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:21:51,090 DEBUG   HandlerThread:6814 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:21:51,090 DEBUG   HandlerThread:6814 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:21:51,090 DEBUG   HandlerThread:6814 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:21:51,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/wandb-summary.json
-2022-12-14 23:21:51,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/conda-environment.yaml
-2022-12-14 23:21:51,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/requirements.txt
-2022-12-14 23:21:52,171 DEBUG   HandlerThread:6814 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:21:52,171 DEBUG   HandlerThread:6814 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:21:52,178 DEBUG   HandlerThread:6814 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:21:52,178 DEBUG   HandlerThread:6814 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:21:52,239 DEBUG   HandlerThread:6814 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:21:52,241 INFO    HandlerThread:6814 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:21:52,329 DEBUG   SenderThread:6814 [sender.py:send():303] send: files
-2022-12-14 23:21:52,329 INFO    SenderThread:6814 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:21:52,329 INFO    SenderThread:6814 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:21:52,330 INFO    SenderThread:6814 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:21:52,336 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:21:52,338 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:21:52,685 DEBUG   SenderThread:6814 [sender.py:send():303] send: telemetry
-2022-12-14 23:21:52,696 INFO    Thread-23 :6814 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpqt0qzz0swandb/1ap3xr70-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:21:52,911 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/conda-environment.yaml
-2022-12-14 23:21:52,911 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/wandb-metadata.json
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/diff.patch
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/output.log
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/code
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/code/PPO
-2022-12-14 23:21:52,912 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/code/PPO/ppo_torch
-2022-12-14 23:21:53,297 INFO    Thread-22 :6814 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpqt0qzz0swandb/7kuxlv8u-wandb-metadata.json
-2022-12-14 23:21:53,380 INFO    Thread-24 :6814 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpqt0qzz0swandb/h1n1wc4j-diff.patch
-2022-12-14 23:21:54,924 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/output.log
-2022-12-14 23:22:07,695 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:22:07,695 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:22:22,071 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/config.yaml
-2022-12-14 23:22:22,923 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:22:22,923 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:22:38,184 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:22:38,185 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:22:51,074 DEBUG   SystemMonitor:6814 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:22:51,078 DEBUG   SenderThread:6814 [sender.py:send():303] send: telemetry
-2022-12-14 23:22:51,080 DEBUG   SenderThread:6814 [sender.py:send():303] send: stats
-2022-12-14 23:22:53,226 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/output.log
-2022-12-14 23:22:53,226 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/config.yaml
-2022-12-14 23:22:53,446 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:22:53,447 DEBUG   SenderThread:6814 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:22:59,350 DEBUG   HandlerThread:6814 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:22:59,352 DEBUG   SenderThread:6814 [sender.py:send():303] send: history
-2022-12-14 23:22:59,352 DEBUG   SenderThread:6814 [sender.py:send():303] send: summary
-2022-12-14 23:22:59,353 INFO    SenderThread:6814 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:00,263 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/wandb-summary.json
-2022-12-14 23:23:01,268 INFO    Thread-19 :6814 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/files/output.log
diff --git a/wandb/run-20221214_232150-120lppgd/logs/debug.log b/wandb/run-20221214_232150-120lppgd/logs/debug.log
deleted file mode 100644
index c815cba..0000000
--- a/wandb/run-20221214_232150-120lppgd/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_setup.py:_flush():68] Configure stats pid to 6802
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/logs/debug.log
-2022-12-14 23:21:50,330 INFO    MainThread:6802 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232150-120lppgd/logs/debug-internal.log
-2022-12-14 23:21:50,331 INFO    MainThread:6802 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:21:50,331 INFO    MainThread:6802 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:21:50,331 INFO    MainThread:6802 [wandb_init.py:init():569] starting backend
-2022-12-14 23:21:50,331 INFO    MainThread:6802 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:21:50,357 INFO    MainThread:6802 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:21:50,362 INFO    MainThread:6802 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:21:50,369 INFO    MainThread:6802 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:21:50,381 INFO    MainThread:6802 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:21:50,904 INFO    MainThread:6802 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:21:51,024 INFO    MainThread:6802 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:21:51,024 INFO    MainThread:6802 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:21:52,336 INFO    MainThread:6802 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:21:52,337 INFO    MainThread:6802 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:21:52,337 INFO    MainThread:6802 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:21:52,337 INFO    MainThread:6802 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:21:52,338 INFO    MainThread:6802 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_232150-120lppgd/run-120lppgd.wandb b/wandb/run-20221214_232150-120lppgd/run-120lppgd.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_232308-3kvr57go/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_232308-3kvr57go/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index ad68b50..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,471 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-                    trajectory_advantages = advantage
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-        best_mean_reward = 0
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(rewards.numpy()) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_232308-3kvr57go/files/conda-environment.yaml b/wandb/run-20221214_232308-3kvr57go/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_232308-3kvr57go/files/config.yaml b/wandb/run-20221214_232308-3kvr57go/files/config.yaml
deleted file mode 100644
index 84c6b21..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/config.yaml
+++ /dev/null
@@ -1,58 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056588.151645
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_232308-3kvr57go/files/diff.patch b/wandb/run-20221214_232308-3kvr57go/files/diff.patch
deleted file mode 100644
index 47aefee..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/diff.patch
+++ /dev/null
@@ -1,552 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..ad68b50 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-+                    trajectory_advantages = advantage
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,45 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+        best_mean_reward = 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(rewards.numpy()) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +339,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +381,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +432,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +453,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_232308-3kvr57go/files/output.log b/wandb/run-20221214_232308-3kvr57go/files/output.log
deleted file mode 100644
index 8b13789..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/output.log
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/wandb/run-20221214_232308-3kvr57go/files/requirements.txt b/wandb/run-20221214_232308-3kvr57go/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_232308-3kvr57go/files/wandb-metadata.json b/wandb/run-20221214_232308-3kvr57go/files/wandb-metadata.json
deleted file mode 100644
index decdd86..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:23:08.887734",
-    "startedAt": "2022-12-14T22:23:08.109481",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_232308-3kvr57go/files/wandb-summary.json b/wandb/run-20221214_232308-3kvr57go/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221214_232308-3kvr57go/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221214_232308-3kvr57go/logs/debug-internal.log b/wandb/run-20221214_232308-3kvr57go/logs/debug-internal.log
deleted file mode 100644
index 29d5230..0000000
--- a/wandb/run-20221214_232308-3kvr57go/logs/debug-internal.log
+++ /dev/null
@@ -1,63 +0,0 @@
-2022-12-14 23:23:08,166 INFO    StreamThr :6931 [internal.py:wandb_internal():87] W&B internal server running at pid: 6931, started at: 2022-12-14 23:23:08.164708
-2022-12-14 23:23:08,170 DEBUG   HandlerThread:6931 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:23:08,173 DEBUG   SenderThread:6931 [sender.py:send_request():317] send_request: status
-2022-12-14 23:23:08,176 DEBUG   SenderThread:6931 [sender.py:send():303] send: header
-2022-12-14 23:23:08,176 INFO    WriterThread:6931 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/run-3kvr57go.wandb
-2022-12-14 23:23:08,178 DEBUG   SenderThread:6931 [sender.py:send():303] send: run
-2022-12-14 23:23:08,704 INFO    SenderThread:6931 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files
-2022-12-14 23:23:08,704 INFO    SenderThread:6931 [sender.py:_start_run_threads():928] run started: 3kvr57go with start time 1671056588.151645
-2022-12-14 23:23:08,704 DEBUG   SenderThread:6931 [sender.py:send():303] send: summary
-2022-12-14 23:23:08,705 INFO    SenderThread:6931 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:08,707 DEBUG   HandlerThread:6931 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:23:08,707 DEBUG   SenderThread:6931 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:23:08,879 DEBUG   HandlerThread:6931 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:23:08,886 DEBUG   HandlerThread:6931 [system_info.py:__init__():31] System info init
-2022-12-14 23:23:08,886 DEBUG   HandlerThread:6931 [system_info.py:__init__():46] System info init done
-2022-12-14 23:23:08,886 INFO    HandlerThread:6931 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:23:08,886 INFO    SystemMonitor:6931 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:23:08,887 INFO    HandlerThread:6931 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:23:08,887 DEBUG   HandlerThread:6931 [system_info.py:probe():195] Probing system
-2022-12-14 23:23:08,888 INFO    SystemMonitor:6931 [interfaces.py:start():168] Started cpu
-2022-12-14 23:23:08,888 INFO    SystemMonitor:6931 [interfaces.py:start():168] Started disk
-2022-12-14 23:23:08,889 INFO    SystemMonitor:6931 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:23:08,895 INFO    SystemMonitor:6931 [interfaces.py:start():168] Started memory
-2022-12-14 23:23:08,896 INFO    SystemMonitor:6931 [interfaces.py:start():168] Started network
-2022-12-14 23:23:08,897 DEBUG   HandlerThread:6931 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:23:08,913 DEBUG   HandlerThread:6931 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:23:08,913 DEBUG   HandlerThread:6931 [system_info.py:probe():241] Probing system done
-2022-12-14 23:23:08,913 DEBUG   HandlerThread:6931 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:23:08.887734', 'startedAt': '2022-12-14T22:23:08.109481', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:23:08,913 INFO    HandlerThread:6931 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:23:08,913 INFO    HandlerThread:6931 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:23:08,913 DEBUG   HandlerThread:6931 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:23:08,914 DEBUG   HandlerThread:6931 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:23:08,914 DEBUG   HandlerThread:6931 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:23:09,713 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/wandb-summary.json
-2022-12-14 23:23:09,713 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/conda-environment.yaml
-2022-12-14 23:23:09,713 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/requirements.txt
-2022-12-14 23:23:09,984 DEBUG   HandlerThread:6931 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:23:09,984 DEBUG   HandlerThread:6931 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:23:09,992 DEBUG   HandlerThread:6931 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:23:09,992 DEBUG   HandlerThread:6931 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:23:10,053 DEBUG   HandlerThread:6931 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:23:10,054 INFO    HandlerThread:6931 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:23:10,143 DEBUG   SenderThread:6931 [sender.py:send():303] send: files
-2022-12-14 23:23:10,143 INFO    SenderThread:6931 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:23:10,144 INFO    SenderThread:6931 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:23:10,144 INFO    SenderThread:6931 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:23:10,150 DEBUG   HandlerThread:6931 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:23:10,150 DEBUG   SenderThread:6931 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:23:10,480 DEBUG   SenderThread:6931 [sender.py:send():303] send: telemetry
-2022-12-14 23:23:10,483 INFO    Thread-23 :6931 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpydsbd35fwandb/1a84ex8m-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:23:10,715 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/conda-environment.yaml
-2022-12-14 23:23:10,715 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/wandb-metadata.json
-2022-12-14 23:23:10,715 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/output.log
-2022-12-14 23:23:10,716 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/diff.patch
-2022-12-14 23:23:10,716 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:23:10,716 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/code/PPO/ppo_torch
-2022-12-14 23:23:10,716 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/code/PPO
-2022-12-14 23:23:10,716 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/code
-2022-12-14 23:23:10,856 INFO    Thread-22 :6931 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpydsbd35fwandb/300jb52l-wandb-metadata.json
-2022-12-14 23:23:11,096 INFO    Thread-24 :6931 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpydsbd35fwandb/3tvo5gc7-diff.patch
-2022-12-14 23:23:12,727 INFO    Thread-19 :6931 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/files/output.log
-2022-12-14 23:23:25,503 DEBUG   HandlerThread:6931 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:23:25,503 DEBUG   SenderThread:6931 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221214_232308-3kvr57go/logs/debug.log b/wandb/run-20221214_232308-3kvr57go/logs/debug.log
deleted file mode 100644
index f11ada5..0000000
--- a/wandb/run-20221214_232308-3kvr57go/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_setup.py:_flush():68] Configure stats pid to 6917
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/logs/debug.log
-2022-12-14 23:23:08,114 INFO    MainThread:6917 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232308-3kvr57go/logs/debug-internal.log
-2022-12-14 23:23:08,115 INFO    MainThread:6917 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:23:08,115 INFO    MainThread:6917 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:23:08,115 INFO    MainThread:6917 [wandb_init.py:init():569] starting backend
-2022-12-14 23:23:08,116 INFO    MainThread:6917 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:23:08,145 INFO    MainThread:6917 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:23:08,151 INFO    MainThread:6917 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:23:08,158 INFO    MainThread:6917 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:23:08,170 INFO    MainThread:6917 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:23:08,705 INFO    MainThread:6917 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:23:08,845 INFO    MainThread:6917 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:23:08,845 INFO    MainThread:6917 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:23:10,149 INFO    MainThread:6917 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:23:10,150 INFO    MainThread:6917 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:23:10,150 INFO    MainThread:6917 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:23:10,151 INFO    MainThread:6917 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:23:10,152 INFO    MainThread:6917 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221214_232308-3kvr57go/run-3kvr57go.wandb b/wandb/run-20221214_232308-3kvr57go/run-3kvr57go.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index ad68b50..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,471 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# This is a TODO Section - please mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Fix incorrect calculation of rewards2go --> should be mean reward
-# 3) Fix calculation of Advantage
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # keep track of previous rewards and 
-        # perform steps to calculate mean return
-        self.ep_returns = deque(maxlen=100)
-        self.num_steps = 0
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-
-        done = False
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # tracking of values
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-                self.num_steps += 1
-
-                # break out of loop if episode is terminated
-                if done:
-                    # calculate stats and reset
-                    self.ep_returns.append(sum(trajectory_rewards))
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-                    trajectory_advantages = advantage
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-        best_mean_reward = 0
-
-        while self.num_steps < self.total_steps:
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = np.mean(rewards.numpy()) # mean return
-
-            _, curr_log_probs = self.get_values(obs, actions)
-            # STEP 6-7: calculate loss and update weights
-            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-            
-            logging.info('###########################################')
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {self.num_steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': self.num_steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': self.num_steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 100000        # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml b/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221214_232341-3hc18z86/files/config.yaml b/wandb/run-20221214_232341-3hc18z86/files/config.yaml
deleted file mode 100644
index 905420b..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/config.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671056622.010177
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 2
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 5
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 100000
diff --git a/wandb/run-20221214_232341-3hc18z86/files/diff.patch b/wandb/run-20221214_232341-3hc18z86/files/diff.patch
deleted file mode 100644
index 47aefee..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/diff.patch
+++ /dev/null
@@ -1,552 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..ad68b50 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -35,7 +36,6 @@ MODEL_PATH = './models/'
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -121,6 +123,11 @@ class PPO_PolicyGradient:
-         self.epsilon = epsilon
-         self.adam_eps = adam_eps
- 
-+        # keep track of previous rewards and 
-+        # perform steps to calculate mean return
-+        self.ep_returns = deque(maxlen=100)
-+        self.num_steps = 0
-+
-         # environment
-         self.env = env
- 
-@@ -132,13 +139,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +163,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +192,77 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+
-+        done = False
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
-                 # tracking of values
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+                self.num_steps += 1
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # calculate stats and reset
-+                    self.ep_returns.append(sum(trajectory_rewards))
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(trajectory_rewards), np.array(trajectory_values))
-+                    trajectory_advantages = advantage
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +270,45 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
--        
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+        best_mean_reward = 0
-+
-+        while self.num_steps < self.total_steps:
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
--            
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
--                # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
-+            # calculate mean reward per episode
-+            mean_reward = np.mean(rewards.numpy()) # mean return
-+
-+            _, curr_log_probs = self.get_values(obs, actions)
-+            # STEP 6-7: calculate loss and update weights
-+            policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-             
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {self.num_steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': self.num_steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': self.num_steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +339,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +381,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 100000        # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
-     num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +432,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +453,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
diff --git a/wandb/run-20221214_232341-3hc18z86/files/output.log b/wandb/run-20221214_232341-3hc18z86/files/output.log
deleted file mode 100644
index 247ede0..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/output.log
+++ /dev/null
@@ -1,354 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 43.40618133544922
-INFO:root:Time step: 2000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 5.9604643443123e-10
-INFO:root:Value loss: 54.37372589111328
-INFO:root:Time step: 4000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.8610229740877458e-09
-INFO:root:Value loss: 45.682090759277344
-INFO:root:Time step: 6000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.19209286886246e-09
-INFO:root:Value loss: 37.83928298950195
-INFO:root:Time step: 8000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 40.16980743408203
-INFO:root:Time step: 10000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.907348723406699e-09
-INFO:root:Value loss: 33.43211364746094
-INFO:root:Time step: 12000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -3.6954879156780862e-09
-INFO:root:Value loss: 50.62261962890625
-INFO:root:Time step: 14000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.3841859042583735e-10
-INFO:root:Value loss: 37.697654724121094
-INFO:root:Time step: 16000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 36.25606155395508
-INFO:root:Time step: 18000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.8610229740877458e-09
-INFO:root:Value loss: 39.03650665283203
-INFO:root:Time step: 20000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -1.4305114870438729e-09
-INFO:root:Value loss: 37.34614562988281
-INFO:root:Time step: 22000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.145767119543507e-09
-INFO:root:Value loss: 37.438594818115234
-INFO:root:Time step: 24000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.3841859042583735e-10
-INFO:root:Value loss: 41.756797790527344
-INFO:root:Time step: 26000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -3.3378602104505717e-09
-INFO:root:Value loss: 27.893775939941406
-INFO:root:Time step: 28000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.145767119543507e-09
-INFO:root:Value loss: 29.516477584838867
-INFO:root:Time step: 30000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -9.536743617033494e-10
-INFO:root:Value loss: 29.775753021240234
-INFO:root:Time step: 32000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 4.768371808516747e-10
-INFO:root:Value loss: 30.935928344726562
-INFO:root:Time step: 34000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
-DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/5288891/envelope/ HTTP/1.1" 200 2
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 3.814697446813398e-09
-INFO:root:Value loss: 28.816373825073242
-INFO:root:Time step: 36000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.19209286886246e-09
-INFO:root:Value loss: 33.39970397949219
-INFO:root:Time step: 38000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.5497207961345794e-09
-INFO:root:Value loss: 30.250436782836914
-INFO:root:Time step: 40000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.907348723406699e-09
-INFO:root:Value loss: 31.88054656982422
-INFO:root:Time step: 42000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -5.364417798858767e-10
-INFO:root:Value loss: 22.350801467895508
-INFO:root:Time step: 44000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.19209286886246e-09
-INFO:root:Value loss: 21.625774383544922
-INFO:root:Time step: 46000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.9802322831784522e-09
-INFO:root:Value loss: 28.428491592407227
-INFO:root:Time step: 48000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.38418573772492e-09
-INFO:root:Value loss: 14.0596342086792
-INFO:root:Time step: 50000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.2649764286342133e-09
-INFO:root:Value loss: 21.228694915771484
-INFO:root:Time step: 52000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 7.152557435219364e-10
-INFO:root:Value loss: 22.926198959350586
-INFO:root:Time step: 54000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 5.2452087118126656e-09
-INFO:root:Value loss: 17.225967407226562
-INFO:root:Time step: 56000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -1.19209286886246e-09
-INFO:root:Value loss: 27.984020233154297
-INFO:root:Time step: 58000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.6226043559063328e-09
-INFO:root:Value loss: 13.068812370300293
-INFO:root:Time step: 60000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.3841859042583735e-10
-INFO:root:Value loss: 21.537742614746094
-INFO:root:Time step: 62000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -1.0728835597717534e-09
-INFO:root:Value loss: 13.602362632751465
-INFO:root:Time step: 64000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.145767119543507e-09
-INFO:root:Value loss: 13.765698432922363
-INFO:root:Time step: 66000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 5.9604645663569045e-09
-INFO:root:Value loss: 15.050045013427734
-INFO:root:Time step: 68000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 0.0
-INFO:root:Value loss: 12.676763534545898
-INFO:root:Time step: 70000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -3.814697446813398e-09
-INFO:root:Value loss: 15.006499290466309
-INFO:root:Time step: 72000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -1.907348723406699e-09
-INFO:root:Value loss: 19.001707077026367
-INFO:root:Time step: 74000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -3.0994415922691587e-09
-INFO:root:Value loss: 24.808868408203125
-INFO:root:Time step: 76000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -5.006790093631253e-09
-INFO:root:Value loss: 11.963274002075195
-INFO:root:Time step: 78000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -4.768371808516747e-10
-INFO:root:Value loss: 24.587522506713867
-INFO:root:Time step: 80000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.38418573772492e-09
-INFO:root:Value loss: 15.616292953491211
-INFO:root:Time step: 82000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.19209286886246e-09
-INFO:root:Value loss: 10.769879341125488
-INFO:root:Time step: 84000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 4.76837147544984e-09
-INFO:root:Value loss: 14.923088073730469
-INFO:root:Time step: 86000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -2.0265580324974053e-09
-INFO:root:Value loss: 10.573598861694336
-INFO:root:Time step: 88000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 1.1920929521291868e-10
-INFO:root:Value loss: 8.488037109375
-INFO:root:Time step: 90000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -8.344650526126429e-10
-INFO:root:Value loss: 6.4005608558654785
-INFO:root:Time step: 92000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 5.9604643443123e-10
-INFO:root:Value loss: 13.146652221679688
-INFO:root:Time step: 94000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: -4.649162388403738e-09
-INFO:root:Value loss: 8.51028060913086
-INFO:root:Time step: 96000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 9.536743617033494e-10
-INFO:root:Value loss: 4.856411457061768
-INFO:root:Time step: 98000
-INFO:root:###########################################
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-INFO:root:###########################################
-INFO:root:Policy loss: 2.5033950468156263e-09
-INFO:root:Value loss: 13.574505805969238
-INFO:root:Time step: 100000
-INFO:root:###########################################
-INFO:root:### Done ###
\ No newline at end of file
diff --git a/wandb/run-20221214_232341-3hc18z86/files/requirements.txt b/wandb/run-20221214_232341-3hc18z86/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221214_232341-3hc18z86/files/wandb-metadata.json b/wandb/run-20221214_232341-3hc18z86/files/wandb-metadata.json
deleted file mode 100644
index 48af1e0..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-14T22:23:42.761581",
-    "startedAt": "2022-12-14T22:23:41.979723",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json b/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
deleted file mode 100644
index 88a5f4a..0000000
--- a/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"time/step": 100000, "loss/policy loss": 2.5033950468156263e-09, "loss/value loss": 13.574505805969238, "reward/mean return": -5.657357215881348, "_timestamp": 1671056767.4209979, "_runtime": 145.41082096099854, "_step": 49, "_wandb": {"runtime": 144}}
\ No newline at end of file
diff --git a/wandb/run-20221214_232341-3hc18z86/logs/debug-internal.log b/wandb/run-20221214_232341-3hc18z86/logs/debug-internal.log
deleted file mode 100644
index 067da11..0000000
--- a/wandb/run-20221214_232341-3hc18z86/logs/debug-internal.log
+++ /dev/null
@@ -1,506 +0,0 @@
-2022-12-14 23:23:42,023 INFO    StreamThr :7006 [internal.py:wandb_internal():87] W&B internal server running at pid: 7006, started at: 2022-12-14 23:23:42.022857
-2022-12-14 23:23:42,028 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: status
-2022-12-14 23:23:42,029 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: status
-2022-12-14 23:23:42,033 INFO    WriterThread:7006 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/run-3hc18z86.wandb
-2022-12-14 23:23:42,033 DEBUG   SenderThread:7006 [sender.py:send():303] send: header
-2022-12-14 23:23:42,033 DEBUG   SenderThread:7006 [sender.py:send():303] send: run
-2022-12-14 23:23:42,582 INFO    SenderThread:7006 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files
-2022-12-14 23:23:42,583 INFO    SenderThread:7006 [sender.py:_start_run_threads():928] run started: 3hc18z86 with start time 1671056622.010177
-2022-12-14 23:23:42,583 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:23:42,583 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:42,586 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: check_version
-2022-12-14 23:23:42,586 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: check_version
-2022-12-14 23:23:42,754 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: run_start
-2022-12-14 23:23:42,760 DEBUG   HandlerThread:7006 [system_info.py:__init__():31] System info init
-2022-12-14 23:23:42,760 DEBUG   HandlerThread:7006 [system_info.py:__init__():46] System info init done
-2022-12-14 23:23:42,760 INFO    HandlerThread:7006 [system_monitor.py:start():150] Starting system monitor
-2022-12-14 23:23:42,761 INFO    SystemMonitor:7006 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-14 23:23:42,761 INFO    HandlerThread:7006 [system_monitor.py:probe():171] Collecting system info
-2022-12-14 23:23:42,761 DEBUG   HandlerThread:7006 [system_info.py:probe():195] Probing system
-2022-12-14 23:23:42,761 INFO    SystemMonitor:7006 [interfaces.py:start():168] Started cpu
-2022-12-14 23:23:42,762 INFO    SystemMonitor:7006 [interfaces.py:start():168] Started disk
-2022-12-14 23:23:42,764 INFO    SystemMonitor:7006 [interfaces.py:start():168] Started gpuapple
-2022-12-14 23:23:42,767 INFO    SystemMonitor:7006 [interfaces.py:start():168] Started memory
-2022-12-14 23:23:42,768 INFO    SystemMonitor:7006 [interfaces.py:start():168] Started network
-2022-12-14 23:23:42,772 DEBUG   HandlerThread:7006 [system_info.py:_probe_git():180] Probing git
-2022-12-14 23:23:42,788 DEBUG   HandlerThread:7006 [system_info.py:_probe_git():188] Probing git done
-2022-12-14 23:23:42,788 DEBUG   HandlerThread:7006 [system_info.py:probe():241] Probing system done
-2022-12-14 23:23:42,788 DEBUG   HandlerThread:7006 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-14T22:23:42.761581', 'startedAt': '2022-12-14T22:23:41.979723', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-14 23:23:42,788 INFO    HandlerThread:7006 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-14 23:23:42,788 INFO    HandlerThread:7006 [system_monitor.py:probe():184] Publishing system info
-2022-12-14 23:23:42,789 DEBUG   HandlerThread:7006 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-14 23:23:42,789 DEBUG   HandlerThread:7006 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-14 23:23:42,789 DEBUG   HandlerThread:7006 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-14 23:23:43,591 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:23:43,592 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml
-2022-12-14 23:23:43,592 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/requirements.txt
-2022-12-14 23:23:43,891 DEBUG   HandlerThread:7006 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-14 23:23:43,891 DEBUG   HandlerThread:7006 [system_info.py:_save_code():89] Saving code
-2022-12-14 23:23:43,898 DEBUG   HandlerThread:7006 [system_info.py:_save_code():110] Saving code done
-2022-12-14 23:23:43,898 DEBUG   HandlerThread:7006 [system_info.py:_save_patches():127] Saving git patches
-2022-12-14 23:23:43,959 DEBUG   HandlerThread:7006 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-14 23:23:43,960 INFO    HandlerThread:7006 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-14 23:23:44,051 DEBUG   SenderThread:7006 [sender.py:send():303] send: files
-2022-12-14 23:23:44,051 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-14 23:23:44,052 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-14 23:23:44,052 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-14 23:23:44,058 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:23:44,059 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:23:44,371 INFO    Thread-23 :7006 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp1jpcg2w2wandb/3hc20mrb-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:23:44,382 DEBUG   SenderThread:7006 [sender.py:send():303] send: telemetry
-2022-12-14 23:23:44,593 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml
-2022-12-14 23:23:44,593 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-metadata.json
-2022-12-14 23:23:44,593 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:23:44,593 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/diff.patch
-2022-12-14 23:23:44,593 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:23:44,594 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/code
-2022-12-14 23:23:44,594 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/code/PPO
-2022-12-14 23:23:44,594 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch
-2022-12-14 23:23:44,730 INFO    Thread-22 :7006 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp1jpcg2w2wandb/16eb0v8n-wandb-metadata.json
-2022-12-14 23:23:44,822 INFO    Thread-24 :7006 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp1jpcg2w2wandb/2355f3h8-diff.patch
-2022-12-14 23:23:46,606 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:23:53,298 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:23:53,299 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:23:53,299 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:23:53,302 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:53,636 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:23:54,642 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:23:56,038 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:23:56,040 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:23:56,040 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:23:56,045 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:56,648 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:23:58,654 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:23:59,134 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:23:59,136 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:23:59,136 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:23:59,136 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:23:59,402 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:23:59,402 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:23:59,660 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:00,665 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:02,366 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:02,369 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:02,369 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:02,370 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:02,671 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:04,678 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:05,867 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:05,869 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:05,869 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:05,870 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:06,686 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:06,686 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:09,330 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:09,332 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:09,332 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:09,333 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:09,703 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:10,706 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:12,268 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:12,528 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:12,528 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:12,528 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:12,712 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:12,712 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/config.yaml
-2022-12-14 23:24:14,663 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:24:14,663 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:24:14,721 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:15,025 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:15,026 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:15,027 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:15,028 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:15,726 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:16,731 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:17,814 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:17,815 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:17,816 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:17,817 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:18,742 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:18,743 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:20,523 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:20,524 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:20,525 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:20,530 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:20,750 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:22,760 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:24,162 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:24,164 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:24,165 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:24,166 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:24,769 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:24,769 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:26,769 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:26,770 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:26,770 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:26,771 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:26,780 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:28,791 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:29,698 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:29,699 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:29,699 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:29,700 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:29,797 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:29,895 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:24:29,896 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:24:30,803 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:32,250 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:32,251 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:32,252 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:32,253 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:32,814 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:32,814 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:34,823 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:34,825 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:34,825 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:34,827 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:34,828 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:36,839 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:37,373 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:37,374 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:37,375 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:37,381 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:37,845 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:38,848 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:40,203 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:40,204 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:40,205 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:40,207 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:40,859 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:40,859 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:42,770 DEBUG   SystemMonitor:7006 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-14 23:24:42,773 DEBUG   SenderThread:7006 [sender.py:send():303] send: telemetry
-2022-12-14 23:24:42,773 DEBUG   SenderThread:7006 [sender.py:send():303] send: stats
-2022-12-14 23:24:42,872 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:42,874 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:42,874 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:42,875 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:43,874 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:44,875 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:45,134 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:24:45,135 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:24:45,489 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:45,492 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:45,492 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:45,492 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:45,881 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:46,886 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:48,031 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:48,033 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:48,033 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:48,034 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:48,893 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:48,894 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:52,403 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:52,405 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:52,405 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:52,411 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:52,905 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:52,906 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:54,906 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:54,908 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:54,908 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:54,909 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:54,914 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:56,923 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:57,383 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:57,385 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:57,386 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:57,387 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:57,929 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:24:58,935 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:24:59,900 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:24:59,901 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:24:59,901 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:24:59,902 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:24:59,940 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:00,388 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:25:00,388 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:25:00,946 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:02,414 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:02,416 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:02,416 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:02,418 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:02,953 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:02,954 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:04,922 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:04,923 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:04,923 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:04,924 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:04,961 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:06,970 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:07,427 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:07,427 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:07,428 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:07,429 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:07,976 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:08,979 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:09,935 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:09,937 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:09,937 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:09,938 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:09,985 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:10,990 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:12,444 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:12,445 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:12,445 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:12,447 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:12,774 DEBUG   SenderThread:7006 [sender.py:send():303] send: stats
-2022-12-14 23:25:13,000 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:14,006 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/config.yaml
-2022-12-14 23:25:14,940 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:14,941 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:14,941 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:14,943 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:15,012 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:15,012 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:15,630 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:25:15,630 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:25:17,023 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:17,438 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:17,440 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:17,440 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:17,441 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:18,029 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:19,032 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:19,888 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:19,890 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:19,890 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:19,891 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:20,035 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:21,039 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:22,363 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:22,365 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:22,365 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:22,367 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:23,050 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:24,930 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:24,931 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:24,932 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:24,932 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:25,061 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:25,062 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:27,069 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:27,440 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:27,441 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:27,441 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:27,446 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:28,075 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:29,081 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:29,953 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:29,954 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:29,954 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:29,955 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:30,086 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:30,862 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:25:30,862 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:25:31,090 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:32,464 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:32,465 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:32,465 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:32,466 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:33,097 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:35,107 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:35,943 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:35,945 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:35,945 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:35,946 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:36,113 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:37,115 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:38,644 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:38,645 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:38,646 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:38,647 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:39,126 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:41,137 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:41,307 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:41,308 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:41,308 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:41,309 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:42,143 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:42,776 DEBUG   SenderThread:7006 [sender.py:send():303] send: stats
-2022-12-14 23:25:43,148 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:43,861 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:43,862 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:43,863 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:43,863 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:44,154 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:45,159 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:46,096 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:25:46,097 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:25:46,427 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:46,428 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:46,428 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:46,433 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:47,169 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:47,169 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:48,974 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:48,975 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:48,975 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:48,976 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:49,179 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:51,191 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:51,541 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:51,543 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:51,543 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:51,547 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:52,193 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:53,199 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:54,095 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:54,097 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:54,097 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:54,098 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:54,201 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:55,206 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:56,665 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:56,666 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:56,667 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:56,668 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:57,215 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:25:59,216 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:25:59,217 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:25:59,217 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:25:59,219 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:25:59,226 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:25:59,226 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:26:01,238 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:26:01,334 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-14 23:26:01,334 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: stop_status
-2022-12-14 23:26:01,790 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:26:01,791 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:26:01,791 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:26:01,792 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:26:02,243 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:26:03,249 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:26:04,560 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:26:04,561 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:26:04,562 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:26:04,563 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:26:05,257 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:26:05,257 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:26:07,424 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: partial_history
-2022-12-14 23:26:07,426 DEBUG   SenderThread:7006 [sender.py:send():303] send: history
-2022-12-14 23:26:07,426 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:26:07,426 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:26:07,429 DEBUG   SenderThread:7006 [sender.py:send():303] send: telemetry
-2022-12-14 23:26:07,432 DEBUG   SenderThread:7006 [sender.py:send():303] send: exit
-2022-12-14 23:26:07,432 INFO    SenderThread:7006 [sender.py:send_exit():442] handling exit code: 0
-2022-12-14 23:26:07,433 INFO    SenderThread:7006 [sender.py:send_exit():444] handling runtime: 144
-2022-12-14 23:26:07,433 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:26:07,433 INFO    SenderThread:7006 [sender.py:send_exit():450] send defer
-2022-12-14 23:26:07,434 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,434 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 0
-2022-12-14 23:26:07,434 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,434 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 0
-2022-12-14 23:26:07,434 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 1
-2022-12-14 23:26:07,435 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,435 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 1
-2022-12-14 23:26:07,435 INFO    HandlerThread:7006 [system_monitor.py:finish():160] Stopping system monitor
-2022-12-14 23:26:07,460 DEBUG   SystemMonitor:7006 [system_monitor.py:_start():137] Finished system metrics aggregation loop
-2022-12-14 23:26:07,460 DEBUG   SystemMonitor:7006 [system_monitor.py:_start():141] Publishing last batch of metrics
-2022-12-14 23:26:07,463 INFO    HandlerThread:7006 [interfaces.py:finish():175] Joined cpu
-2022-12-14 23:26:07,463 INFO    HandlerThread:7006 [interfaces.py:finish():175] Joined disk
-2022-12-14 23:26:07,463 INFO    HandlerThread:7006 [interfaces.py:finish():175] Joined gpuapple
-2022-12-14 23:26:07,463 INFO    HandlerThread:7006 [interfaces.py:finish():175] Joined memory
-2022-12-14 23:26:07,463 INFO    HandlerThread:7006 [interfaces.py:finish():175] Joined network
-2022-12-14 23:26:07,464 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,464 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 1
-2022-12-14 23:26:07,464 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 2
-2022-12-14 23:26:07,464 DEBUG   SenderThread:7006 [sender.py:send():303] send: stats
-2022-12-14 23:26:07,466 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,466 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 2
-2022-12-14 23:26:07,466 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,466 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 2
-2022-12-14 23:26:07,466 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 3
-2022-12-14 23:26:07,466 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,467 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 3
-2022-12-14 23:26:07,467 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,467 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 3
-2022-12-14 23:26:07,467 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 4
-2022-12-14 23:26:07,467 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,467 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 4
-2022-12-14 23:26:07,468 DEBUG   SenderThread:7006 [sender.py:send():303] send: summary
-2022-12-14 23:26:07,468 INFO    SenderThread:7006 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-14 23:26:07,469 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,469 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 4
-2022-12-14 23:26:07,469 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 5
-2022-12-14 23:26:07,469 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,469 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 5
-2022-12-14 23:26:07,469 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,469 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 5
-2022-12-14 23:26:07,738 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 6
-2022-12-14 23:26:07,739 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:07,739 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 6
-2022-12-14 23:26:07,739 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:07,739 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 6
-2022-12-14 23:26:08,271 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:26:08,271 INFO    Thread-19 :7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/config.yaml
-2022-12-14 23:26:08,435 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-14 23:26:08,810 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 7
-2022-12-14 23:26:08,810 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: poll_exit
-2022-12-14 23:26:08,812 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:08,812 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 7
-2022-12-14 23:26:08,813 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:08,813 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 7
-2022-12-14 23:26:08,813 INFO    SenderThread:7006 [dir_watcher.py:finish():362] shutting down directory watcher
-2022-12-14 23:26:09,275 INFO    SenderThread:7006 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:26:09,276 INFO    SenderThread:7006 [dir_watcher.py:finish():392] scan: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files
-2022-12-14 23:26:09,276 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml conda-environment.yaml
-2022-12-14 23:26:09,276 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/config.yaml config.yaml
-2022-12-14 23:26:09,277 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/diff.patch diff.patch
-2022-12-14 23:26:09,277 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log output.log
-2022-12-14 23:26:09,282 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/requirements.txt requirements.txt
-2022-12-14 23:26:09,287 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-metadata.json wandb-metadata.json
-2022-12-14 23:26:09,288 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json wandb-summary.json
-2022-12-14 23:26:09,301 INFO    SenderThread:7006 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/code/PPO/ppo_torch/ppo_continuous.py code/PPO/ppo_torch/ppo_continuous.py
-2022-12-14 23:26:09,302 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 8
-2022-12-14 23:26:09,303 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:09,303 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 8
-2022-12-14 23:26:09,304 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:09,304 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 8
-2022-12-14 23:26:09,304 INFO    SenderThread:7006 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-14 23:26:09,436 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-14 23:26:09,437 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: poll_exit
-2022-12-14 23:26:10,301 INFO    Thread-25 :7006 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/conda-environment.yaml
-2022-12-14 23:26:10,303 INFO    Thread-26 :7006 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/config.yaml
-2022-12-14 23:26:10,308 INFO    Thread-28 :7006 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/requirements.txt
-2022-12-14 23:26:10,356 INFO    Thread-29 :7006 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/wandb-summary.json
-2022-12-14 23:26:10,408 INFO    Thread-27 :7006 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/files/output.log
-2022-12-14 23:26:10,438 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-14 23:26:10,438 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: poll_exit
-2022-12-14 23:26:10,613 INFO    Thread-18 (_thread_body):7006 [sender.py:transition_state():463] send defer: 9
-2022-12-14 23:26:10,614 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:10,615 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 9
-2022-12-14 23:26:10,615 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:10,615 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 9
-2022-12-14 23:26:10,615 INFO    SenderThread:7006 [file_pusher.py:join():173] waiting for file pusher
-2022-12-14 23:26:10,615 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 10
-2022-12-14 23:26:10,616 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:10,616 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 10
-2022-12-14 23:26:10,616 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:10,616 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 10
-2022-12-14 23:26:11,164 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 11
-2022-12-14 23:26:11,165 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:11,165 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 11
-2022-12-14 23:26:11,166 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:11,166 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 11
-2022-12-14 23:26:11,166 INFO    SenderThread:7006 [sender.py:transition_state():463] send defer: 12
-2022-12-14 23:26:11,167 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: defer
-2022-12-14 23:26:11,168 DEBUG   SenderThread:7006 [sender.py:send():303] send: final
-2022-12-14 23:26:11,168 INFO    HandlerThread:7006 [handler.py:handle_request_defer():162] handle defer: 12
-2022-12-14 23:26:11,168 DEBUG   SenderThread:7006 [sender.py:send():303] send: footer
-2022-12-14 23:26:11,168 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: defer
-2022-12-14 23:26:11,169 INFO    SenderThread:7006 [sender.py:send_request_defer():459] handle sender defer: 12
-2022-12-14 23:26:11,173 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-14 23:26:11,173 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: server_info
-2022-12-14 23:26:11,173 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: poll_exit
-2022-12-14 23:26:11,174 DEBUG   SenderThread:7006 [sender.py:send_request():317] send_request: server_info
-2022-12-14 23:26:11,177 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: get_summary
-2022-12-14 23:26:11,179 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: sampled_history
-2022-12-14 23:26:11,667 DEBUG   HandlerThread:7006 [handler.py:handle_request():139] handle_request: shutdown
-2022-12-14 23:26:11,667 INFO    HandlerThread:7006 [handler.py:finish():814] shutting down handler
-2022-12-14 23:26:12,173 INFO    WriterThread:7006 [datastore.py:close():279] close: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/run-3hc18z86.wandb
-2022-12-14 23:26:12,666 INFO    SenderThread:7006 [sender.py:finish():1331] shutting down sender
-2022-12-14 23:26:12,667 INFO    SenderThread:7006 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-14 23:26:12,667 INFO    SenderThread:7006 [file_pusher.py:join():173] waiting for file pusher
-2022-12-14 23:26:15,426 INFO    MainThread:7006 [internal.py:handle_exit():77] Internal process exited
diff --git a/wandb/run-20221214_232341-3hc18z86/logs/debug.log b/wandb/run-20221214_232341-3hc18z86/logs/debug.log
deleted file mode 100644
index 625dd94..0000000
--- a/wandb/run-20221214_232341-3hc18z86/logs/debug.log
+++ /dev/null
@@ -1,32 +0,0 @@
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_setup.py:_flush():68] Configure stats pid to 6996
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/logs/debug.log
-2022-12-14 23:23:41,982 INFO    MainThread:6996 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221214_232341-3hc18z86/logs/debug-internal.log
-2022-12-14 23:23:41,983 INFO    MainThread:6996 [wandb_init.py:init():516] calling init triggers
-2022-12-14 23:23:41,983 INFO    MainThread:6996 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 100000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 5, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-14 23:23:41,983 INFO    MainThread:6996 [wandb_init.py:init():569] starting backend
-2022-12-14 23:23:41,983 INFO    MainThread:6996 [wandb_init.py:init():573] setting up manager
-2022-12-14 23:23:42,004 INFO    MainThread:6996 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-14 23:23:42,009 INFO    MainThread:6996 [wandb_init.py:init():580] backend started and connected
-2022-12-14 23:23:42,015 INFO    MainThread:6996 [wandb_init.py:init():658] updated telemetry
-2022-12-14 23:23:42,027 INFO    MainThread:6996 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-14 23:23:42,584 INFO    MainThread:6996 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-14 23:23:42,719 INFO    MainThread:6996 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-14 23:23:42,720 INFO    MainThread:6996 [wandb_init.py:init():728] starting run threads in backend
-2022-12-14 23:23:44,058 INFO    MainThread:6996 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-14 23:23:44,058 INFO    MainThread:6996 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-14 23:23:44,059 INFO    MainThread:6996 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-14 23:23:44,059 INFO    MainThread:6996 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-14 23:23:44,059 INFO    MainThread:6996 [wandb_init.py:init():765] run started, returning control to user process
-2022-12-14 23:26:07,428 INFO    MainThread:6996 [wandb_run.py:_finish():1753] finishing run drone-mechanics/drone-mechanics-ppo-OpenAIGym/3hc18z86
-2022-12-14 23:26:07,430 INFO    MainThread:6996 [wandb_run.py:_atexit_cleanup():1955] got exitcode: 0
-2022-12-14 23:26:07,431 INFO    MainThread:6996 [wandb_run.py:_restore():1938] restore
-2022-12-14 23:26:07,431 INFO    MainThread:6996 [wandb_run.py:_restore():1944] restore done
-2022-12-14 23:26:12,671 INFO    MainThread:6996 [wandb_run.py:_footer_history_summary_info():3408] rendering history
-2022-12-14 23:26:12,673 INFO    MainThread:6996 [wandb_run.py:_footer_history_summary_info():3440] rendering summary
-2022-12-14 23:26:12,679 INFO    MainThread:6996 [wandb_run.py:_footer_sync_info():3364] logging synced files
diff --git a/wandb/run-20221214_232341-3hc18z86/run-3hc18z86.wandb b/wandb/run-20221214_232341-3hc18z86/run-3hc18z86.wandb
deleted file mode 100644
index 7f3711a..0000000
Binary files a/wandb/run-20221214_232341-3hc18z86/run-3hc18z86.wandb and /dev/null differ
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 14a9a9c..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,476 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# Hint: Please if working on it mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Check calculation of rewards --> correct mean reward over episodes? 
-# 3) Check calculation of advantage 
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-        
-        self.num_episodes = 0
-        done = False
-
-        # collect trajectories
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # collection of trajectories in batches
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-
-                # break out of loop if episode is terminated
-                if done:
-                    # STEP 4: Calculate cummulated reward
-                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-                    trajectory_advantages = advantage
-
-                    self.num_episodes += 1
-                    
-                    # reset values
-                    trajectory_values = []
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-        best_mean_reward = 0
-        for steps in range(self.total_steps):
-        
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = sum(rewards) / self.num_episodes # mean return
-            
-            for _ in range(self.num_epochs):
-                _, curr_log_probs = self.get_values(obs, actions)
-                # STEP 6-7: calculate loss and update weights
-                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-                
-            logging.info('###########################################')
-            logging.info(f"Mean cummulative reward: {mean_reward}")
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 1000              # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml b/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/diff.patch b/wandb/run-20221215_125246-1ml5dns8/files/diff.patch
deleted file mode 100644
index d54ff71..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/diff.patch
+++ /dev/null
@@ -1,570 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..14a9a9c 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -26,16 +27,15 @@ MODEL_PATH = './models/'
- ####### TODO #######
- ####################
- 
--# This is a TODO Section - please mark a todo as (done) if done
-+# Hint: Please if working on it mark a todo as (done) if done
- # 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
--# 2) Fix incorrect calculation of rewards2go --> should be mean reward
--# 3) Fix calculation of Advantage
-+# 2) Check calculation of rewards --> correct mean reward over episodes? 
-+# 3) Check calculation of advantage 
- 
- ####################
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -132,13 +134,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +158,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +187,84 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+        
-+        self.num_episodes = 0
-+        done = False
-+
-+        # collect trajectories
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
--                # tracking of values
-+                # collection of trajectories in batches
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # STEP 4: Calculate cummulated reward
-+                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-+                    trajectory_advantages = advantage
-+
-+                    self.num_episodes += 1
-+                    
-+                    # reset values
-+                    trajectory_values = []
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +272,47 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
-+        best_mean_reward = 0
-+        for steps in range(self.total_steps):
-         
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            # calculate mean reward per episode
-+            mean_reward = sum(rewards) / self.num_episodes # mean return
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
-+            for _ in range(self.num_epochs):
-+                _, curr_log_probs = self.get_values(obs, actions)
-                 # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
--            
-+                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-+                
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Mean cummulative reward: {mean_reward}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +343,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +385,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 1000              # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
--    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +436,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +457,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
-@@ -480,6 +470,7 @@ if __name__ == '__main__':
-     # run training
-     agent.learn()
-     logging.info('### Done ###')
-+
-     # cleanup 
-     env.close()
-     wandb.run.finish() if wandb and wandb.run else None
-\ No newline at end of file
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/output.log b/wandb/run-20221215_125246-1ml5dns8/files/output.log
deleted file mode 100644
index 2cfe264..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/output.log
+++ /dev/null
@@ -1,13 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-Traceback (most recent call last):
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 471, in <module>
-    agent.learn()
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 289, in learn
-    policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 261, in train
-    policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 85, in loss
-    clip_1 = ratio * advantages
-RuntimeError: The size of tensor a (2000) must match the size of tensor b (200) at non-singleton dimension 0
\ No newline at end of file
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt b/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/wandb-metadata.json b/wandb/run-20221215_125246-1ml5dns8/files/wandb-metadata.json
deleted file mode 100644
index 04d5614..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-15T11:52:47.723954",
-    "startedAt": "2022-12-15T11:52:46.964405",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json b/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json
deleted file mode 100644
index a594e25..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb": {"runtime": 8}}
\ No newline at end of file
diff --git a/wandb/run-20221215_125246-1ml5dns8/logs/debug-internal.log b/wandb/run-20221215_125246-1ml5dns8/logs/debug-internal.log
deleted file mode 100644
index 36e4e5c..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/logs/debug-internal.log
+++ /dev/null
@@ -1,183 +0,0 @@
-2022-12-15 12:52:47,031 INFO    StreamThr :8987 [internal.py:wandb_internal():87] W&B internal server running at pid: 8987, started at: 2022-12-15 12:52:47.027090
-2022-12-15 12:52:47,034 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: status
-2022-12-15 12:52:47,035 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: status
-2022-12-15 12:52:47,037 DEBUG   SenderThread:8987 [sender.py:send():303] send: header
-2022-12-15 12:52:47,037 INFO    WriterThread:8987 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/run-1ml5dns8.wandb
-2022-12-15 12:52:47,037 DEBUG   SenderThread:8987 [sender.py:send():303] send: run
-2022-12-15 12:52:47,547 INFO    SenderThread:8987 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files
-2022-12-15 12:52:47,548 INFO    SenderThread:8987 [sender.py:_start_run_threads():928] run started: 1ml5dns8 with start time 1671105166.986829
-2022-12-15 12:52:47,548 DEBUG   SenderThread:8987 [sender.py:send():303] send: summary
-2022-12-15 12:52:47,548 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 12:52:47,551 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: check_version
-2022-12-15 12:52:47,551 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: check_version
-2022-12-15 12:52:47,717 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: run_start
-2022-12-15 12:52:47,723 DEBUG   HandlerThread:8987 [system_info.py:__init__():31] System info init
-2022-12-15 12:52:47,723 DEBUG   HandlerThread:8987 [system_info.py:__init__():46] System info init done
-2022-12-15 12:52:47,723 INFO    HandlerThread:8987 [system_monitor.py:start():150] Starting system monitor
-2022-12-15 12:52:47,723 INFO    SystemMonitor:8987 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-15 12:52:47,723 INFO    HandlerThread:8987 [system_monitor.py:probe():171] Collecting system info
-2022-12-15 12:52:47,723 DEBUG   HandlerThread:8987 [system_info.py:probe():195] Probing system
-2022-12-15 12:52:47,724 INFO    SystemMonitor:8987 [interfaces.py:start():168] Started cpu
-2022-12-15 12:52:47,724 INFO    SystemMonitor:8987 [interfaces.py:start():168] Started disk
-2022-12-15 12:52:47,725 INFO    SystemMonitor:8987 [interfaces.py:start():168] Started gpuapple
-2022-12-15 12:52:47,726 INFO    SystemMonitor:8987 [interfaces.py:start():168] Started memory
-2022-12-15 12:52:47,727 INFO    SystemMonitor:8987 [interfaces.py:start():168] Started network
-2022-12-15 12:52:47,731 DEBUG   HandlerThread:8987 [system_info.py:_probe_git():180] Probing git
-2022-12-15 12:52:47,743 DEBUG   HandlerThread:8987 [system_info.py:_probe_git():188] Probing git done
-2022-12-15 12:52:47,743 DEBUG   HandlerThread:8987 [system_info.py:probe():241] Probing system done
-2022-12-15 12:52:47,743 DEBUG   HandlerThread:8987 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-15T11:52:47.723954', 'startedAt': '2022-12-15T11:52:46.964405', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-15 12:52:47,743 INFO    HandlerThread:8987 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-15 12:52:47,743 INFO    HandlerThread:8987 [system_monitor.py:probe():184] Publishing system info
-2022-12-15 12:52:47,743 DEBUG   HandlerThread:8987 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-15 12:52:47,744 DEBUG   HandlerThread:8987 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-15 12:52:47,744 DEBUG   HandlerThread:8987 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-15 12:52:48,549 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt
-2022-12-15 12:52:48,550 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml
-2022-12-15 12:52:48,550 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json
-2022-12-15 12:52:49,088 DEBUG   HandlerThread:8987 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-15 12:52:49,088 DEBUG   HandlerThread:8987 [system_info.py:_save_code():89] Saving code
-2022-12-15 12:52:49,095 DEBUG   HandlerThread:8987 [system_info.py:_save_code():110] Saving code done
-2022-12-15 12:52:49,095 DEBUG   HandlerThread:8987 [system_info.py:_save_patches():127] Saving git patches
-2022-12-15 12:52:49,174 DEBUG   HandlerThread:8987 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-15 12:52:49,175 INFO    HandlerThread:8987 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-15 12:52:49,207 DEBUG   SenderThread:8987 [sender.py:send():303] send: files
-2022-12-15 12:52:49,208 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-15 12:52:49,208 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-15 12:52:49,208 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-15 12:52:49,211 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:52:49,211 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:52:49,553 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/diff.patch
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-metadata.json
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/code
-2022-12-15 12:52:49,554 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/code/PPO
-2022-12-15 12:52:49,813 DEBUG   SenderThread:8987 [sender.py:send():303] send: telemetry
-2022-12-15 12:52:49,814 INFO    Thread-17 :8987 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp44a0jxjowandb/2xw55l0z-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 12:52:50,184 INFO    Thread-16 :8987 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp44a0jxjowandb/2m4q3l0h-wandb-metadata.json
-2022-12-15 12:52:50,297 INFO    Thread-18 :8987 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmp44a0jxjowandb/17835nft-diff.patch
-2022-12-15 12:52:50,559 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/output.log
-2022-12-15 12:52:52,567 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/output.log
-2022-12-15 12:52:55,872 DEBUG   SenderThread:8987 [sender.py:send():303] send: exit
-2022-12-15 12:52:55,876 INFO    SenderThread:8987 [sender.py:send_exit():442] handling exit code: 1
-2022-12-15 12:52:55,877 INFO    SenderThread:8987 [sender.py:send_exit():444] handling runtime: 8
-2022-12-15 12:52:55,877 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 12:52:55,878 INFO    SenderThread:8987 [sender.py:send_exit():450] send defer
-2022-12-15 12:52:55,878 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,878 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 0
-2022-12-15 12:52:55,878 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,878 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 0
-2022-12-15 12:52:55,878 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 1
-2022-12-15 12:52:55,878 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,878 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 1
-2022-12-15 12:52:55,878 INFO    HandlerThread:8987 [system_monitor.py:finish():160] Stopping system monitor
-2022-12-15 12:52:55,890 DEBUG   SystemMonitor:8987 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-15 12:52:55,890 DEBUG   SystemMonitor:8987 [system_monitor.py:_start():137] Finished system metrics aggregation loop
-2022-12-15 12:52:55,890 DEBUG   SystemMonitor:8987 [system_monitor.py:_start():141] Publishing last batch of metrics
-2022-12-15 12:52:55,891 INFO    HandlerThread:8987 [interfaces.py:finish():175] Joined cpu
-2022-12-15 12:52:55,891 INFO    HandlerThread:8987 [interfaces.py:finish():175] Joined disk
-2022-12-15 12:52:55,891 INFO    HandlerThread:8987 [interfaces.py:finish():175] Joined gpuapple
-2022-12-15 12:52:55,891 INFO    HandlerThread:8987 [interfaces.py:finish():175] Joined memory
-2022-12-15 12:52:55,891 INFO    HandlerThread:8987 [interfaces.py:finish():175] Joined network
-2022-12-15 12:52:55,892 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,892 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 1
-2022-12-15 12:52:55,892 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 2
-2022-12-15 12:52:55,892 DEBUG   SenderThread:8987 [sender.py:send():303] send: telemetry
-2022-12-15 12:52:55,892 DEBUG   SenderThread:8987 [sender.py:send():303] send: stats
-2022-12-15 12:52:55,892 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,892 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 2
-2022-12-15 12:52:55,892 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,892 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 2
-2022-12-15 12:52:55,892 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 3
-2022-12-15 12:52:55,892 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,892 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 3
-2022-12-15 12:52:55,892 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,893 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 3
-2022-12-15 12:52:55,893 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 4
-2022-12-15 12:52:55,893 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,893 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 4
-2022-12-15 12:52:55,893 DEBUG   SenderThread:8987 [sender.py:send():303] send: summary
-2022-12-15 12:52:55,898 INFO    SenderThread:8987 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 12:52:55,898 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,898 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 4
-2022-12-15 12:52:55,898 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 5
-2022-12-15 12:52:55,899 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:55,899 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 5
-2022-12-15 12:52:55,899 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:55,899 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 5
-2022-12-15 12:52:56,150 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 6
-2022-12-15 12:52:56,151 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:56,151 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 6
-2022-12-15 12:52:56,151 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:56,151 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 6
-2022-12-15 12:52:56,588 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/config.yaml
-2022-12-15 12:52:56,589 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json
-2022-12-15 12:52:56,878 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-15 12:52:57,901 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 7
-2022-12-15 12:52:57,901 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: poll_exit
-2022-12-15 12:52:57,902 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:57,903 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 7
-2022-12-15 12:52:57,903 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:57,903 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 7
-2022-12-15 12:52:57,903 INFO    SenderThread:8987 [dir_watcher.py:finish():362] shutting down directory watcher
-2022-12-15 12:52:58,598 INFO    Thread-13 :8987 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/output.log
-2022-12-15 12:52:58,600 INFO    SenderThread:8987 [dir_watcher.py:finish():392] scan: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files
-2022-12-15 12:52:58,601 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml conda-environment.yaml
-2022-12-15 12:52:58,601 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/config.yaml config.yaml
-2022-12-15 12:52:58,604 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/diff.patch diff.patch
-2022-12-15 12:52:58,604 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/output.log output.log
-2022-12-15 12:52:58,607 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt requirements.txt
-2022-12-15 12:52:58,610 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-metadata.json wandb-metadata.json
-2022-12-15 12:52:58,610 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json wandb-summary.json
-2022-12-15 12:52:58,616 INFO    SenderThread:8987 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/code/PPO/ppo_torch/ppo_continuous.py code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 12:52:58,616 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 8
-2022-12-15 12:52:58,616 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:58,616 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 8
-2022-12-15 12:52:58,616 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:58,616 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 8
-2022-12-15 12:52:58,617 INFO    SenderThread:8987 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-15 12:52:59,523 INFO    Thread-20 :8987 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/config.yaml
-2022-12-15 12:52:59,573 INFO    Thread-19 :8987 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/conda-environment.yaml
-2022-12-15 12:52:59,591 INFO    Thread-23 :8987 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/wandb-summary.json
-2022-12-15 12:52:59,625 INFO    Thread-22 :8987 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/requirements.txt
-2022-12-15 12:52:59,629 INFO    Thread-21 :8987 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/files/output.log
-2022-12-15 12:52:59,834 INFO    Thread-12 (_thread_body):8987 [sender.py:transition_state():463] send defer: 9
-2022-12-15 12:52:59,836 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:59,837 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 9
-2022-12-15 12:52:59,837 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:59,837 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 9
-2022-12-15 12:52:59,837 INFO    SenderThread:8987 [file_pusher.py:join():173] waiting for file pusher
-2022-12-15 12:52:59,837 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 10
-2022-12-15 12:52:59,838 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:52:59,838 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 10
-2022-12-15 12:52:59,838 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:52:59,838 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 10
-2022-12-15 12:53:00,015 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 11
-2022-12-15 12:53:00,015 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:53:00,015 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 11
-2022-12-15 12:53:00,015 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:53:00,015 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 11
-2022-12-15 12:53:00,015 INFO    SenderThread:8987 [sender.py:transition_state():463] send defer: 12
-2022-12-15 12:53:00,016 DEBUG   SenderThread:8987 [sender.py:send():303] send: final
-2022-12-15 12:53:00,016 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 12:53:00,016 INFO    HandlerThread:8987 [handler.py:handle_request_defer():162] handle defer: 12
-2022-12-15 12:53:00,016 DEBUG   SenderThread:8987 [sender.py:send():303] send: footer
-2022-12-15 12:53:00,016 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: defer
-2022-12-15 12:53:00,016 INFO    SenderThread:8987 [sender.py:send_request_defer():459] handle sender defer: 12
-2022-12-15 12:53:00,017 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-15 12:53:00,017 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: poll_exit
-2022-12-15 12:53:00,018 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: server_info
-2022-12-15 12:53:00,018 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: get_summary
-2022-12-15 12:53:00,018 DEBUG   SenderThread:8987 [sender.py:send_request():317] send_request: server_info
-2022-12-15 12:53:00,019 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: sampled_history
-2022-12-15 12:53:00,473 INFO    MainThread:8987 [wandb_run.py:_footer_history_summary_info():3408] rendering history
-2022-12-15 12:53:00,473 INFO    MainThread:8987 [wandb_run.py:_footer_history_summary_info():3440] rendering summary
-2022-12-15 12:53:00,473 INFO    MainThread:8987 [wandb_run.py:_footer_sync_info():3364] logging synced files
-2022-12-15 12:53:00,474 DEBUG   HandlerThread:8987 [handler.py:handle_request():139] handle_request: shutdown
-2022-12-15 12:53:00,474 INFO    HandlerThread:8987 [handler.py:finish():814] shutting down handler
-2022-12-15 12:53:01,022 INFO    WriterThread:8987 [datastore.py:close():279] close: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/run-1ml5dns8.wandb
-2022-12-15 12:53:01,477 INFO    SenderThread:8987 [sender.py:finish():1331] shutting down sender
-2022-12-15 12:53:01,478 INFO    SenderThread:8987 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-15 12:53:01,478 INFO    SenderThread:8987 [file_pusher.py:join():173] waiting for file pusher
-2022-12-15 12:53:01,829 INFO    MainThread:8987 [internal.py:handle_exit():77] Internal process exited
diff --git a/wandb/run-20221215_125246-1ml5dns8/logs/debug.log b/wandb/run-20221215_125246-1ml5dns8/logs/debug.log
deleted file mode 100644
index dad831f..0000000
--- a/wandb/run-20221215_125246-1ml5dns8/logs/debug.log
+++ /dev/null
@@ -1,26 +0,0 @@
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_setup.py:_flush():68] Configure stats pid to 8976
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/logs/debug.log
-2022-12-15 12:52:46,973 INFO    MainThread:8976 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125246-1ml5dns8/logs/debug-internal.log
-2022-12-15 12:52:46,974 INFO    MainThread:8976 [wandb_init.py:init():516] calling init triggers
-2022-12-15 12:52:46,974 INFO    MainThread:8976 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 1000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 20, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-15 12:52:46,974 INFO    MainThread:8976 [wandb_init.py:init():569] starting backend
-2022-12-15 12:52:46,974 INFO    MainThread:8976 [wandb_init.py:init():573] setting up manager
-2022-12-15 12:52:46,983 INFO    MainThread:8976 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-15 12:52:46,986 INFO    MainThread:8976 [wandb_init.py:init():580] backend started and connected
-2022-12-15 12:52:46,990 INFO    MainThread:8976 [wandb_init.py:init():658] updated telemetry
-2022-12-15 12:52:47,001 INFO    MainThread:8976 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-15 12:52:47,549 INFO    MainThread:8976 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-15 12:52:47,708 INFO    MainThread:8976 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-15 12:52:47,708 INFO    MainThread:8976 [wandb_init.py:init():728] starting run threads in backend
-2022-12-15 12:52:49,210 INFO    MainThread:8976 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-15 12:52:49,211 INFO    MainThread:8976 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-15 12:52:49,211 INFO    MainThread:8976 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-15 12:52:49,211 INFO    MainThread:8976 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-15 12:52:49,211 INFO    MainThread:8976 [wandb_init.py:init():765] run started, returning control to user process
-2022-12-15 12:53:01,485 WARNING MsgRouterThr:8976 [router.py:message_loop():77] message_loop has been closed
diff --git a/wandb/run-20221215_125246-1ml5dns8/run-1ml5dns8.wandb b/wandb/run-20221215_125246-1ml5dns8/run-1ml5dns8.wandb
deleted file mode 100644
index ec25272..0000000
Binary files a/wandb/run-20221215_125246-1ml5dns8/run-1ml5dns8.wandb and /dev/null differ
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/code/PPO/ppo_torch/ppo_continuous.py b/wandb/run-20221215_125358-1jfkiupl/files/code/PPO/ppo_torch/ppo_continuous.py
deleted file mode 100644
index 14a9a9c..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/code/PPO/ppo_torch/ppo_continuous.py
+++ /dev/null
@@ -1,476 +0,0 @@
-from collections import deque
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-from torch.distributions import MultivariateNormal
-from torch.distributions import Categorical
-from torch.utils.tensorboard import SummaryWriter
-from distutils.util import strtobool
-import numpy as np
-import datetime
-import gym
-import os
-
-import argparse
-
-# logging python
-import logging
-import sys
-
-# monitoring/logging ML
-import wandb
-
-MODEL_PATH = './models/'
-
-####################
-####### TODO #######
-####################
-
-# Hint: Please if working on it mark a todo as (done) if done
-# 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
-# 2) Check calculation of rewards --> correct mean reward over episodes? 
-# 3) Check calculation of advantage 
-
-####################
-####################
-
-class Net(nn.Module):
-    def __init__(self) -> None:
-        super(Net, self).__init__()
-
-class ValueNet(Net):
-    """Setup Value Network (Critic) optimizer"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(ValueNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=1.0)
-        self.relu = nn.ReLU()
-    
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.relu(self.layer1(obs))
-        x = self.relu(self.layer2(x))
-        out = self.layer3(x) # head has linear activation
-        return out
-    
-    def loss(self, obs, rewards):
-        """Objective function defined by mean-squared error"""
-        values = self(obs).squeeze()
-        #return 0.5 * ((rewards - values)**2).mean() # MSE loss
-        return nn.MSELoss()(values, rewards)
-
-class PolicyNet(Net):
-    """Setup Policy Network (Actor)"""
-    def __init__(self, in_dim, out_dim) -> None:
-        super(PolicyNet, self).__init__()
-        self.layer1 = layer_init(nn.Linear(in_dim, 64))
-        self.layer2 = layer_init(nn.Linear(64, 64))
-        self.layer3 = layer_init(nn.Linear(64, out_dim), std=0.01)
-        self.tanh = nn.Tanh()
-
-    def forward(self, obs):
-        if isinstance(obs, np.ndarray):
-            obs = torch.tensor(obs, dtype=torch.float)
-        x = self.tanh(self.layer1(obs))
-        x = self.tanh(self.layer2(x))
-        out = self.layer3(x) # head has linear activation (continuous space)
-        return out
-    
-    def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-        """Make the clipped surrogate objective function to compute policy loss."""
-        ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-        clip_1 = ratio * advantages
-        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-        return policy_loss
-
-
-####################
-####################
-
-
-class PPO_PolicyGradient:
-    """Autonomous agent using Proximal Policy Optimization 
-        as policy gradient method.
-    """
-    def __init__(self, 
-        env, 
-        in_dim, 
-        out_dim,
-        total_steps,
-        max_trajectory_size,
-        trajectory_iterations,
-        num_epochs=5,
-        lr_p=1e-3,
-        lr_v=1e-3,
-        gamma=0.99,
-        epsilon=0.22,
-        adam_eps=1e-5) -> None:
-        
-        # hyperparams
-        self.in_dim = in_dim
-        self.out_dim = out_dim
-        self.total_steps = total_steps
-        self.max_trajectory_size = max_trajectory_size
-        self.trajectory_iterations = trajectory_iterations
-        self.num_epochs = num_epochs
-        self.lr_p = lr_p
-        self.lr_v = lr_v
-        self.gamma = gamma
-        self.epsilon = epsilon
-        self.adam_eps = adam_eps
-
-        # environment
-        self.env = env
-
-        # add net for actor and critic
-        self.policy_net = PolicyNet(self.in_dim, self.out_dim) # Setup Policy Network (Actor)
-        self.value_net = ValueNet(self.in_dim, 1) # Setup Value Network (Critic)
-
-        # add optimizer for actor and critic
-        self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-        self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
-
-    def get_continuous_policy(self, obs):
-        """Make function to compute action distribution in continuous action space."""
-        # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-        # fixes the detection of outliers, allows to capture correlation between features
-        # https://discuss.pytorch.org/t/understanding-log-prob-for-normal-distribution-in-pytorch/73809
-        # 1) Use Normal distribution for continuous space
-        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
-        cov_matrix = torch.diag(torch.full(size=(self.out_dim,), fill_value=0.5))
-        return MultivariateNormal(action_prob, covariance_matrix=cov_matrix)
-
-    def get_action(self, dist):
-        """Make action selection function (outputs actions, sampled from policy)."""
-        action = dist.sample()
-        log_prob = dist.log_prob(action)
-        entropy = dist.entropy()
-        return action, log_prob, entropy
-    
-    def get_values(self, obs, actions):
-        """Make value selection function (outputs values for obs in a batch)."""
-        values = self.value_net(obs).squeeze()
-        dist = self.get_continuous_policy(obs)
-        log_prob = dist.log_prob(actions)
-        return values, log_prob
-
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
-    def step(self, obs):
-        """ Given an observation, get action and probabilities from policy network (actor)"""
-        action_dist = self.get_continuous_policy(obs) 
-        action, log_prob, entropy = self.get_action(action_dist)
-        return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
-
-    def cummulative_reward(self, rewards):
-        # Cumulative rewards: https://gongybable.medium.com/reinforcement-learning-introduction-609040c8be36
-        # G(t) = R(t) + gamma * R(t-1)
-        cum_rewards = []
-        discounted_reward = 0
-        for reward in reversed(rewards):
-            discounted_reward = reward + (self.gamma * discounted_reward)
-            cum_rewards.append(discounted_reward)
-        return cum_rewards
-
-    def advantage_estimate(self, rewards, values, normalized=True):
-        """Simplest advantage calculation"""
-        # STEP 5: compute advantage estimates A_t
-        advantages = rewards - values
-        if normalized:
-            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-        return advantages
-    
-    def generalized_advantage_estimate(self):
-        pass
-    
-    def collect_rollout(self, obs, n_step=1, render=True):
-        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-        
-        self.num_episodes = 0
-        done = False
-
-        # collect trajectories
-        trajectory_obs = []
-        trajectory_actions = []
-        trajectory_action_probs = []
-        trajectory_rewards = []
-        trajectory_advantages = []
-        trajectory_values = []
-
-        # Run an episode 
-        logging.info("Collecting batch trajectories...")
-        for _ in range(n_step):
-
-            while True: 
-                # render gym env
-                if render:
-                    self.env.render(mode='human')
-                    
-                # action logic
-                action, log_probability, _ = self.step(obs)
-                        
-                # STEP 3: collecting set of trajectories D_k by running action 
-                # that was sampled from policy in environment
-                __obs, reward, done, _ = self.env.step(action)
-                value = self.get_value(__obs)
-
-                # collection of trajectories in batches
-                trajectory_obs.append(obs)
-                trajectory_actions.append(action)
-                trajectory_action_probs.append(log_probability)
-                trajectory_rewards.append(reward)
-                trajectory_values.append(value.detach())
-                    
-                obs = __obs
-
-                # break out of loop if episode is terminated
-                if done:
-                    # STEP 4: Calculate cummulated reward
-                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-                    trajectory_advantages = advantage
-
-                    self.num_episodes += 1
-                    
-                    # reset values
-                    trajectory_values = []
-                    obs = self.env.reset()
-                    break
-        
-        # convert trajectories to torch tensors
-        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-
-        return obs, actions, log_probs, rewards, advantages
-                
-
-    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-        """Calculate loss and update weights of both networks."""
-        logging.info("Updating network parameter...")
-        # loss of the policy network
-        self.policy_net_optim.zero_grad() # reset optimizer
-        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-        policy_loss.backward() # backpropagation
-        self.policy_net_optim.step() # single optimization step (updates parameter)
-
-        # loss of the value network
-        self.value_net_optim.zero_grad() # reset optimizer
-        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
-        value_loss.backward()
-        self.value_net_optim.step()
-
-        return policy_loss, value_loss
-
-    def learn(self):
-        """"""
-        best_mean_reward = 0
-        for steps in range(self.total_steps):
-        
-            next_obs = self.env.reset()
-            # Collect trajectory
-            # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
-            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-            
-            # calculate mean reward per episode
-            mean_reward = sum(rewards) / self.num_episodes # mean return
-            
-            for _ in range(self.num_epochs):
-                _, curr_log_probs = self.get_values(obs, actions)
-                # STEP 6-7: calculate loss and update weights
-                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-                
-            logging.info('###########################################')
-            logging.info(f"Mean cummulative reward: {mean_reward}")
-            logging.info(f"Policy loss: {policy_loss}")
-            logging.info(f"Value loss: {value_loss}")
-            logging.info(f"Time step: {steps}")
-            logging.info('###########################################\n')
-            
-            # logging for monitoring in W&B
-            wandb.log({
-                'time/step': steps,
-                'loss/policy loss': policy_loss,
-                'loss/value loss': value_loss,
-                'reward/mean return': mean_reward})
-            
-            # store model in checkpoints
-            if mean_reward > best_mean_reward:
-                env_name = env.unwrapped.spec.id
-                torch.save({
-                    'epoch': steps,
-                    'model_state_dict': self.policy_net.state_dict(),
-                    'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__policyNet')
-                torch.save({
-                    'epoch': steps,
-                    'model_state_dict': self.value_net.state_dict(),
-                    'optimizer_state_dict': self.value_net_optim.state_dict(),
-                    'loss': policy_loss,
-                    }, f'{MODEL_PATH}{env_name}__valueNet')
-                best_mean_reward = mean_reward
-
-####################
-####################
-
-def arg_parser():
-    parser = argparse.ArgumentParser()
-    # fmt: off
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
-        help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="HalfCheetahBulletEnv-v0",
-        help="the id of the gym environment")
-    parser.add_argument("--learning-rate", type=float, default=3e-4,
-        help="the learning rate of the optimizer")
-    parser.add_argument("--seed", type=int, default=1,
-        help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=2000000,
-        help="total timesteps of the experiments")
-    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, `torch.backends.cudnn.deterministic=False`")
-    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
-        help="if toggled, cuda will be enabled by default")
-    
-    # Parse arguments if they are given
-    args = parser.parse_args()
-    return args
-
-def make_env(env_id='Pendulum-v1', seed=42):
-    # TODO: Needs to be parallized for parallel simulation
-    env = gym.make(env_id)
-    # gym wrapper
-    # env = gym.wrappers.ClipAction(env)
-    # env = gym.wrappers.NormalizeObservation(env)
-    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-    # env = gym.wrappers.NormalizeReward(env)
-    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-    # seed env for reproducability
-    env.seed(seed)
-    env.action_space.seed(seed)
-    env.observation_space.seed(seed)
-    return env
-
-def make_vec_env(num_env=1):
-    """Create a vectorized environment for parallelized training."""
-    pass
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    """Initialize the hidden layers with orthogonal initialization"""
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-def train():
-    # TODO Add Checkpoints to load model 
-    pass
-
-def test():
-    pass
-
-if __name__ == '__main__':
-    
-    if not os.path.exists(MODEL_PATH):
-        os.makedirs(MODEL_PATH)
-
-    args = arg_parser()
-    # Hyperparameter
-    unity_file_name = ''            # name of unity environment
-    total_steps = 1000              # time steps to train agent
-    max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-    trajectory_iterations = 10      # number of batches of episodes
-    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-    learning_rate_p = 1e-4          # learning rate for policy network
-    learning_rate_v = 1e-3          # learning rate for value network
-    gamma = 0.99                    # discount factor
-    adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-    epsilon = 0.2                   # clipping factor
-    env_name = 'Pendulum-v1'        # name of OpenAI gym environment
-    seed = 42                       # seed gym, env, torch, numpy 
-    #'CartPole-v1' 'Pendulum-v1', 'MountainCar-v0'
-
-    # Configure logger
-    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
-    
-    # seed gym, torch and numpy
-    env = make_env(env_name, seed=seed)
-
-    torch.manual_seed(seed)
-    np.random.seed(seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    # get correct device
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # get dimensions of obs (what goes in?)
-    # and actions (what goes out?)
-    obs_shape = env.observation_space.shape
-    act_shape = env.action_space.shape
-
-    logging.info(f'env observation space: {obs_shape}')
-    logging.info(f'env action space: {act_shape}')
-    
-    obs_dim = obs_shape[0] 
-    act_dim = act_shape[0] # 2 at CartPole
-
-    logging.info(f'env observation dim: {obs_dim}')
-    logging.info(f'env action dim: {act_dim}')
-    
-    # upper and lower bound describing the values our obs can take
-    logging.info(f'upper bound for env observation: {env.observation_space.high}')
-    logging.info(f'lower bound for env observation: {env.observation_space.low}')
-    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-   
-    # Monitoring with W&B
-    wandb.init(
-    project=f'drone-mechanics-ppo-OpenAIGym',
-    entity='drone-mechanics',
-    sync_tensorboard=True,
-    config={ # stores hyperparams in job
-            'total number of steps': total_steps,
-            'max sampled trajectories': max_trajectory_size,
-            'batches per episode': trajectory_iterations,
-            'number of epochs for update': num_epochs,
-            'input layer size': obs_dim,
-            'output layer size': act_dim,
-            'learning rate (policy net)': learning_rate_p,
-            'learning rate (value net)': learning_rate_v,
-            'epsilon (adam optimizer)': adam_epsilon,
-            'gamma (discount)': gamma,
-            'epsilon (clipping)': epsilon
-        },
-    name=f"{env_name}__{current_time}",
-    # monitor_gym=True,
-    save_code=True,
-    )
-
-    agent = PPO_PolicyGradient(
-                env, 
-                in_dim=obs_dim, 
-                out_dim=act_dim,
-                total_steps=total_steps,
-                max_trajectory_size=max_trajectory_size,
-                trajectory_iterations=trajectory_iterations,
-                num_epochs=num_epochs,
-                lr_p=learning_rate_p,
-                lr_v=learning_rate_v,
-                gamma=gamma,
-                epsilon=epsilon,
-                adam_eps=adam_epsilon)
-    
-    # run training
-    agent.learn()
-    logging.info('### Done ###')
-
-    # cleanup 
-    env.close()
-    wandb.run.finish() if wandb and wandb.run else None
\ No newline at end of file
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/conda-environment.yaml b/wandb/run-20221215_125358-1jfkiupl/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/config.yaml b/wandb/run-20221215_125358-1jfkiupl/files/config.yaml
deleted file mode 100644
index 5d8913f..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671105238.697344
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 20
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 1000
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/diff.patch b/wandb/run-20221215_125358-1jfkiupl/files/diff.patch
deleted file mode 100644
index d54ff71..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/diff.patch
+++ /dev/null
@@ -1,570 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..14a9a9c 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -26,16 +27,15 @@ MODEL_PATH = './models/'
- ####### TODO #######
- ####################
- 
--# This is a TODO Section - please mark a todo as (done) if done
-+# Hint: Please if working on it mark a todo as (done) if done
- # 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
--# 2) Fix incorrect calculation of rewards2go --> should be mean reward
--# 3) Fix calculation of Advantage
-+# 2) Check calculation of rewards --> correct mean reward over episodes? 
-+# 3) Check calculation of advantage 
- 
- ####################
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -132,13 +134,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +158,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -189,80 +187,84 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+        
-+        self.num_episodes = 0
-+        done = False
-+
-+        # collect trajectories
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
--                # tracking of values
-+                # collection of trajectories in batches
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # STEP 4: Calculate cummulated reward
-+                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-+                    trajectory_advantages = advantage
-+
-+                    self.num_episodes += 1
-+                    
-+                    # reset values
-+                    trajectory_values = []
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +272,47 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
-+        best_mean_reward = 0
-+        for steps in range(self.total_steps):
-         
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            # calculate mean reward per episode
-+            mean_reward = sum(rewards) / self.num_episodes # mean return
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
-+            for _ in range(self.num_epochs):
-+                _, curr_log_probs = self.get_values(obs, actions)
-                 # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
--            
-+                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-+                
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Mean cummulative reward: {mean_reward}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +343,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +385,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 1000              # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
--    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +436,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +457,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
-@@ -480,6 +470,7 @@ if __name__ == '__main__':
-     # run training
-     agent.learn()
-     logging.info('### Done ###')
-+
-     # cleanup 
-     env.close()
-     wandb.run.finish() if wandb and wandb.run else None
-\ No newline at end of file
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/output.log b/wandb/run-20221215_125358-1jfkiupl/files/output.log
deleted file mode 100644
index 5202b7c..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/output.log
+++ /dev/null
@@ -1,3 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/requirements.txt b/wandb/run-20221215_125358-1jfkiupl/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/wandb-metadata.json b/wandb/run-20221215_125358-1jfkiupl/files/wandb-metadata.json
deleted file mode 100644
index c4005c4..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-15T11:53:59.533741",
-    "startedAt": "2022-12-15T11:53:58.653552",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221215_125358-1jfkiupl/files/wandb-summary.json b/wandb/run-20221215_125358-1jfkiupl/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/wandb/run-20221215_125358-1jfkiupl/logs/debug-internal.log b/wandb/run-20221215_125358-1jfkiupl/logs/debug-internal.log
deleted file mode 100644
index b70597a..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/logs/debug-internal.log
+++ /dev/null
@@ -1,185 +0,0 @@
-2022-12-15 12:53:58,713 INFO    StreamThr :9116 [internal.py:wandb_internal():87] W&B internal server running at pid: 9116, started at: 2022-12-15 12:53:58.712590
-2022-12-15 12:53:58,716 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: status
-2022-12-15 12:53:58,718 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: status
-2022-12-15 12:53:58,722 DEBUG   SenderThread:9116 [sender.py:send():303] send: header
-2022-12-15 12:53:58,722 DEBUG   SenderThread:9116 [sender.py:send():303] send: run
-2022-12-15 12:53:58,728 INFO    WriterThread:9116 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/run-1jfkiupl.wandb
-2022-12-15 12:53:59,273 INFO    SenderThread:9116 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files
-2022-12-15 12:53:59,273 INFO    SenderThread:9116 [sender.py:_start_run_threads():928] run started: 1jfkiupl with start time 1671105238.697344
-2022-12-15 12:53:59,274 DEBUG   SenderThread:9116 [sender.py:send():303] send: summary
-2022-12-15 12:53:59,274 INFO    SenderThread:9116 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 12:53:59,276 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: check_version
-2022-12-15 12:53:59,276 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: check_version
-2022-12-15 12:53:59,525 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: run_start
-2022-12-15 12:53:59,532 DEBUG   HandlerThread:9116 [system_info.py:__init__():31] System info init
-2022-12-15 12:53:59,532 DEBUG   HandlerThread:9116 [system_info.py:__init__():46] System info init done
-2022-12-15 12:53:59,532 INFO    HandlerThread:9116 [system_monitor.py:start():150] Starting system monitor
-2022-12-15 12:53:59,533 INFO    HandlerThread:9116 [system_monitor.py:probe():171] Collecting system info
-2022-12-15 12:53:59,533 DEBUG   HandlerThread:9116 [system_info.py:probe():195] Probing system
-2022-12-15 12:53:59,533 INFO    SystemMonitor:9116 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-15 12:53:59,534 INFO    SystemMonitor:9116 [interfaces.py:start():168] Started cpu
-2022-12-15 12:53:59,535 INFO    SystemMonitor:9116 [interfaces.py:start():168] Started disk
-2022-12-15 12:53:59,537 INFO    SystemMonitor:9116 [interfaces.py:start():168] Started gpuapple
-2022-12-15 12:53:59,540 INFO    SystemMonitor:9116 [interfaces.py:start():168] Started memory
-2022-12-15 12:53:59,544 INFO    SystemMonitor:9116 [interfaces.py:start():168] Started network
-2022-12-15 12:53:59,545 DEBUG   HandlerThread:9116 [system_info.py:_probe_git():180] Probing git
-2022-12-15 12:53:59,561 DEBUG   HandlerThread:9116 [system_info.py:_probe_git():188] Probing git done
-2022-12-15 12:53:59,561 DEBUG   HandlerThread:9116 [system_info.py:probe():241] Probing system done
-2022-12-15 12:53:59,561 DEBUG   HandlerThread:9116 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-15T11:53:59.533741', 'startedAt': '2022-12-15T11:53:58.653552', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-15 12:53:59,561 INFO    HandlerThread:9116 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-15 12:53:59,561 INFO    HandlerThread:9116 [system_monitor.py:probe():184] Publishing system info
-2022-12-15 12:53:59,561 DEBUG   HandlerThread:9116 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-15 12:53:59,562 DEBUG   HandlerThread:9116 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-15 12:53:59,562 DEBUG   HandlerThread:9116 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-15 12:54:00,282 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/conda-environment.yaml
-2022-12-15 12:54:00,283 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/requirements.txt
-2022-12-15 12:54:00,283 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/wandb-summary.json
-2022-12-15 12:54:00,781 DEBUG   HandlerThread:9116 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-15 12:54:00,781 DEBUG   HandlerThread:9116 [system_info.py:_save_code():89] Saving code
-2022-12-15 12:54:00,789 DEBUG   HandlerThread:9116 [system_info.py:_save_code():110] Saving code done
-2022-12-15 12:54:00,789 DEBUG   HandlerThread:9116 [system_info.py:_save_patches():127] Saving git patches
-2022-12-15 12:54:00,854 DEBUG   HandlerThread:9116 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-15 12:54:00,855 INFO    HandlerThread:9116 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-15 12:54:00,942 DEBUG   SenderThread:9116 [sender.py:send():303] send: files
-2022-12-15 12:54:00,942 INFO    SenderThread:9116 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-15 12:54:00,942 INFO    SenderThread:9116 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-15 12:54:00,943 INFO    SenderThread:9116 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-15 12:54:00,955 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:54:00,955 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:54:01,282 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/conda-environment.yaml
-2022-12-15 12:54:01,282 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/diff.patch
-2022-12-15 12:54:01,282 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 12:54:01,283 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/wandb-metadata.json
-2022-12-15 12:54:01,283 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/code/PPO
-2022-12-15 12:54:01,283 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/code/PPO/ppo_torch
-2022-12-15 12:54:01,284 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/code
-2022-12-15 12:54:01,307 DEBUG   SenderThread:9116 [sender.py:send():303] send: telemetry
-2022-12-15 12:54:01,314 INFO    Thread-23 :9116 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpq9b1_kdiwandb/1ezlnc1z-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 12:54:01,746 INFO    Thread-22 :9116 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpq9b1_kdiwandb/18p5kksx-wandb-metadata.json
-2022-12-15 12:54:01,785 INFO    Thread-24 :9116 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpq9b1_kdiwandb/fs69hxmw-diff.patch
-2022-12-15 12:54:02,288 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/output.log
-2022-12-15 12:54:04,299 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/output.log
-2022-12-15 12:54:16,317 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:54:16,321 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:54:30,428 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/config.yaml
-2022-12-15 12:54:31,576 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:54:31,576 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:54:46,878 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:54:46,879 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:54:59,545 DEBUG   SystemMonitor:9116 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-15 12:54:59,554 DEBUG   SenderThread:9116 [sender.py:send():303] send: telemetry
-2022-12-15 12:54:59,554 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:55:01,606 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/config.yaml
-2022-12-15 12:55:01,607 INFO    Thread-19 :9116 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/files/output.log
-2022-12-15 12:55:02,137 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:55:02,138 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:55:17,444 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:55:17,445 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:55:29,565 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:55:32,794 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:55:32,794 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:55:48,045 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:55:48,045 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:55:59,569 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:56:03,289 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:56:03,290 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:56:18,562 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:56:18,567 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:56:29,582 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:56:33,823 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:56:33,824 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:56:49,157 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:56:49,158 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:56:59,581 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:57:04,451 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:57:04,457 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:57:19,755 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:57:19,755 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:57:29,591 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:57:34,990 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:57:34,991 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:57:50,270 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:57:50,271 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:57:59,590 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:58:05,523 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:58:05,523 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:58:20,946 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:58:20,952 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:58:29,601 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:58:36,263 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:58:36,264 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:58:51,545 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:58:51,550 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:58:59,611 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:59:06,802 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:59:06,806 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:59:22,060 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:59:22,065 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:59:29,607 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 12:59:37,337 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:59:37,338 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:59:52,632 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 12:59:52,636 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 12:59:59,621 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:00:07,881 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:00:07,882 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:00:23,151 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:00:23,156 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:00:29,626 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:00:38,402 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:00:38,404 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:00:53,660 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:00:53,665 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:00:59,629 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:01:08,938 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:01:08,942 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:01:24,422 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:01:24,427 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:01:29,641 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:01:39,762 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:01:39,762 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:01:55,054 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:01:55,056 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:01:59,645 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:02:10,385 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:02:10,391 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:02:25,681 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:02:25,682 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:02:29,663 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:02:42,578 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:02:42,583 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:02:57,834 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:02:57,839 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:02:59,666 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:03:13,086 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:03:13,086 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:03:28,385 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:03:28,386 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:03:29,668 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:03:43,637 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:03:43,643 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:03:58,934 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:03:58,935 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:03:59,671 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:04:14,265 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:04:14,266 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:04:29,583 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:04:29,591 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:04:29,852 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:04:44,874 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:04:44,879 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:04:59,682 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:05:00,136 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:05:00,136 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:05:15,643 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:05:15,645 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:05:29,682 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:05:30,975 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:05:30,976 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:05:46,320 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:05:46,324 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:05:59,698 DEBUG   SenderThread:9116 [sender.py:send():303] send: stats
-2022-12-15 13:06:01,587 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:06:01,588 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:06:16,900 DEBUG   HandlerThread:9116 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:06:16,900 DEBUG   SenderThread:9116 [sender.py:send_request():317] send_request: stop_status
diff --git a/wandb/run-20221215_125358-1jfkiupl/logs/debug.log b/wandb/run-20221215_125358-1jfkiupl/logs/debug.log
deleted file mode 100644
index a516fb8..0000000
--- a/wandb/run-20221215_125358-1jfkiupl/logs/debug.log
+++ /dev/null
@@ -1,25 +0,0 @@
-2022-12-15 12:53:58,662 INFO    MainThread:9106 [wandb_setup.py:_flush():68] Configure stats pid to 9106
-2022-12-15 12:53:58,662 INFO    MainThread:9106 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-15 12:53:58,662 INFO    MainThread:9106 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-15 12:53:58,662 INFO    MainThread:9106 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-15 12:53:58,662 INFO    MainThread:9106 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-15 12:53:58,663 INFO    MainThread:9106 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/logs/debug.log
-2022-12-15 12:53:58,663 INFO    MainThread:9106 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_125358-1jfkiupl/logs/debug-internal.log
-2022-12-15 12:53:58,663 INFO    MainThread:9106 [wandb_init.py:init():516] calling init triggers
-2022-12-15 12:53:58,663 INFO    MainThread:9106 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 1000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 20, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-15 12:53:58,664 INFO    MainThread:9106 [wandb_init.py:init():569] starting backend
-2022-12-15 12:53:58,664 INFO    MainThread:9106 [wandb_init.py:init():573] setting up manager
-2022-12-15 12:53:58,690 INFO    MainThread:9106 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-15 12:53:58,697 INFO    MainThread:9106 [wandb_init.py:init():580] backend started and connected
-2022-12-15 12:53:58,704 INFO    MainThread:9106 [wandb_init.py:init():658] updated telemetry
-2022-12-15 12:53:58,716 INFO    MainThread:9106 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-15 12:53:59,275 INFO    MainThread:9106 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-15 12:53:59,489 INFO    MainThread:9106 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-15 12:53:59,489 INFO    MainThread:9106 [wandb_init.py:init():728] starting run threads in backend
-2022-12-15 12:54:00,949 INFO    MainThread:9106 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-15 12:54:00,950 INFO    MainThread:9106 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-15 12:54:00,950 INFO    MainThread:9106 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-15 12:54:00,950 INFO    MainThread:9106 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-15 12:54:00,951 INFO    MainThread:9106 [wandb_init.py:init():765] run started, returning control to user process
diff --git a/wandb/run-20221215_125358-1jfkiupl/run-1jfkiupl.wandb b/wandb/run-20221215_125358-1jfkiupl/run-1jfkiupl.wandb
deleted file mode 100644
index 30586a9..0000000
Binary files a/wandb/run-20221215_125358-1jfkiupl/run-1jfkiupl.wandb and /dev/null differ
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml b/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml
deleted file mode 100644
index c783797..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml
+++ /dev/null
@@ -1,146 +0,0 @@
-name: pytorch-ppo-research
-channels:
-  - conda-forge
-dependencies:
-  - aom=3.5.0=h7ea286d_0
-  - box2d-py=2.3.8=py310h0f1eb42_7
-  - bzip2=1.0.8=h3422bc3_4
-  - c-ares=1.18.1=h3422bc3_0
-  - ca-certificates=2022.9.24=h4653dfc_0
-  - cairo=1.16.0=h73a0509_1014
-  - cffi=1.15.1=py310h2399d43_2
-  - cloudpickle=2.2.0=pyhd8ed1ab_0
-  - expat=2.5.0=hb7217d7_0
-  - ffmpeg=4.4.2=gpl_hf4c414c_110
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.1=h82840c6_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - freetype=2.12.1=hd633e50_1
-  - future=0.18.2=pyhd8ed1ab_6
-  - gettext=0.21.1=h0186832_0
-  - gmp=6.2.1=h9f76cd9_0
-  - gnutls=3.7.8=h9f1a10d_0
-  - graphite2=1.3.13=h9f76cd9_1001
-  - gym=0.21.0=py310hbe9552e_2
-  - gym-all=0.21.0=py310hb6292c7_2
-  - gym-box2d=0.21.0=py310hb6292c7_2
-  - gym-classic_control=0.21.0=py310hb6292c7_2
-  - gym-other=0.21.0=py310hb6292c7_2
-  - gym-toy_text=0.21.0=py310hb6292c7_2
-  - harfbuzz=5.3.0=hddbc195_0
-  - hdf5=1.12.2=nompi_h33dac16_100
-  - icu=70.1=h6b3803e_0
-  - jasper=2.0.33=hba35424_0
-  - jpeg=9e=he4db4b2_2
-  - krb5=1.19.3=he492e65_0
-  - lame=3.100=h1a8c8d9_1003
-  - lerc=4.0.0=h9a09cb3_0
-  - libblas=3.9.0=16_osxarm64_openblas
-  - libcblas=3.9.0=16_osxarm64_openblas
-  - libcurl=7.86.0=h1c293e1_1
-  - libcxx=14.0.6=h2692d47_0
-  - libdeflate=1.14=h1a8c8d9_0
-  - libedit=3.1.20191231=hc8eb9b7_2
-  - libev=4.33=h642e427_1
-  - libffi=3.4.2=h3422bc3_5
-  - libgfortran=5.0.0=11_3_0_hd922786_26
-  - libgfortran5=11.3.0=hdaf2cc0_26
-  - libglib=2.74.1=h4646484_1
-  - libiconv=1.17=he4db4b2_0
-  - libidn2=2.3.4=h1a8c8d9_0
-  - liblapack=3.9.0=16_osxarm64_openblas
-  - liblapacke=3.9.0=16_osxarm64_openblas
-  - libnghttp2=1.47.0=h519802c_1
-  - libopenblas=0.3.21=openmp_hc731615_3
-  - libopencv=4.6.0=py310hb63fde9_4
-  - libpng=1.6.39=h76d750c_0
-  - libprotobuf=3.21.9=hb5ab8b9_0
-  - libsqlite=3.40.0=h76d750c_0
-  - libssh2=1.10.0=h7a5bd25_3
-  - libtasn1=4.19.0=h1a8c8d9_0
-  - libtiff=4.4.0=hfa0b094_4
-  - libunistring=0.9.10=h3422bc3_0
-  - libvpx=1.11.0=hc470f4d_3
-  - libwebp-base=1.2.4=h57fd34a_0
-  - libxml2=2.10.3=h87b0503_0
-  - libzlib=1.2.13=h03a7124_4
-  - llvm-openmp=15.0.5=h7cfbb63_0
-  - lz4=4.0.2=py310ha6df754_0
-  - lz4-c=1.9.3=hbdafb3b_1
-  - ncurses=6.3=h07bb92c_1
-  - nettle=3.8.1=h63371fa_1
-  - ninja=1.11.0=hf86a087_0
-  - numpy=1.23.5=py310h5d7c261_0
-  - opencv=4.6.0=py310hb6292c7_4
-  - openh264=2.3.1=hb7217d7_1
-  - openssl=3.0.7=h03a7124_0
-  - p11-kit=0.24.1=h29577a5_0
-  - pcre2=10.40=hb34f9b4_0
-  - pip=22.3.1=pyhd8ed1ab_0
-  - pixman=0.40.0=h27ca646_0
-  - py-opencv=4.6.0=py310h69fb684_4
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyglet=1.5.27=py310hbe9552e_1
-  - python=3.10.8=h3ba56d0_0_cpython
-  - python_abi=3.10=3_cp310
-  - pytorch=1.12.1=cpu_py310h7410233_1
-  - readline=8.1.2=h46ed386_0
-  - scipy=1.9.3=py310ha0d8a01_2
-  - setuptools=65.5.1=pyhd8ed1ab_0
-  - sleef=3.5.1=h156473d_2
-  - svt-av1=1.3.0=h7ea286d_0
-  - tk=8.6.12=he1e0b03_0
-  - typing_extensions=4.4.0=pyha770c72_0
-  - tzdata=2022f=h191b570_0
-  - wheel=0.38.4=pyhd8ed1ab_0
-  - x264=1!164.3095=h57fd34a_2
-  - x265=3.5=hbc6ce65_3
-  - xz=5.2.6=h57fd34a_0
-  - zlib=1.2.13=h03a7124_4
-  - zstd=1.5.2=h8128057_4
-  - pip:
-      - absl-py==1.3.0
-      - cachetools==5.2.0
-      - certifi==2022.9.24
-      - charset-normalizer==2.1.1
-      - click==8.1.3
-      - docker-pycreds==0.4.0
-      - gitdb==4.0.10
-      - gitpython==3.1.29
-      - google-auth==2.15.0
-      - google-auth-oauthlib==0.4.6
-      - grpcio==1.51.1
-      - idna==3.4
-      - markdown==3.4.1
-      - markupsafe==2.1.1
-      - oauthlib==3.2.2
-      - pandas==1.5.2
-      - pathtools==0.1.2
-      - promise==2.3
-      - protobuf==3.20.3
-      - psutil==5.9.4
-      - pyasn1==0.4.8
-      - pyasn1-modules==0.2.8
-      - python-dateutil==2.8.2
-      - pytz==2022.6
-      - pyyaml==6.0
-      - requests==2.28.1
-      - requests-oauthlib==1.3.1
-      - rsa==4.9
-      - sentry-sdk==1.11.1
-      - setproctitle==1.3.2
-      - shortuuid==1.0.11
-      - six==1.16.0
-      - smmap==5.0.0
-      - tensorboard==2.11.0
-      - tensorboard-data-server==0.6.1
-      - tensorboard-plugin-wit==1.8.1
-      - torch-tb-profiler==0.4.0
-      - urllib3==1.26.13
-      - wandb==0.13.6
-      - werkzeug==2.2.2
-prefix: /Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml b/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml
deleted file mode 100644
index a8bd4cc..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.6
-    code_path: code/PPO/ppo_torch/ppo_continuous.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.8
-    start_time: 1671105987.943827
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.8
-      5: 0.13.6
-      8:
-      - 4
-      - 5
-batches per episode:
-  desc: null
-  value: 10
-epsilon (adam optimizer):
-  desc: null
-  value: 1.0e-05
-epsilon (clipping):
-  desc: null
-  value: 0.2
-gamma (discount):
-  desc: null
-  value: 0.99
-input layer size:
-  desc: null
-  value: 3
-learning rate (policy net):
-  desc: null
-  value: 0.0001
-learning rate (value net):
-  desc: null
-  value: 0.001
-max sampled trajectories:
-  desc: null
-  value: 10000
-number of epochs for update:
-  desc: null
-  value: 20
-output layer size:
-  desc: null
-  value: 1
-total number of steps:
-  desc: null
-  value: 1000
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/diff.patch b/wandb/run-20221215_130627-g5gz7u2b/files/diff.patch
deleted file mode 100644
index 73cee05..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/diff.patch
+++ /dev/null
@@ -1,579 +0,0 @@
-diff --git a/PPO/asp_exercises/ppo_torch.py b/PPO/asp_exercises/ppo_torch.py
-index 3fbb130..09507fb 100644
---- a/PPO/asp_exercises/ppo_torch.py
-+++ b/PPO/asp_exercises/ppo_torch.py
-@@ -40,7 +40,8 @@ class PolicyNet(Net):
- class PPO:
-   """ Autonomous agent using vanilla policy gradient. """
-   def __init__(self, env, seed=42,  gamma=0.99):
--    self.env = env; self.gamma = gamma;                       # Setup env and discount 
-+    self.env = env; 
-+    self.gamma = gamma;                       # Setup env and discount 
-     th.manual_seed(seed);np.random.seed(seed);env.seed(seed)  # Seed Torch, numpy and gym
-     # Keep track of previous rewards and performed steps to calcule the mean Return metric
-     self._episode, self.ep_returns, self.num_steps = [], deque(maxlen=100), 0
-@@ -52,7 +53,8 @@ class PPO:
- 
-   def step(self, obs):
-     """ Given an observation, get action and probs from policy and values from critc"""
--    with th.no_grad(): (a, prob), v = self.pi(obs), self.vf(obs)
-+    with th.no_grad(): 
-+      (a, prob), v = self.pi(obs), self.vf(obs)
-     return a.numpy(), v.numpy()
- 
-   def policy(self, obs, det=True): return self.pi(th.tensor(obs), det=det)[0].numpy()
-@@ -60,7 +62,8 @@ class PPO:
-   def finish_episode(self):
-     """Process self._episode & reset self.env, Returns (s,a,G,V)-Tuple and new inital state"""
-     s, a, R, V = (np.array(e) for e in zip(*self._episode)) # Get trajectories from rollout
--    self.ep_returns.append(sum(R)); self._episode = []      # Add epoisode return to buffer & reset
-+    self.ep_returns.append(sum(R))
-+    self._episode = []                                      # Add epoisode return to buffer & reset
-     return (s,a,R,V), self.env.reset()                      # state, action, Return, Value Tensors + new state
- 
-   def collect_rollout(self, state, n_step=1):               # n_step=1 -> Temporal difference 
-@@ -71,8 +74,11 @@ class PPO:
-       _state, reward, done, _ = self.env.step(action)       # Execute selected action
-       self._episode.append((state, action, reward, value))  # Save experience to agent episode for logging
-       rollout.append((state, action, reward, value))        # Integrate new experience into rollout
--      state = _state; self.num_steps += 1                   # Update state & step
--      if done: _, state = self.finish_episode()             # Reset env if done 
-+      state = _state; 
-+      self.num_steps += 1                                   # Update state & step
-+      if done: 
-+        _, state = self.finish_episode()             # Reset env if done 
-+    
-     s,a,R,V = (np.array(e) for e in zip(*rollout))          # Get trajectories from rollout
-     value = self.step(th.tensor(state))[1]                  # Get value of next state 
-     A = G = cumulate_discount(R, self.gamma)                # REINFORCE Advantages (TODO 4-1)
-@@ -108,7 +114,8 @@ if __name__ == '__main__':
-   dir = f"./results/run_{len(next(os.walk('./results'))[1])}"
-   env = gym.wrappers.Monitor(agent.env, dir, force=True)
-   state, done = env.reset(), False
--  while not done: state,_,done,_ = env.step(agent.policy(state))
-+  while not done: 
-+    state,_,done,_ = env.step(agent.policy(state))
-   plt.plot(*zip(*stats)); plt.title("Progress")
-   plt.xlabel("Timestep"); plt.ylabel("Mean Return")
-   plt.savefig(f"{dir}/training.png")
-diff --git a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 b/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2
-deleted file mode 100644
-index dc73266..0000000
-Binary files a/PPO/logs/gradient_tape/CartPole-v120221210-004118/train/events.out.tfevents.1670629278.Janinas-MacBook-Pro.local.88222.0.v2 and /dev/null differ
-diff --git a/PPO/ppo_tf/ppo_tf.py b/PPO/ppo_tf/ppo_tf.py
-index 556132f..72639b9 100644
---- a/PPO/ppo_tf/ppo_tf.py
-+++ b/PPO/ppo_tf/ppo_tf.py
-@@ -86,11 +86,9 @@ class ValueNetwork(tf.keras.Model):
- # Setup the actor/critic networks
- policy_net = PolicyNetwork()
- value_net = ValueNetwork()
--
- #policy_net = tf.keras.models.load_model(f"{env_name}_policy_model")
- #value_net = tf.keras.models.load_model(f"{env_name}_value_model")
- 
--
- #
- #
- #
-@@ -161,6 +159,12 @@ num_passed_timesteps = 0
- sum_rewards = 0
- num_episodes = 1
- last_mean_reward = 0
-+
-+'''
-+    Main training loop.
-+
-+    The agent is trained for @num_total_steps times.
-+'''
- for epochs in range(num_total_steps):
- 
-     episodes = []
-@@ -175,25 +179,23 @@ for epochs in range(num_total_steps):
-     total_value = 0
-     observation, info = env.reset()
- 
--    # Collect trajectory
-     total_reward = 0
-     num_batches = 0
-     mean_return = 0
-     batch = 0
-     num_episodes = 0
- 
--    print("Collecting batch trajectories...")
-+    '''
-+        The trajectories are collected in batches and will be saved to memory.
-+        The information is used for training the policy and value networks.
-+    '''
-     for iter in range(trajectory_iterations):
-         while True:
--
--            batch = 0
-             trajectory_observations.append(observation)
- 
--            num_batches += 1
--
-+            # Sample action of the agent
-             current_action_prob = policy_net(observation.reshape(1,input_length_net))
-             current_action_dist = tfd.Categorical(probs=current_action_prob)
--
-             if continous:
-                 action_std = tf.ones_like(current_action_prob)
-                 current_action_dist = tfd.MultivariateNormalDiag(current_action_prob, action_std)
-@@ -201,7 +203,7 @@ for epochs in range(num_total_steps):
-             current_action = current_action_dist.sample(seed=42).numpy()[0]
-             trajectory_actions.append(current_action)
- 
--            # Sample new state etc. from environment
-+            # Sample new state from environment with the current action
-             observation, reward, terminated, truncated, info = env.step(current_action)
-             num_passed_timesteps += 1
-             sum_rewards += reward
-@@ -210,27 +212,24 @@ for epochs in range(num_total_steps):
-             # Collect trajectory sample
-             trajectory_rewards.append(reward)
-             trajectory_action_probs.append(current_action_dist.prob(current_action))
--
-             value = value_net(observation.reshape((1,input_length_net)))
-             values.append(value)
--
--            batch += 1
-                 
-             if terminated or truncated:
-                 observation, info = env.reset()
- 
--                # Compute advantages at the end of the episode
-+                # Compute advantages at the end of the trajectory
-                 new_adv = np.array(total_reward, dtype=np.float32) - np.array(values, dtype=np.float32)
-                 new_adv = np.squeeze(new_adv)
-                 trajectory_advantages = np.append(trajectory_advantages, new_adv)
-                 trajectory_advantages = trajectory_advantages.flatten()
- 
-                 num_episodes += 1
--                batch = 0
-                 total_reward = 0
-                 values = []
-                 break
- 
-+    # Compute the mean cumulative reward.
-     mean_return = sum_rewards / num_episodes
-     sum_rewards = 0
-     print(f"Mean cumulative reward: {mean_return}", flush=True)
-@@ -243,12 +242,10 @@ for epochs in range(num_total_steps):
-     trajectory_advantages    = (trajectory_advantages - np.mean(trajectory_advantages)) / (np.std(trajectory_advantages) + 1e-8)
- 
-     # Update the network loop
--    print("Updating the neural networks...")
-     for epoch in range(num_epochs):
- 
-         with tf.GradientTape() as policy_tape:
-             policy_dist             = policy_net(trajectory_observations)
--            #policy_action_prob      = tf.experimental.numpy.max(policy_dist[:, :], axis=1)
-             dist                    = tfd.Categorical(probs=policy_dist)
-             if continous:
-                 action_std = tf.ones_like(policy_dist)
-@@ -288,7 +285,7 @@ for epochs in range(num_total_steps):
- 
-     # Log into tensorboard & Wandb
-     wandb.log({
--        'steps/time steps': num_passed_timesteps, 
-+        'time/time steps': num_passed_timesteps, 
-         'loss/policy loss': policy_loss, 
-         'loss/value loss': value_loss, 
-         'reward/mean reward': mean_return})
-@@ -307,6 +304,7 @@ wandb.run.finish() if wandb and wandb.run else None
- #
- #
- #
-+
- # Save the policy and value networks for further training/tests
- policy_net.save(f"{env_name}_policy_model_{num_passed_timesteps}")
- value_net.save(f"{env_name}_value_model_{num_passed_timesteps}")
-\ No newline at end of file
-diff --git a/PPO/ppo_torch/models/CartPole-v1__policyNet b/PPO/ppo_torch/models/CartPole-v1__policyNet
-deleted file mode 100644
-index acadbb4..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__policyNet and /dev/null differ
-diff --git a/PPO/ppo_torch/models/CartPole-v1__valueNet b/PPO/ppo_torch/models/CartPole-v1__valueNet
-deleted file mode 100644
-index 911e05a..0000000
-Binary files a/PPO/ppo_torch/models/CartPole-v1__valueNet and /dev/null differ
-diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
-index 19e0508..2a80dba 100644
---- a/PPO/ppo_torch/ppo_continuous.py
-+++ b/PPO/ppo_torch/ppo_continuous.py
-@@ -1,3 +1,4 @@
-+from collections import deque
- import torch
- from torch import nn
- import torch.nn.functional as F
-@@ -26,16 +27,15 @@ MODEL_PATH = './models/'
- ####### TODO #######
- ####################
- 
--# This is a TODO Section - please mark a todo as (done) if done
-+# Hint: Please if working on it mark a todo as (done) if done
- # 1) Check current implementation against article: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
--# 2) Fix incorrect calculation of rewards2go --> should be mean reward
--# 3) Fix calculation of Advantage
-+# 2) Check calculation of rewards --> correct mean reward over episodes? 
-+# 3) Check calculation of advantage 
- 
- ####################
- ####################
- 
- class Net(nn.Module):
--    
-     def __init__(self) -> None:
-         super(Net, self).__init__()
- 
-@@ -53,7 +53,7 @@ class ValueNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.relu(self.layer1(obs))
-         x = self.relu(self.layer2(x))
--        out = self.layer3(x) # linear activation
-+        out = self.layer3(x) # head has linear activation
-         return out
-     
-     def loss(self, obs, rewards):
-@@ -76,15 +76,15 @@ class PolicyNet(Net):
-             obs = torch.tensor(obs, dtype=torch.float)
-         x = self.tanh(self.layer1(obs))
-         x = self.tanh(self.layer2(x))
--        out = self.layer3(x) # linear if action space is continuous
-+        out = self.layer3(x) # head has linear activation (continuous space)
-         return out
-     
-     def loss(self, advantages, batch_log_probs, curr_log_probs, clip_eps=0.2):
-         """Make the clipped surrogate objective function to compute policy loss."""
-         ratio = torch.exp(curr_log_probs - batch_log_probs) # ratio between pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)
-         clip_1 = ratio * advantages
--        clip_2 = torch.clamp(ratio, min=1.0 - clip_eps, max=1.0 + clip_eps) * advantages
--        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss - we want to max it
-+        clip_2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages
-+        policy_loss = (-torch.min(clip_1, clip_2)).mean() # negative as Adam mins loss, but we want to max it
-         return policy_loss
- 
- 
-@@ -93,12 +93,14 @@ class PolicyNet(Net):
- 
- 
- class PPO_PolicyGradient:
--
-+    """Autonomous agent using Proximal Policy Optimization 
-+        as policy gradient method.
-+    """
-     def __init__(self, 
-         env, 
-         in_dim, 
-         out_dim,
--        total_timesteps,
-+        total_steps,
-         max_trajectory_size,
-         trajectory_iterations,
-         num_epochs=5,
-@@ -111,7 +113,7 @@ class PPO_PolicyGradient:
-         # hyperparams
-         self.in_dim = in_dim
-         self.out_dim = out_dim
--        self.total_timesteps = total_timesteps
-+        self.total_steps = total_steps
-         self.max_trajectory_size = max_trajectory_size
-         self.trajectory_iterations = trajectory_iterations
-         self.num_epochs = num_epochs
-@@ -132,13 +134,6 @@ class PPO_PolicyGradient:
-         self.policy_net_optim = Adam(self.policy_net.parameters(), lr=self.lr_p, eps=self.adam_eps) # Setup Policy Network (Actor) optimizer
-         self.value_net_optim = Adam(self.value_net.parameters(), lr=self.lr_v, eps=self.adam_eps)  # Setup Value Network (Critic) optimizer
- 
--    def get_discrete_policy(self, obs):
--        """Make function to compute action distribution in discrete action space."""
--        # 2) Use Categorial distribution for discrete space
--        # https://pytorch.org/docs/stable/distributions.html
--        action_prob = self.policy_net(obs) # query Policy Network (Actor) for mean action
--        return Categorical(logits=action_prob)
--
-     def get_continuous_policy(self, obs):
-         """Make function to compute action distribution in continuous action space."""
-         # Multivariate Normal Distribution Lecture 15.7 (Andrew Ng) https://www.youtube.com/watch?v=JjB58InuTqM
-@@ -163,9 +158,12 @@ class PPO_PolicyGradient:
-         log_prob = dist.log_prob(actions)
-         return values, log_prob
- 
-+    def get_value(self, obs):
-+        return self.value_net(obs).squeeze()
-+
-     def step(self, obs):
-         """ Given an observation, get action and probabilities from policy network (actor)"""
--        action_dist = self.get_continuous_policy(obs) # self.get_discrete_policy(obs)
-+        action_dist = self.get_continuous_policy(obs) 
-         action, log_prob, entropy = self.get_action(action_dist)
-         return action.detach().numpy(), log_prob.detach().numpy(), entropy.detach().numpy()
- 
-@@ -181,7 +179,7 @@ class PPO_PolicyGradient:
- 
-     def advantage_estimate(self, rewards, values, normalized=True):
-         """Simplest advantage calculation"""
--        # STEP 5: compute advantage estimates A_t
-+        # STEP 5: compute advantage estimates A_t at step t
-         advantages = rewards - values
-         if normalized:
-             advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
-@@ -189,80 +187,84 @@ class PPO_PolicyGradient:
-     
-     def generalized_advantage_estimate(self):
-         pass
--
--    def collect_rollout(self, sum_rewards, num_episodes, num_passed_timesteps, render=True):
--        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-     
-+    def collect_rollout(self, obs, n_step=1, render=True):
-+        """Collect a batch of simulated data each time we iterate the actor/critic network (on-policy)"""
-+        
-+        self.num_episodes = 0
-+        done = False
-+
-+        # collect trajectories
-         trajectory_obs = []
-         trajectory_actions = []
-         trajectory_action_probs = []
-         trajectory_rewards = []
--        trajectory_rewards_to_go = []
--
--        next_obs = self.env.reset()
--        total_reward, num_batches, mean_reward = 0, 0, 0
-+        trajectory_advantages = []
-+        trajectory_values = []
- 
-+        # Run an episode 
-         logging.info("Collecting batch trajectories...")
--        for iteration in range(0, trajectory_iterations):
--
--            # render gym env
--            if render:
--                self.env.render(mode='human')
-+        for _ in range(n_step):
- 
--            while True:
--                # Run an episode 
--                num_batches += 1
--                num_passed_timesteps += 1
--
--                # collect observation and get action with log probs
--                trajectory_obs.append(next_obs)
-+            while True: 
-+                # render gym env
-+                if render:
-+                    self.env.render(mode='human')
-+                    
-                 # action logic
--                with torch.no_grad():
--                    action, log_probability, _ = self.step(next_obs)
--                
-+                action, log_probability, _ = self.step(obs)
-+                        
-                 # STEP 3: collecting set of trajectories D_k by running action 
-                 # that was sampled from policy in environment
--                next_obs, reward, done, info = self.env.step(action)
--
--                total_reward += reward
--                sum_rewards += reward
-+                __obs, reward, done, _ = self.env.step(action)
-+                value = self.get_value(__obs)
- 
--                # tracking of values
-+                # collection of trajectories in batches
-+                trajectory_obs.append(obs)
-                 trajectory_actions.append(action)
-                 trajectory_action_probs.append(log_probability)
-                 trajectory_rewards.append(reward)
--                
-+                trajectory_values.append(value.detach())
-+                    
-+                obs = __obs
-+
-                 # break out of loop if episode is terminated
-                 if done:
--                    next_obs = self.env.reset()
--                    # calculate stats and reset all values
--                    num_episodes += 1
--                    total_reward, trajectory_values = 0, []
-+                    # STEP 4: Calculate cummulated reward
-+                    total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-+                    # STEP 5: compute advantage estimates A_t at timestep num_steps
-+                    advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), np.array(trajectory_values, dtype=np.float32))
-+                    trajectory_advantages = advantage
-+
-+                    self.num_episodes += 1
-+                    
-+                    # reset values
-+                    trajectory_values = []
-+                    obs = self.env.reset()
-                     break
--            
--        # STEP 4: Calculate rewards to go R_t
--        mean_reward = sum_rewards / num_episodes
--        logging.info(f"Mean cumulative reward: {mean_reward}")
--        trajectory_rewards_to_go = self.cummulative_reward(np.array(trajectory_rewards))
-         
--        return torch.tensor(np.array(trajectory_obs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_actions), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_action_probs), dtype=torch.float), \
--                torch.tensor(np.array(trajectory_rewards_to_go), dtype=torch.float), \
--                mean_reward, \
--                num_passed_timesteps
--
--    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, clip_eps):
-+        # convert trajectories to torch tensors
-+        obs = torch.tensor(np.array(trajectory_obs), dtype=torch.float)
-+        actions = torch.tensor(np.array(trajectory_actions), dtype=torch.float)
-+        log_probs = torch.tensor(np.array(trajectory_action_probs), dtype=torch.float)
-+        rewards = torch.tensor(np.array(trajectory_rewards), dtype=torch.float)
-+        advantages = torch.tensor(np.array(trajectory_advantages), dtype=torch.float)
-+
-+        return obs, actions, log_probs, rewards, advantages
-+                
-+
-+    def train(self, obs, rewards, advantages, batch_log_probs, curr_log_probs, epsilon):
-         """Calculate loss and update weights of both networks."""
-+        logging.info("Updating network parameter...")
-         # loss of the policy network
-         self.policy_net_optim.zero_grad() # reset optimizer
--        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, clip_eps)
-+        policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-         policy_loss.backward() # backpropagation
-         self.policy_net_optim.step() # single optimization step (updates parameter)
- 
-         # loss of the value network
-         self.value_net_optim.zero_grad() # reset optimizer
--        value_loss = self.value_net.loss(obs, rewards)
-+        value_loss = self.value_net.loss(obs, rewards) # TODO: Use V - rewards or V - advantages? 
-         value_loss.backward()
-         self.value_net_optim.step()
- 
-@@ -270,56 +272,47 @@ class PPO_PolicyGradient:
- 
-     def learn(self):
-         """"""
--        # logging info 
--        logging.info('Updating the neural network...')
--        num_passed_timesteps, mean_reward, sum_rewards, best_mean_reward, num_episodes, t_simulated = 0, 0, 0, 0, 1, 0 # number of timesteps simulated
-+        best_mean_reward = 0
-+        for steps in range(self.total_steps):
-         
--        for t_step in range(self.total_timesteps):
--            policy_loss, value_loss = 0, 0
-+            next_obs = self.env.reset()
-             # Collect trajectory
-             # STEP 3-4: imulate and collect trajectories --> the following values are all per batch
--            batch_obs, batch_actions, batch_a_log_probs, batch_rewards2go, mean_reward, num_passed_timesteps = self.collect_rollout(sum_rewards, num_episodes, num_passed_timesteps)
-+            obs, actions, log_probs, rewards, advantages = self.collect_rollout(obs=next_obs, n_step=self.trajectory_iterations)
-             
--            values = self.value_net(batch_obs).squeeze()
--            # STEP 5: compute advantage estimates A_t at timestep t_step
--            advantages = self.advantage_estimate(batch_rewards2go, values.detach()) # TODO: FIX - do use mean reward, not rewards2go
-+            # calculate mean reward per episode
-+            mean_reward = sum(rewards) / self.num_episodes # mean return
-             
--            # reset
--            sum_rewards = 0
--
--            # loop for network update
--            for epoch in range(self.num_epochs):
--                values, curr_v_log_probs = self.get_values(batch_obs, batch_actions)
-+            for _ in range(self.num_epochs):
-+                _, curr_log_probs = self.get_values(obs, actions)
-                 # STEP 6-7: calculate loss and update weights
--                policy_loss, value_loss = self.train(batch_obs, \
--                    batch_rewards2go, advantages, batch_log_probs=batch_a_log_probs, \
--                    curr_log_probs=curr_v_log_probs, clip_eps=self.epsilon)
--            
-+                policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-+                
-             logging.info('###########################################')
--            logging.info(f"Step: {t_step}, Policy loss: {policy_loss}")
--            logging.info(f"Step: {t_step}, Value loss: {value_loss}")
--            logging.info(f"Total time steps: {num_passed_timesteps}")
-+            logging.info(f"Mean cummulative reward: {mean_reward}")
-+            logging.info(f"Policy loss: {policy_loss}")
-+            logging.info(f"Value loss: {value_loss}")
-+            logging.info(f"Time step: {steps}")
-             logging.info('###########################################\n')
-             
-             # logging for monitoring in W&B
-             wandb.log({
--                'time steps': num_passed_timesteps,
-+                'time/step': steps,
-                 'loss/policy loss': policy_loss,
-                 'loss/value loss': value_loss,
--                'reward/cummulative reward': batch_rewards2go,
--                'reward/mean reward': mean_reward})
-+                'reward/mean return': mean_reward})
-             
-             # store model in checkpoints
-             if mean_reward > best_mean_reward:
-                 env_name = env.unwrapped.spec.id
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.policy_net.state_dict(),
-                     'optimizer_state_dict': self.policy_net_optim.state_dict(),
-                     'loss': policy_loss,
-                     }, f'{MODEL_PATH}{env_name}__policyNet')
-                 torch.save({
--                    'epoch': epoch,
-+                    'epoch': steps,
-                     'model_state_dict': self.value_net.state_dict(),
-                     'optimizer_state_dict': self.value_net_optim.state_dict(),
-                     'loss': policy_loss,
-@@ -350,9 +343,6 @@ def arg_parser():
-     
-     # Parse arguments if they are given
-     args = parser.parse_args()
--    # calculate batch and minibatch sizes
--    args.batch_size = int(args.num_envs * args.num_steps)
--    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     return args
- 
- def make_env(env_id='Pendulum-v1', seed=42):
-@@ -395,11 +385,11 @@ if __name__ == '__main__':
-     args = arg_parser()
-     # Hyperparameter
-     unity_file_name = ''            # name of unity environment
--    total_timesteps = 1000          # Total number of epochs to run the training
-+    total_steps = 1000              # time steps to train agent
-     max_trajectory_size = 10000     # max number of trajectory samples to be sampled per time step. 
-     trajectory_iterations = 10      # number of batches of episodes
--    num_epochs = 5                  # Number of epochs per time step to optimize the neural networks
--    learning_rate_p = 1e-3          # learning rate for policy network
-+    num_epochs = 20                 # Number of epochs per time step to optimize the neural networks
-+    learning_rate_p = 1e-4          # learning rate for policy network
-     learning_rate_v = 1e-3          # learning rate for value network
-     gamma = 0.99                    # discount factor
-     adam_epsilon = 1e-5             # default in the PPO baseline implementation is 1e-5, the pytorch default is 1e-8
-@@ -446,7 +436,7 @@ if __name__ == '__main__':
-     entity='drone-mechanics',
-     sync_tensorboard=True,
-     config={ # stores hyperparams in job
--            'total number of epochs': total_timesteps,
-+            'total number of steps': total_steps,
-             'max sampled trajectories': max_trajectory_size,
-             'batches per episode': trajectory_iterations,
-             'number of epochs for update': num_epochs,
-@@ -467,7 +457,7 @@ if __name__ == '__main__':
-                 env, 
-                 in_dim=obs_dim, 
-                 out_dim=act_dim,
--                total_timesteps=total_timesteps,
-+                total_steps=total_steps,
-                 max_trajectory_size=max_trajectory_size,
-                 trajectory_iterations=trajectory_iterations,
-                 num_epochs=num_epochs,
-@@ -480,6 +470,7 @@ if __name__ == '__main__':
-     # run training
-     agent.learn()
-     logging.info('### Done ###')
-+
-     # cleanup 
-     env.close()
-     wandb.run.finish() if wandb and wandb.run else None
-\ No newline at end of file
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/output.log b/wandb/run-20221215_130627-g5gz7u2b/files/output.log
deleted file mode 100644
index 2cfe264..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/output.log
+++ /dev/null
@@ -1,13 +0,0 @@
-
-INFO:root:Collecting batch trajectories...
-INFO:root:Updating network parameter...
-Traceback (most recent call last):
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 471, in <module>
-    agent.learn()
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 289, in learn
-    policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 261, in train
-    policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
-  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 85, in loss
-    clip_1 = ratio * advantages
-RuntimeError: The size of tensor a (2000) must match the size of tensor b (200) at non-singleton dimension 0
\ No newline at end of file
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt b/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt
deleted file mode 100644
index 9832a6c..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt
+++ /dev/null
@@ -1,56 +0,0 @@
-absl-py==1.3.0
-box2d-py==2.3.8
-cachetools==5.2.0
-certifi==2022.9.24
-cffi==1.15.1
-charset-normalizer==2.1.1
-click==8.1.3
-cloudpickle==2.2.0
-docker-pycreds==0.4.0
-future==0.18.2
-gitdb==4.0.10
-gitpython==3.1.29
-google-auth-oauthlib==0.4.6
-google-auth==2.15.0
-grpcio==1.51.1
-gym==0.21.0
-idna==3.4
-lz4==4.0.2
-markdown==3.4.1
-markupsafe==2.1.1
-numpy==1.23.5
-oauthlib==3.2.2
-opencv-python==4.6.0
-pandas==1.5.2
-pathtools==0.1.2
-pip==22.3.1
-promise==2.3
-protobuf==3.20.3
-psutil==5.9.4
-pyasn1-modules==0.2.8
-pyasn1==0.4.8
-pycparser==2.21
-pyglet==1.5.27
-python-dateutil==2.8.2
-pytz==2022.6
-pyyaml==6.0
-requests-oauthlib==1.3.1
-requests==2.28.1
-rsa==4.9
-scipy==1.9.3
-sentry-sdk==1.11.1
-setproctitle==1.3.2
-setuptools==65.5.1
-shortuuid==1.0.11
-six==1.16.0
-smmap==5.0.0
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.0
-torch-tb-profiler==0.4.0
-torch==1.12.1
-typing-extensions==4.4.0
-urllib3==1.26.13
-wandb==0.13.6
-werkzeug==2.2.2
-wheel==0.38.4
\ No newline at end of file
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/wandb-metadata.json b/wandb/run-20221215_130627-g5gz7u2b/files/wandb-metadata.json
deleted file mode 100644
index abd4aa9..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/wandb-metadata.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-    "os": "macOS-13.0.1-arm64-arm-64bit",
-    "python": "3.10.8",
-    "heartbeatAt": "2022-12-15T12:06:29.048684",
-    "startedAt": "2022-12-15T12:06:27.920251",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py",
-    "codePath": "PPO/ppo_torch/ppo_continuous.py",
-    "git": {
-        "remote": "git@github.com:bkunters/ml-explorer-drone.git",
-        "commit": "2f929e99dda18d14875731274576413223f58d8e"
-    },
-    "email": "janina.alica.mattes@gmail.com",
-    "root": "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone",
-    "host": "Janinas-MacBook-Pro.local",
-    "username": "janinaalicamattes",
-    "executable": "/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "disk": {
-        "total": 460.4317207336426,
-        "used": 8.218391418457031
-    },
-    "gpuapple": {
-        "type": "arm",
-        "vendor": "Apple"
-    },
-    "memory": {
-        "total": 16.0
-    }
-}
diff --git a/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json b/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json
deleted file mode 100644
index bee5d62..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb": {"runtime": 10}}
\ No newline at end of file
diff --git a/wandb/run-20221215_130627-g5gz7u2b/logs/debug-internal.log b/wandb/run-20221215_130627-g5gz7u2b/logs/debug-internal.log
deleted file mode 100644
index f991a79..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/logs/debug-internal.log
+++ /dev/null
@@ -1,184 +0,0 @@
-2022-12-15 13:06:27,952 INFO    StreamThr :9711 [internal.py:wandb_internal():87] W&B internal server running at pid: 9711, started at: 2022-12-15 13:06:27.950662
-2022-12-15 13:06:27,953 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: status
-2022-12-15 13:06:27,954 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: status
-2022-12-15 13:06:27,955 DEBUG   SenderThread:9711 [sender.py:send():303] send: header
-2022-12-15 13:06:27,955 INFO    WriterThread:9711 [datastore.py:open_for_write():75] open: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/run-g5gz7u2b.wandb
-2022-12-15 13:06:27,961 DEBUG   SenderThread:9711 [sender.py:send():303] send: run
-2022-12-15 13:06:28,916 INFO    SenderThread:9711 [dir_watcher.py:__init__():216] watching files in: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files
-2022-12-15 13:06:28,916 INFO    SenderThread:9711 [sender.py:_start_run_threads():928] run started: g5gz7u2b with start time 1671105987.943827
-2022-12-15 13:06:28,916 DEBUG   SenderThread:9711 [sender.py:send():303] send: summary
-2022-12-15 13:06:28,916 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 13:06:28,918 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: check_version
-2022-12-15 13:06:28,918 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: check_version
-2022-12-15 13:06:29,040 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: run_start
-2022-12-15 13:06:29,047 DEBUG   HandlerThread:9711 [system_info.py:__init__():31] System info init
-2022-12-15 13:06:29,048 DEBUG   HandlerThread:9711 [system_info.py:__init__():46] System info init done
-2022-12-15 13:06:29,048 INFO    HandlerThread:9711 [system_monitor.py:start():150] Starting system monitor
-2022-12-15 13:06:29,048 INFO    SystemMonitor:9711 [system_monitor.py:_start():116] Starting system asset monitoring threads
-2022-12-15 13:06:29,048 INFO    HandlerThread:9711 [system_monitor.py:probe():171] Collecting system info
-2022-12-15 13:06:29,048 DEBUG   HandlerThread:9711 [system_info.py:probe():195] Probing system
-2022-12-15 13:06:29,052 INFO    SystemMonitor:9711 [interfaces.py:start():168] Started cpu
-2022-12-15 13:06:29,053 DEBUG   HandlerThread:9711 [system_info.py:_probe_git():180] Probing git
-2022-12-15 13:06:29,053 INFO    SystemMonitor:9711 [interfaces.py:start():168] Started disk
-2022-12-15 13:06:29,059 INFO    SystemMonitor:9711 [interfaces.py:start():168] Started gpuapple
-2022-12-15 13:06:29,062 INFO    SystemMonitor:9711 [interfaces.py:start():168] Started memory
-2022-12-15 13:06:29,063 INFO    SystemMonitor:9711 [interfaces.py:start():168] Started network
-2022-12-15 13:06:29,074 DEBUG   HandlerThread:9711 [system_info.py:_probe_git():188] Probing git done
-2022-12-15 13:06:29,074 DEBUG   HandlerThread:9711 [system_info.py:probe():241] Probing system done
-2022-12-15 13:06:29,074 DEBUG   HandlerThread:9711 [system_monitor.py:probe():180] {'os': 'macOS-13.0.1-arm64-arm-64bit', 'python': '3.10.8', 'heartbeatAt': '2022-12-15T12:06:29.048684', 'startedAt': '2022-12-15T12:06:27.920251', 'docker': None, 'cuda': None, 'args': (), 'state': 'running', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py', 'codePath': 'PPO/ppo_torch/ppo_continuous.py', 'git': {'remote': 'git@github.com:bkunters/ml-explorer-drone.git', 'commit': '2f929e99dda18d14875731274576413223f58d8e'}, 'email': 'janina.alica.mattes@gmail.com', 'root': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone', 'host': 'Janinas-MacBook-Pro.local', 'username': 'janinaalicamattes', 'executable': '/Users/janinaalicamattes/miniforge3/envs/pytorch-ppo-research/bin/python', 'cpu_count': 8, 'cpu_count_logical': 8, 'disk': {'total': 460.4317207336426, 'used': 8.218391418457031}, 'gpuapple': {'type': 'arm', 'vendor': 'Apple'}, 'memory': {'total': 16.0}}
-2022-12-15 13:06:29,074 INFO    HandlerThread:9711 [system_monitor.py:probe():181] Finished collecting system info
-2022-12-15 13:06:29,074 INFO    HandlerThread:9711 [system_monitor.py:probe():184] Publishing system info
-2022-12-15 13:06:29,074 DEBUG   HandlerThread:9711 [system_info.py:_save_pip():51] Saving list of pip packages installed into the current environment
-2022-12-15 13:06:29,075 DEBUG   HandlerThread:9711 [system_info.py:_save_pip():67] Saving pip packages done
-2022-12-15 13:06:29,077 DEBUG   HandlerThread:9711 [system_info.py:_save_conda():74] Saving list of conda packages installed into the current environment
-2022-12-15 13:06:29,922 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json
-2022-12-15 13:06:29,923 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml
-2022-12-15 13:06:29,923 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt
-2022-12-15 13:06:31,172 DEBUG   HandlerThread:9711 [system_info.py:_save_conda():86] Saving conda packages done
-2022-12-15 13:06:31,172 DEBUG   HandlerThread:9711 [system_info.py:_save_code():89] Saving code
-2022-12-15 13:06:31,178 DEBUG   HandlerThread:9711 [system_info.py:_save_code():110] Saving code done
-2022-12-15 13:06:31,178 DEBUG   HandlerThread:9711 [system_info.py:_save_patches():127] Saving git patches
-2022-12-15 13:06:31,253 DEBUG   HandlerThread:9711 [system_info.py:_save_patches():169] Saving git patches done
-2022-12-15 13:06:31,253 INFO    HandlerThread:9711 [system_monitor.py:probe():186] Finished publishing system info
-2022-12-15 13:06:31,279 DEBUG   SenderThread:9711 [sender.py:send():303] send: files
-2022-12-15 13:06:31,279 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file wandb-metadata.json with policy now
-2022-12-15 13:06:31,280 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file code/PPO/ppo_torch/ppo_continuous.py with policy now
-2022-12-15 13:06:31,280 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file diff.patch with policy now
-2022-12-15 13:06:31,284 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: stop_status
-2022-12-15 13:06:31,285 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: stop_status
-2022-12-15 13:06:31,724 DEBUG   SenderThread:9711 [sender.py:send():303] send: telemetry
-2022-12-15 13:06:31,890 INFO    Thread-17 :9711 [upload_job.py:push():123] Skipped uploading /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpksuwsdnuwandb/mkyf9027-code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 13:06:31,932 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-metadata.json
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/diff.patch
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/code
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch
-2022-12-15 13:06:31,933 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_created():275] file/dir created: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO
-2022-12-15 13:06:32,232 INFO    Thread-18 :9711 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpksuwsdnuwandb/zdcxv3xk-diff.patch
-2022-12-15 13:06:32,373 INFO    Thread-16 :9711 [upload_job.py:push():143] Uploaded file /var/folders/4h/v0fwv1zs4596mmdvwj516k840000gn/T/tmpksuwsdnuwandb/3h7hyk7m-wandb-metadata.json
-2022-12-15 13:06:33,943 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log
-2022-12-15 13:06:39,207 DEBUG   SenderThread:9711 [sender.py:send():303] send: exit
-2022-12-15 13:06:39,208 INFO    SenderThread:9711 [sender.py:send_exit():442] handling exit code: 1
-2022-12-15 13:06:39,208 INFO    SenderThread:9711 [sender.py:send_exit():444] handling runtime: 10
-2022-12-15 13:06:39,209 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 13:06:39,209 INFO    SenderThread:9711 [sender.py:send_exit():450] send defer
-2022-12-15 13:06:39,209 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,209 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 0
-2022-12-15 13:06:39,209 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,209 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 0
-2022-12-15 13:06:39,209 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 1
-2022-12-15 13:06:39,210 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,210 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 1
-2022-12-15 13:06:39,210 INFO    HandlerThread:9711 [system_monitor.py:finish():160] Stopping system monitor
-2022-12-15 13:06:39,232 DEBUG   SystemMonitor:9711 [system_monitor.py:_start():130] Starting system metrics aggregation loop
-2022-12-15 13:06:39,232 DEBUG   SystemMonitor:9711 [system_monitor.py:_start():137] Finished system metrics aggregation loop
-2022-12-15 13:06:39,232 DEBUG   SystemMonitor:9711 [system_monitor.py:_start():141] Publishing last batch of metrics
-2022-12-15 13:06:39,233 INFO    HandlerThread:9711 [interfaces.py:finish():175] Joined cpu
-2022-12-15 13:06:39,233 INFO    HandlerThread:9711 [interfaces.py:finish():175] Joined disk
-2022-12-15 13:06:39,233 INFO    HandlerThread:9711 [interfaces.py:finish():175] Joined gpuapple
-2022-12-15 13:06:39,233 INFO    HandlerThread:9711 [interfaces.py:finish():175] Joined memory
-2022-12-15 13:06:39,233 INFO    HandlerThread:9711 [interfaces.py:finish():175] Joined network
-2022-12-15 13:06:39,233 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,233 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 1
-2022-12-15 13:06:39,234 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 2
-2022-12-15 13:06:39,234 DEBUG   SenderThread:9711 [sender.py:send():303] send: telemetry
-2022-12-15 13:06:39,234 DEBUG   SenderThread:9711 [sender.py:send():303] send: stats
-2022-12-15 13:06:39,234 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,234 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 2
-2022-12-15 13:06:39,234 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,234 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 2
-2022-12-15 13:06:39,234 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 3
-2022-12-15 13:06:39,234 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,234 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 3
-2022-12-15 13:06:39,234 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,234 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 3
-2022-12-15 13:06:39,234 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 4
-2022-12-15 13:06:39,234 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,234 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 4
-2022-12-15 13:06:39,234 DEBUG   SenderThread:9711 [sender.py:send():303] send: summary
-2022-12-15 13:06:39,239 INFO    SenderThread:9711 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end
-2022-12-15 13:06:39,239 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,239 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 4
-2022-12-15 13:06:39,239 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 5
-2022-12-15 13:06:39,239 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,239 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 5
-2022-12-15 13:06:39,239 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,239 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 5
-2022-12-15 13:06:39,498 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 6
-2022-12-15 13:06:39,499 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:39,499 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 6
-2022-12-15 13:06:39,499 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:39,499 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 6
-2022-12-15 13:06:39,970 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json
-2022-12-15 13:06:39,971 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log
-2022-12-15 13:06:39,971 INFO    Thread-13 :9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml
-2022-12-15 13:06:40,213 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-15 13:06:41,215 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 7
-2022-12-15 13:06:41,216 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: poll_exit
-2022-12-15 13:06:41,216 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:41,216 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 7
-2022-12-15 13:06:41,217 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:41,217 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 7
-2022-12-15 13:06:41,217 INFO    SenderThread:9711 [dir_watcher.py:finish():362] shutting down directory watcher
-2022-12-15 13:06:41,978 INFO    SenderThread:9711 [dir_watcher.py:_on_file_modified():292] file/dir modified: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log
-2022-12-15 13:06:41,979 INFO    SenderThread:9711 [dir_watcher.py:finish():392] scan: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files
-2022-12-15 13:06:41,980 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml conda-environment.yaml
-2022-12-15 13:06:41,980 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml config.yaml
-2022-12-15 13:06:41,983 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/diff.patch diff.patch
-2022-12-15 13:06:41,984 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log output.log
-2022-12-15 13:06:41,986 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt requirements.txt
-2022-12-15 13:06:41,990 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-metadata.json wandb-metadata.json
-2022-12-15 13:06:41,990 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json wandb-summary.json
-2022-12-15 13:06:41,995 INFO    SenderThread:9711 [dir_watcher.py:finish():406] scan save: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/code/PPO/ppo_torch/ppo_continuous.py code/PPO/ppo_torch/ppo_continuous.py
-2022-12-15 13:06:41,995 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 8
-2022-12-15 13:06:41,996 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:41,996 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 8
-2022-12-15 13:06:41,996 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:41,996 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 8
-2022-12-15 13:06:41,996 INFO    SenderThread:9711 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-15 13:06:43,145 INFO    Thread-22 :9711 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/requirements.txt
-2022-12-15 13:06:43,146 INFO    Thread-19 :9711 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/conda-environment.yaml
-2022-12-15 13:06:43,146 INFO    Thread-20 :9711 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/config.yaml
-2022-12-15 13:06:43,147 INFO    Thread-23 :9711 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/wandb-summary.json
-2022-12-15 13:06:43,147 INFO    Thread-21 :9711 [upload_job.py:push():143] Uploaded file /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/files/output.log
-2022-12-15 13:06:43,353 INFO    Thread-12 (_thread_body):9711 [sender.py:transition_state():463] send defer: 9
-2022-12-15 13:06:43,353 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:43,353 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 9
-2022-12-15 13:06:43,353 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:43,353 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 9
-2022-12-15 13:06:43,353 INFO    SenderThread:9711 [file_pusher.py:join():173] waiting for file pusher
-2022-12-15 13:06:43,353 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 10
-2022-12-15 13:06:43,354 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:43,354 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 10
-2022-12-15 13:06:43,354 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:43,354 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 10
-2022-12-15 13:06:43,546 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 11
-2022-12-15 13:06:43,546 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:43,546 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 11
-2022-12-15 13:06:43,546 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:43,546 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 11
-2022-12-15 13:06:43,546 INFO    SenderThread:9711 [sender.py:transition_state():463] send defer: 12
-2022-12-15 13:06:43,547 DEBUG   SenderThread:9711 [sender.py:send():303] send: final
-2022-12-15 13:06:43,547 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: defer
-2022-12-15 13:06:43,547 INFO    HandlerThread:9711 [handler.py:handle_request_defer():162] handle defer: 12
-2022-12-15 13:06:43,547 DEBUG   SenderThread:9711 [sender.py:send():303] send: footer
-2022-12-15 13:06:43,547 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: defer
-2022-12-15 13:06:43,547 INFO    SenderThread:9711 [sender.py:send_request_defer():459] handle sender defer: 12
-2022-12-15 13:06:43,547 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: poll_exit
-2022-12-15 13:06:43,548 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: server_info
-2022-12-15 13:06:43,548 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: poll_exit
-2022-12-15 13:06:43,548 DEBUG   SenderThread:9711 [sender.py:send_request():317] send_request: server_info
-2022-12-15 13:06:43,549 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: get_summary
-2022-12-15 13:06:43,549 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: sampled_history
-2022-12-15 13:06:44,060 INFO    MainThread:9711 [wandb_run.py:_footer_history_summary_info():3408] rendering history
-2022-12-15 13:06:44,061 INFO    MainThread:9711 [wandb_run.py:_footer_history_summary_info():3440] rendering summary
-2022-12-15 13:06:44,061 INFO    MainThread:9711 [wandb_run.py:_footer_sync_info():3364] logging synced files
-2022-12-15 13:06:44,062 DEBUG   HandlerThread:9711 [handler.py:handle_request():139] handle_request: shutdown
-2022-12-15 13:06:44,062 INFO    HandlerThread:9711 [handler.py:finish():814] shutting down handler
-2022-12-15 13:06:44,553 INFO    WriterThread:9711 [datastore.py:close():279] close: /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/run-g5gz7u2b.wandb
-2022-12-15 13:06:45,068 INFO    SenderThread:9711 [sender.py:finish():1331] shutting down sender
-2022-12-15 13:06:45,068 INFO    SenderThread:9711 [file_pusher.py:finish():168] shutting down file pusher
-2022-12-15 13:06:45,068 INFO    SenderThread:9711 [file_pusher.py:join():173] waiting for file pusher
-2022-12-15 13:06:45,742 INFO    MainThread:9711 [internal.py:handle_exit():77] Internal process exited
diff --git a/wandb/run-20221215_130627-g5gz7u2b/logs/debug.log b/wandb/run-20221215_130627-g5gz7u2b/logs/debug.log
deleted file mode 100644
index a87c058..0000000
--- a/wandb/run-20221215_130627-g5gz7u2b/logs/debug.log
+++ /dev/null
@@ -1,26 +0,0 @@
-2022-12-15 13:06:27,928 INFO    MainThread:9702 [wandb_setup.py:_flush():68] Configure stats pid to 9702
-2022-12-15 13:06:27,928 INFO    MainThread:9702 [wandb_setup.py:_flush():68] Loading settings from /Users/janinaalicamattes/.config/wandb/settings
-2022-12-15 13:06:27,928 INFO    MainThread:9702 [wandb_setup.py:_flush():68] Loading settings from /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/settings
-2022-12-15 13:06:27,928 INFO    MainThread:9702 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
-2022-12-15 13:06:27,928 INFO    MainThread:9702 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'PPO/ppo_torch/ppo_continuous.py', 'program': '/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py'}
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:_log_setup():476] Logging user logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/logs/debug.log
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:_log_setup():477] Logging internal logs to /Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/wandb/run-20221215_130627-g5gz7u2b/logs/debug-internal.log
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:init():516] calling init triggers
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
-config: {'total number of steps': 1000, 'max sampled trajectories': 10000, 'batches per episode': 10, 'number of epochs for update': 20, 'input layer size': 3, 'output layer size': 1, 'learning rate (policy net)': 0.0001, 'learning rate (value net)': 0.001, 'epsilon (adam optimizer)': 1e-05, 'gamma (discount)': 0.99, 'epsilon (clipping)': 0.2}
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:init():569] starting backend
-2022-12-15 13:06:27,929 INFO    MainThread:9702 [wandb_init.py:init():573] setting up manager
-2022-12-15 13:06:27,939 INFO    MainThread:9702 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
-2022-12-15 13:06:27,943 INFO    MainThread:9702 [wandb_init.py:init():580] backend started and connected
-2022-12-15 13:06:27,947 INFO    MainThread:9702 [wandb_init.py:init():658] updated telemetry
-2022-12-15 13:06:27,959 INFO    MainThread:9702 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
-2022-12-15 13:06:28,917 INFO    MainThread:9702 [wandb_run.py:_on_init():2006] communicating current version
-2022-12-15 13:06:29,030 INFO    MainThread:9702 [wandb_run.py:_on_init():2010] got version response upgrade_message: "wandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
-
-2022-12-15 13:06:29,030 INFO    MainThread:9702 [wandb_init.py:init():728] starting run threads in backend
-2022-12-15 13:06:31,283 INFO    MainThread:9702 [wandb_run.py:_console_start():1986] atexit reg
-2022-12-15 13:06:31,283 INFO    MainThread:9702 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
-2022-12-15 13:06:31,283 INFO    MainThread:9702 [wandb_run.py:_redirect():1909] Wrapping output streams.
-2022-12-15 13:06:31,283 INFO    MainThread:9702 [wandb_run.py:_redirect():1931] Redirects installed.
-2022-12-15 13:06:31,284 INFO    MainThread:9702 [wandb_init.py:init():765] run started, returning control to user process
-2022-12-15 13:06:45,075 WARNING MsgRouterThr:9702 [router.py:message_loop():77] message_loop has been closed
diff --git a/wandb/run-20221215_130627-g5gz7u2b/run-g5gz7u2b.wandb b/wandb/run-20221215_130627-g5gz7u2b/run-g5gz7u2b.wandb
deleted file mode 100644
index 2052757..0000000
Binary files a/wandb/run-20221215_130627-g5gz7u2b/run-g5gz7u2b.wandb and /dev/null differ
